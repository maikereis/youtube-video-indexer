{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"YouTube Video Indexer","text":"<p>A high-performance, event-driven system for indexing YouTube content through PubSubHubbub notifications. Built with Python, FastAPI, and modern async patterns for scalable video metadata processing and search.</p>"},{"location":"#overview","title":"Overview","text":"<p>The YouTube Video Indexer is designed to efficiently receive, process, and index YouTube video content in real-time. The system follows an event-driven architecture that emphasizes scalability, fault tolerance, and separation of concerns.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Real-time Processing: Handles YouTube PubSubHubbub notifications as they arrive</li> <li>Scalable Architecture: Microservices design allowing independent scaling of components</li> <li>Full-text Search: Elasticsearch integration for fast content discovery</li> <li>Fault Tolerant: Queue-based processing with automatic retry mechanisms</li> <li>API-First: RESTful API for integration with external systems</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>Content Monitoring: Track channel updates and video publications and indexing thein content</li> <li>Analytics Platform: Build dashboards around YouTube content trends</li> <li>Search Engine: Create searchable indexes of video metadata</li> <li>RAG System: Retrieval augmented systems</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>The system consists of three main services working together:</p> <pre><code>graph TD\n    A[YouTube PubSubHubbub] --&gt;|Notification| B[API Gateway]\n    B --&gt;|Queue| C[Queue Worker]\n    C --&gt;|Process| D[Indexing Service]\n    D --&gt;|Store| E[MongoDB]\n    D --&gt;|Index| F[Elasticsearch]\n    B --&gt;|Cache/Queue| G[Valkey/Redis]\n\n    H[API Clients] --&gt;|Search/Query| B\n\n    style A fill:#e8f4f8,stroke:#2c3e50,stroke-width:2px\n    style B fill:#f0f8e8,stroke:#2c3e50,stroke-width:2px\n    style C fill:#f8f0e8,stroke:#2c3e50,stroke-width:2px\n    style D fill:#e8e8f8,stroke:#2c3e50,stroke-width:2px\n    style E fill:#f8e8f0,stroke:#2c3e50,stroke-width:2px\n    style F fill:#e8f8f0,stroke:#2c3e50,stroke-width:2px\n    style G fill:#f8f8e8,stroke:#2c3e50,stroke-width:2px</code></pre>"},{"location":"#services","title":"Services","text":"<ol> <li>API Gateway: Handles webhook verification, API requests, and rate limiting</li> <li>Queue Worker: Processes notifications and extracts metadata</li> <li>Indexing Service: Stores data in MongoDB and maintains Elasticsearch indices</li> </ol>"},{"location":"#data-stores","title":"Data Stores","text":"<ul> <li>MongoDB: Primary document store for video and channel metadata</li> <li>Elasticsearch: Full-text search and advanced filtering capabilities</li> <li>Valkey/Redis: Message queues, rate limiting, and caching</li> </ul> <p>See the installation guide and  quick-start</p>"},{"location":"#technology-stack","title":"Technology Stack","text":""},{"location":"#backend","title":"Backend","text":"<ul> <li>FastAPI: Modern, fast web framework for building APIs</li> <li>Python: Async/await patterns for high-performance I/O</li> <li>SlowAPI: Rate limiting and request throttling</li> </ul>"},{"location":"#data-layer","title":"Data Layer","text":"<ul> <li>MongoDB: Document storage with Motor async driver</li> <li>Elasticsearch: Search engine with AsyncElasticsearch client</li> <li>Valkey/Redis: Queue management and caching</li> </ul>"},{"location":"#infrastructure","title":"Infrastructure","text":"<ul> <li>Docker: Containerization for consistent deployments</li> <li>Docker Compose: Multi-service orchestration</li> <li>Nginx: Reverse proxy and load balancing (optional)</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>Issue Tracker</li> <li>Discussions</li> </ul>"},{"location":"architecture/data-flow/","title":"Data Flow","text":""},{"location":"architecture/data-flow/#data-flow-architecture","title":"Data Flow Architecture","text":"<p>The system processes data through a series of well-defined stages:</p> <pre><code>sequenceDiagram\n    participant YT as YouTube\n    participant GW as API Gateway\n    participant NQ as Notification Queue\n    participant QW as Queue Worker\n    participant MQ as Metadata Queue\n    participant IS as Index Service\n    participant DB as MongoDB\n    participant ES as Elasticsearch\n\n    YT-&gt;&gt;GW: PubSubHubbub Notification\n    GW-&gt;&gt;GW: Validate &amp; Authenticate\n    GW-&gt;&gt;NQ: Enqueue Notification\n    GW-&gt;&gt;YT: HTTP 200 OK\n\n    NQ-&gt;&gt;QW: Dequeue Notification\n    QW-&gt;&gt;QW: Parse XML Data\n    QW-&gt;&gt;QW: Extract Metadata\n    QW-&gt;&gt;MQ: Enqueue Metadata\n\n    MQ-&gt;&gt;IS: Dequeue Metadata\n    IS-&gt;&gt;IS: Metadata enrichment\n    IS-&gt;&gt;DB: Store Video Metadata\n    IS-&gt;&gt;ES: Update Search Index\n    IS-&gt;&gt;IS: Update Channel Stats</code></pre>"},{"location":"architecture/data-flow/#processing-stages","title":"Processing Stages","text":"<ol> <li>Notification Receipt: YouTube sends PubSubHubbub notifications to the API Gateway</li> <li>Validation: The gateway validates the notification signature and source</li> <li>Queuing: Valid notifications are queued for asynchronous processing</li> <li>Extraction: The Queue Worker parses XML and extracts video metadata</li> <li>Enrichment: Additional processing and data validation occurs</li> <li>Storage: The Indexing Service persists data to MongoDB</li> <li>Indexing: Video metadata is indexed in Elasticsearch for search</li> <li>Statistics: Channel-level statistics are updated</li> </ol>"},{"location":"architecture/data-stores/","title":"Data Stores","text":""},{"location":"architecture/data-stores/#data-storage-strategy","title":"Data Storage Strategy","text":""},{"location":"architecture/data-stores/#multi-store-architecture","title":"Multi-Store Architecture","text":"<p>The system uses specialized data stores for different use cases:</p> <p>MongoDB (Primary Store):</p> <ul> <li>Document-oriented storage for flexible video metadata</li> <li>Rich querying capabilities for complex data relationships</li> <li>Atomic operations for data consistency</li> <li>Horizontal scaling through sharding</li> </ul> <p>Elasticsearch (Search Engine):</p> <ul> <li>Full-text search across video titles, descriptions, and tags</li> <li>Advanced filtering and aggregation capabilities</li> <li>Near real-time search updates</li> <li>Distributed architecture for high availability</li> </ul> <p>Valkey/Redis (Cache &amp; Queue):</p> <ul> <li>Message queues for inter-service communication</li> <li>Rate limiting data storage</li> <li>Session management and caching</li> <li>High-performance in-memory operations</li> </ul>"},{"location":"architecture/data-stores/#data-consistency","title":"Data Consistency","text":"<p>The system implements eventual consistency between stores:</p> <ul> <li>MongoDB serves as the source of truth</li> <li>Elasticsearch indices are updated asynchronously</li> <li>Failed indexing operations are retried automatically</li> <li>Data inconsistencies are detected and resolved</li> </ul>"},{"location":"architecture/overview/","title":"System Architecture Overview","text":"<p>The YouTube Video Indexer follows an event-driven, microservices architecture designed for high scalability, fault tolerance, and maintainability. This document provides a comprehensive overview of the system's design principles and architectural decisions.</p>"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":""},{"location":"architecture/overview/#event-driven-architecture","title":"Event-Driven Architecture","text":"<p>The system is built around asynchronous message passing, allowing services to operate independently while maintaining loose coupling. This approach provides several benefits:</p> <ul> <li>Scalability: Each service can be scaled independently based on demand</li> <li>Resilience: Failures in one service don't cascade to others</li> <li>Flexibility: New services can be added without modifying existing ones</li> <li>Performance: Non-blocking operations maximize throughput</li> </ul>"},{"location":"architecture/overview/#separation-of-concerns","title":"Separation of Concerns","text":"<p>Each service has a single, well-defined responsibility:</p> <ul> <li>API Gateway: HTTP interface and request routing</li> <li>Queue Worker: Message processing and data extraction</li> <li>Indexing Service: Data persistence and search indexing</li> </ul>"},{"location":"architecture/overview/#fault-tolerance","title":"Fault Tolerance","text":"<p>The system implements multiple layers of fault tolerance:</p> <ul> <li>Queue-based Processing: Messages persist even if services are temporarily unavailable</li> <li>Retry Logic: Failed operations are automatically retried with exponential backoff</li> <li>Circuit Breakers: Prevent cascading failures by temporarily disabling unhealthy dependencies</li> <li>Health Checks: Monitor service availability and trigger alerts</li> </ul>"},{"location":"architecture/overview/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n   subgraph \"External\"\n       YT[YouTube PubSubHubbub]\n       CLIENT[API Clients]\n   end\n\n   subgraph \"API Layer\"\n       GW[API Gateway&lt;br/&gt;FastAPI + SlowAPI]\n   end\n\n   subgraph \"Processing Layer\"\n       QW[Queue Worker&lt;br/&gt;Async Python]\n       IS[Indexing Service&lt;br/&gt;Motor + AsyncES]\n   end\n\n   subgraph \"Data Layer\"\n       REDIS[(Valkey/Redis&lt;br/&gt;Queues + Cache)]\n       MONGO[(MongoDB&lt;br/&gt;Document Store)]\n       ES[(Elasticsearch&lt;br/&gt;Search Index)]\n   end\n\n   YT --&gt;|Webhook| GW\n   CLIENT --&gt;|HTTP API| GW\n   GW &lt;--&gt;|Queue Operations| REDIS\n   GW --&gt;|Enqueue| QW\n   QW &lt;--&gt;|Queue Operations| REDIS\n   QW --&gt;|Process| IS\n   IS --&gt;|Store| MONGO\n   IS --&gt;|Index| ES\n\n   style YT fill:#e8f4f8,stroke:#2c3e50,stroke-width:2px\n   style CLIENT fill:#f0f8e8,stroke:#2c3e50,stroke-width:2px\n   style GW fill:#f8f0e8,stroke:#2c3e50,stroke-width:2px\n   style QW fill:#e8e8f8,stroke:#2c3e50,stroke-width:2px\n   style IS fill:#f8e8f0,stroke:#2c3e50,stroke-width:2px\n   style REDIS fill:#e8f8f0,stroke:#2c3e50,stroke-width:2px\n   style MONGO fill:#f8f8e8,stroke:#2c3e50,stroke-width:2px\n   style ES fill:#f4f0f8,stroke:#2c3e50,stroke-width:2px</code></pre>"},{"location":"architecture/scaling/","title":"Scaling","text":""},{"location":"architecture/scaling/#scalability-design","title":"Scalability Design","text":""},{"location":"architecture/scaling/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>Each service can be scaled independently:</p> <pre><code>graph LR\n    subgraph \"Load Balancer\"\n        LB[Nginx/HAProxy]\n    end\n\n    subgraph \"API Gateway Cluster\"\n        GW1[Gateway 1]\n        GW2[Gateway 2]\n        GW3[Gateway N]\n    end\n\n    subgraph \"Queue Worker Cluster\"\n        QW1[Worker 1]\n        QW2[Worker 2]\n        QW3[Worker N]\n    end\n\n    subgraph \"Indexing Service Cluster\"\n        IS1[Indexer 1]\n        IS2[Indexer 2]\n        IS3[Indexer N]\n    end\n\n    LB --&gt; GW1\n    LB --&gt; GW2\n    LB --&gt; GW3\n\n    GW1 -.-&gt; QW1\n    GW2 -.-&gt; QW2\n    GW3 -.-&gt; QW3\n\n    QW1 -.-&gt; IS1\n    QW2 -.-&gt; IS2\n    QW3 -.-&gt; IS3</code></pre>"},{"location":"architecture/scaling/#performance-characteristics","title":"Performance Characteristics","text":"<p>Based on recent benchmarking:</p> <ul> <li>API Gateway: Handles 245+ RPS with median 1.2s latency</li> <li>Queue Worker: Processes thousands of messages per minute</li> <li>Indexing Service: Maintains search indices for millions of documents</li> </ul>"},{"location":"architecture/scaling/#scaling-strategies","title":"Scaling Strategies","text":"<p>API Gateway Scaling:</p> <ul> <li>Add more instances behind a load balancer</li> <li>Optimize for I/O-bound webhook processing</li> <li>Scale based on HTTP request volume</li> </ul> <p>Queue Worker Scaling:</p> <ul> <li>Increase worker instances to handle queue backlog</li> <li>Monitor queue depth and processing time</li> <li>Scale based on notification volume</li> </ul> <p>Indexing Service Scaling:</p> <ul> <li>Scale for write-heavy workloads to MongoDB and Elasticsearch</li> <li>Batch operations for improved throughput</li> <li>Scale based on indexing latency and volume</li> </ul>"},{"location":"architecture/services/","title":"Services","text":""},{"location":"architecture/services/#component-responsibilities","title":"Component Responsibilities","text":""},{"location":"architecture/services/#api-gateway-service","title":"API Gateway Service","text":"<p>Primary Role: HTTP interface and request management</p> <p>Key Responsibilities:</p> <ul> <li>Accept and validate YouTube PubSubHubbub webhook notifications</li> <li>Provide RESTful API endpoints for video search and retrieval</li> <li>Implement rate limiting to protect against abuse</li> <li>Manage trusted host validation for security</li> <li>Queue incoming notifications for processing</li> <li>Expose monitoring and statistics endpoints</li> </ul> <p>Technologies:</p> <ul> <li>FastAPI for high-performance async HTTP handling</li> <li>SlowAPI for sophisticated rate limiting</li> <li>Valkey/Redis for queue management and rate limiting storage</li> </ul>"},{"location":"architecture/services/#queue-worker-service","title":"Queue Worker Service","text":"<p>Primary Role: Asynchronous message processing</p> <p>Key Responsibilities:</p> <ul> <li>Dequeue notification messages from the processing queue</li> <li>Parse XML notification payloads to extract metadata</li> <li>Validate and channel information</li> <li>Forward processed data to the indexing service via queue</li> <li>Handle processing errors and implement retry logic</li> </ul> <p>Technologies:</p> <ul> <li>Python asyncio for concurrent message processing</li> <li>XML parsing libraries for notification data extraction</li> <li>Valkey/Redis client for queue operations</li> </ul>"},{"location":"architecture/services/#indexing-service","title":"Indexing Service","text":"<p>Primary Role: Data persistence and search indexing</p> <p>Key Responsibilities:</p> <ul> <li>Store video and channel metadata in MongoDB</li> <li>Provide data enrichment through transcript download</li> <li>Create and maintain search indices in Elasticsearch</li> <li>Update channel statistics based on video notifications</li> <li>Manage data consistency across storage systems</li> </ul> <p>Technologies:</p> <ul> <li>Motor (async MongoDB driver) for document operations</li> <li>AsyncElasticsearch for search index management</li> <li>Python asyncio for concurrent database operations</li> </ul>"},{"location":"documentation/api/","title":"API","text":""},{"location":"documentation/api/#ytindexer.indexer.ChannelStatsService","title":"<code>ChannelStatsService</code>","text":"<p>               Bases: <code>HealthCheckable</code></p> <p>Handles channel statistics updates</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/stats.py</code> <pre><code>class ChannelStatsService(HealthCheckable):\n    \"\"\"Handles channel statistics updates\"\"\"\n\n    def __init__(self, client: Any, config: MongoDBConfig, retry_config: RetryConfig):\n        self.client = client\n        self.config = config\n        self.retry = RetryableOperation(retry_config)\n        self.db = self.client[config.database_name]\n        self.channels_collection = self.db[config.channels_collection]\n        logger.info(\"Initialized ChannelStatsService\")\n\n    async def ensure_indices(self) -&gt; OperationResult:\n        \"\"\"Ensure the required database indices exist\"\"\"\n        created_indexes = []\n        failed_indexes = []\n\n        for index_name, index_config in self.config.channel_indexes.items():\n            try:\n                await self.channels_collection.create_index(index_name, **index_config)\n                created_indexes.append(index_name)\n                logger.debug(f\"Created index: {index_name}\")\n            except OperationFailure as e:\n                if \"already exists\" in str(e):\n                    logger.debug(f\"Index '{index_name}' already exists\")\n                    created_indexes.append(index_name)\n                else:\n                    failed_indexes.append(index_name)\n                    logger.error(f\"Failed to create index {index_name}: {str(e)}\")\n\n        if failed_indexes:\n            return OperationResult.failure(\n                f\"Failed to create indexes: {failed_indexes}\",\n                metadata={\"created\": created_indexes, \"failed\": failed_indexes},\n            )\n        else:\n            return OperationResult.success(\n                f\"Ensured indexes: {created_indexes}\",\n                metadata={\"indexes\": created_indexes},\n            )\n\n    async def update_channel_stats(self, video_data: Dict[str, Any]) -&gt; OperationResult:\n        \"\"\"Update channel statistics based on video data with retry logic\"\"\"\n        channel_id = video_data.get(\"channel_id\")\n        if not channel_id:\n            return OperationResult.failure(\"Video data missing channel_id\")\n\n        async def _update_operation():\n            result = await self.channels_collection.update_one(\n                {\"channel_id\": channel_id},\n                {\n                    \"$inc\": {\"video_count\": 1},\n                    \"$set\": {\"last_activity\": datetime.now(timezone.utc)},\n                    \"$setOnInsert\": {\"first_seen\": datetime.now(timezone.utc)},\n                },\n                upsert=True,\n            )\n            return result\n\n        try:\n            result = await self.retry.execute(\n                _update_operation, f\"update_channel_stats_{channel_id}\"\n            )\n            logger.debug(f\"Updated channel stats: {channel_id}\")\n\n            action = \"updated\" if result.matched_count &gt; 0 else \"created\"\n            return OperationResult.success(\n                f\"Channel stats {action}: {channel_id}\",\n                metadata={\"channel_id\": channel_id, \"action\": action},\n            )\n        except Exception as e:\n            logger.error(f\"Failed to update channel stats: {str(e)}\")\n            logger.debug(traceback.format_exc())\n            return OperationResult.failure(\n                f\"Failed to update channel stats: {str(e)}\", e\n            )\n\n    async def health_check(self) -&gt; HealthStatus:\n        \"\"\"Check MongoDB connection health for channels collection\"\"\"\n        start_time = time.time()\n        try:\n            # Test access to channels collection\n            await self.channels_collection.count_documents({}, limit=1)\n            response_time = (time.time() - start_time) * 1000\n\n            return HealthStatus(\n                service_name=\"mongodb_channels\",\n                is_healthy=True,\n                response_time_ms=response_time,\n                message=\"Channels collection accessible\",\n            )\n        except Exception as e:\n            response_time = (time.time() - start_time) * 1000\n            return HealthStatus(\n                service_name=\"mongodb_channels\",\n                is_healthy=False,\n                response_time_ms=response_time,\n                message=f\"Health check failed: {str(e)}\",\n            )\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.ChannelStatsService.ensure_indices","title":"<code>ensure_indices()</code>  <code>async</code>","text":"<p>Ensure the required database indices exist</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/stats.py</code> <pre><code>async def ensure_indices(self) -&gt; OperationResult:\n    \"\"\"Ensure the required database indices exist\"\"\"\n    created_indexes = []\n    failed_indexes = []\n\n    for index_name, index_config in self.config.channel_indexes.items():\n        try:\n            await self.channels_collection.create_index(index_name, **index_config)\n            created_indexes.append(index_name)\n            logger.debug(f\"Created index: {index_name}\")\n        except OperationFailure as e:\n            if \"already exists\" in str(e):\n                logger.debug(f\"Index '{index_name}' already exists\")\n                created_indexes.append(index_name)\n            else:\n                failed_indexes.append(index_name)\n                logger.error(f\"Failed to create index {index_name}: {str(e)}\")\n\n    if failed_indexes:\n        return OperationResult.failure(\n            f\"Failed to create indexes: {failed_indexes}\",\n            metadata={\"created\": created_indexes, \"failed\": failed_indexes},\n        )\n    else:\n        return OperationResult.success(\n            f\"Ensured indexes: {created_indexes}\",\n            metadata={\"indexes\": created_indexes},\n        )\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.ChannelStatsService.health_check","title":"<code>health_check()</code>  <code>async</code>","text":"<p>Check MongoDB connection health for channels collection</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/stats.py</code> <pre><code>async def health_check(self) -&gt; HealthStatus:\n    \"\"\"Check MongoDB connection health for channels collection\"\"\"\n    start_time = time.time()\n    try:\n        # Test access to channels collection\n        await self.channels_collection.count_documents({}, limit=1)\n        response_time = (time.time() - start_time) * 1000\n\n        return HealthStatus(\n            service_name=\"mongodb_channels\",\n            is_healthy=True,\n            response_time_ms=response_time,\n            message=\"Channels collection accessible\",\n        )\n    except Exception as e:\n        response_time = (time.time() - start_time) * 1000\n        return HealthStatus(\n            service_name=\"mongodb_channels\",\n            is_healthy=False,\n            response_time_ms=response_time,\n            message=f\"Health check failed: {str(e)}\",\n        )\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.ChannelStatsService.update_channel_stats","title":"<code>update_channel_stats(video_data)</code>  <code>async</code>","text":"<p>Update channel statistics based on video data with retry logic</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/stats.py</code> <pre><code>async def update_channel_stats(self, video_data: Dict[str, Any]) -&gt; OperationResult:\n    \"\"\"Update channel statistics based on video data with retry logic\"\"\"\n    channel_id = video_data.get(\"channel_id\")\n    if not channel_id:\n        return OperationResult.failure(\"Video data missing channel_id\")\n\n    async def _update_operation():\n        result = await self.channels_collection.update_one(\n            {\"channel_id\": channel_id},\n            {\n                \"$inc\": {\"video_count\": 1},\n                \"$set\": {\"last_activity\": datetime.now(timezone.utc)},\n                \"$setOnInsert\": {\"first_seen\": datetime.now(timezone.utc)},\n            },\n            upsert=True,\n        )\n        return result\n\n    try:\n        result = await self.retry.execute(\n            _update_operation, f\"update_channel_stats_{channel_id}\"\n        )\n        logger.debug(f\"Updated channel stats: {channel_id}\")\n\n        action = \"updated\" if result.matched_count &gt; 0 else \"created\"\n        return OperationResult.success(\n            f\"Channel stats {action}: {channel_id}\",\n            metadata={\"channel_id\": channel_id, \"action\": action},\n        )\n    except Exception as e:\n        logger.error(f\"Failed to update channel stats: {str(e)}\")\n        logger.debug(traceback.format_exc())\n        return OperationResult.failure(\n            f\"Failed to update channel stats: {str(e)}\", e\n        )\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.ElasticsearchConfig","title":"<code>ElasticsearchConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Elasticsearch indexing and search operations.</p> <p>This class manages Elasticsearch-specific settings including index configuration, field mappings, and analysis settings. It provides a computed mapping property that generates the complete index configuration based on the configured parameters.</p> <p>The mapping includes optimized field types for video metadata, proper analyzers for text search, and index settings for performance tuning.</p> <p>Attributes:</p> Name Type Description <code>index_name</code> <code>str</code> <p>Name of the Elasticsearch index for storing video documents.</p> <code>shards</code> <code>int</code> <p>Number of primary shards for the index. More shards allow better distribution across nodes but increase overhead.</p> <code>replicas</code> <code>int</code> <p>Number of replica shards for each primary shard. Replicas provide redundancy and can improve search throughput.</p> Example <p>config = ElasticsearchConfig( ...     index_name=\"videos_production\", ...     shards=3, ...     replicas=2 ... )</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/config.py</code> <pre><code>@dataclass\nclass ElasticsearchConfig:\n    \"\"\"Configuration for Elasticsearch indexing and search operations.\n\n    This class manages Elasticsearch-specific settings including index configuration,\n    field mappings, and analysis settings. It provides a computed mapping property\n    that generates the complete index configuration based on the configured parameters.\n\n    The mapping includes optimized field types for video metadata, proper analyzers\n    for text search, and index settings for performance tuning.\n\n    Attributes:\n        index_name: Name of the Elasticsearch index for storing video documents.\n        shards: Number of primary shards for the index. More shards allow better\n            distribution across nodes but increase overhead.\n        replicas: Number of replica shards for each primary shard. Replicas provide\n            redundancy and can improve search throughput.\n\n    Example:\n        &gt;&gt;&gt; config = ElasticsearchConfig(\n        ...     index_name=\"videos_production\",\n        ...     shards=3,\n        ...     replicas=2\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Create index with the computed mapping\n        &gt;&gt;&gt; es_client.indices.create(\n        ...     index=config.index_name,\n        ...     body=config.mapping\n        ... )\n    \"\"\"\n\n    index_name: str = \"videos\"\n    shards: int = 1\n    replicas: int = 0\n\n    @property\n    def mapping(self) -&gt; Dict[str, Any]:\n        \"\"\"Generate the complete Elasticsearch index mapping and settings.\n\n        Creates a comprehensive mapping configuration that defines how video\n        documents are indexed and stored. The mapping includes:\n\n        - Keyword fields for exact matching (video_id, channel_id, tags)\n        - Text fields with standard analyzer for full-text search\n        - Multi-field configurations for both search and aggregations\n        - Appropriate data types for metrics and timestamps\n        - Index settings based on configured shard and replica counts\n\n        Returns:\n            Dictionary containing the complete Elasticsearch mapping configuration\n            with both field mappings and index settings.\n\n        Note:\n            The mapping is generated dynamically based on current attribute values,\n            so changes to shards or replicas will be reflected in subsequent calls.\n\n        Example:\n            &gt;&gt;&gt; config = ElasticsearchConfig(shards=2, replicas=1)\n            &gt;&gt;&gt; mapping = config.mapping\n            &gt;&gt;&gt; print(mapping[\"settings\"][\"number_of_shards\"])  # 2\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Text fields support both search and keyword aggregations\n            &gt;&gt;&gt; title_mapping = mapping[\"mappings\"][\"properties\"][\"title\"]\n            &gt;&gt;&gt; print(title_mapping[\"type\"])  # \"text\"\n            &gt;&gt;&gt; print(title_mapping[\"fields\"][\"keyword\"][\"type\"])  # \"keyword\"\n        \"\"\"\n        return {\n            \"mappings\": {\n                \"properties\": {\n                    \"video_id\": {\"type\": \"keyword\"},\n                    \"channel_id\": {\"type\": \"keyword\"},\n                    \"title\": {\n                        \"type\": \"text\",\n                        \"analyzer\": \"standard\",\n                        \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n                    },\n                    \"description\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n                    \"published\": {\"type\": \"date\"},\n                    \"updated\": {\"type\": \"date\"},\n                    \"author\": {\n                        \"type\": \"text\",\n                        \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n                    },\n                    \"tags\": {\"type\": \"keyword\"},\n                    \"categories\": {\"type\": \"keyword\"},\n                    \"duration\": {\"type\": \"integer\"},\n                    \"view_count\": {\"type\": \"long\"},\n                    \"like_count\": {\"type\": \"long\"},\n                    \"comment_count\": {\"type\": \"long\"},\n                    \"processed_at\": {\"type\": \"date\"},\n                }\n            },\n            \"settings\": {\n                \"number_of_shards\": self.shards,\n                \"number_of_replicas\": self.replicas,\n            },\n        }\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.ElasticsearchConfig--create-index-with-the-computed-mapping","title":"Create index with the computed mapping","text":"<p>es_client.indices.create( ...     index=config.index_name, ...     body=config.mapping ... )</p>"},{"location":"documentation/api/#ytindexer.indexer.ElasticsearchConfig.mapping","title":"<code>mapping</code>  <code>property</code>","text":"<p>Generate the complete Elasticsearch index mapping and settings.</p> <p>Creates a comprehensive mapping configuration that defines how video documents are indexed and stored. The mapping includes:</p> <ul> <li>Keyword fields for exact matching (video_id, channel_id, tags)</li> <li>Text fields with standard analyzer for full-text search</li> <li>Multi-field configurations for both search and aggregations</li> <li>Appropriate data types for metrics and timestamps</li> <li>Index settings based on configured shard and replica counts</li> </ul> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing the complete Elasticsearch mapping configuration</p> <code>Dict[str, Any]</code> <p>with both field mappings and index settings.</p> Note <p>The mapping is generated dynamically based on current attribute values, so changes to shards or replicas will be reflected in subsequent calls.</p> Example <p>config = ElasticsearchConfig(shards=2, replicas=1) mapping = config.mapping print(mapping[\"settings\"][\"number_of_shards\"])  # 2</p>"},{"location":"documentation/api/#ytindexer.indexer.ElasticsearchConfig.mapping--text-fields-support-both-search-and-keyword-aggregations","title":"Text fields support both search and keyword aggregations","text":"<p>title_mapping = mapping[\"mappings\"][\"properties\"][\"title\"] print(title_mapping[\"type\"])  # \"text\" print(title_mapping[\"fields\"][\"keyword\"][\"type\"])  # \"keyword\"</p>"},{"location":"documentation/api/#ytindexer.indexer.MongoDBConfig","title":"<code>MongoDBConfig</code>  <code>dataclass</code>","text":"<p>Configuration for MongoDB collections and database indexes.</p> <p>This class manages MongoDB-specific configuration including database and collection names, as well as index definitions for optimal query performance. It provides computed properties that generate index configurations for different collection types.</p> <p>The index configurations are optimized for common query patterns including lookups by ID, filtering by channel, date-based queries, and subscription management operations.</p> <p>Attributes:</p> Name Type Description <code>database_name</code> <code>str</code> <p>Name of the MongoDB database containing the collections.</p> <code>videos_collection</code> <code>str</code> <p>Name of the collection storing video metadata documents.</p> <code>channels_collection</code> <code>str</code> <p>Name of the collection storing channel information.</p> Example <p>config = MongoDBConfig( ...     database_name=\"video_platform\", ...     videos_collection=\"video_metadata\", ...     channels_collection=\"channel_data\" ... )</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/config.py</code> <pre><code>@dataclass\nclass MongoDBConfig:\n    \"\"\"Configuration for MongoDB collections and database indexes.\n\n    This class manages MongoDB-specific configuration including database and\n    collection names, as well as index definitions for optimal query performance.\n    It provides computed properties that generate index configurations for\n    different collection types.\n\n    The index configurations are optimized for common query patterns including\n    lookups by ID, filtering by channel, date-based queries, and subscription\n    management operations.\n\n    Attributes:\n        database_name: Name of the MongoDB database containing the collections.\n        videos_collection: Name of the collection storing video metadata documents.\n        channels_collection: Name of the collection storing channel information.\n\n    Example:\n        &gt;&gt;&gt; config = MongoDBConfig(\n        ...     database_name=\"video_platform\",\n        ...     videos_collection=\"video_metadata\",\n        ...     channels_collection=\"channel_data\"\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Create indexes for optimal query performance\n        &gt;&gt;&gt; db = mongo_client[config.database_name]\n        &gt;&gt;&gt; videos = db[config.videos_collection]\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; for index_config in config.video_indexes.values():\n        ...     videos.create_index(index_config[\"key\"], **index_config)\n    \"\"\"\n\n    database_name: str = \"mongo\"\n    videos_collection: str = \"videos\"\n    channels_collection: str = \"channels\"\n\n    @property\n    def video_indexes(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Generate index configurations for the videos collection.\n\n        Creates index definitions optimized for common video query patterns\n        including unique video lookups, channel-based filtering, and date-based\n        sorting and filtering operations.\n\n        Returns:\n            Dictionary mapping index names to their MongoDB index configurations.\n            Each configuration includes the key specification, uniqueness constraint,\n            and index name.\n\n        Note:\n            The video_id index enforces uniqueness to prevent duplicate video\n            documents, while other indexes are non-unique to support filtering\n            and sorting operations.\n\n        Example:\n            &gt;&gt;&gt; config = MongoDBConfig()\n            &gt;&gt;&gt; indexes = config.video_indexes\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Unique index for video lookups\n            &gt;&gt;&gt; video_id_idx = indexes[\"video_id\"]\n            &gt;&gt;&gt; print(video_id_idx[\"unique\"])  # True\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Non-unique index for channel filtering\n            &gt;&gt;&gt; channel_idx = indexes[\"channel_id_non\"]\n            &gt;&gt;&gt; print(channel_idx[\"unique\"])  # False\n        \"\"\"\n        return {\n            \"video_id\": {\n                \"key\": [(\"video_id\", 1)],\n                \"unique\": True,\n                \"name\": \"video_id_idx\",\n            },\n            \"channel_id_non\": {\n                \"key\": [(\"channel_id\", 1)],\n                \"unique\": False,\n                \"name\": \"channel_id_non_idx\",\n            },\n            \"published_non\": {\n                \"key\": [(\"published\", 1)],\n                \"unique\": False,\n                \"name\": \"published_non_idx\",\n            },\n        }\n\n    @property\n    def channel_indexes(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Generate index configurations for the channels collection.\n\n        Creates index definitions for channel-related queries, primarily focused\n        on unique channel identification and lookups.\n\n        Returns:\n            Dictionary mapping index names to their MongoDB index configurations\n            for the channels collection.\n\n        Example:\n            &gt;&gt;&gt; config = MongoDBConfig()\n            &gt;&gt;&gt; channel_indexes = config.channel_indexes\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Unique index for channel identification\n            &gt;&gt;&gt; channel_idx = channel_indexes[\"channel_id\"]\n            &gt;&gt;&gt; print(channel_idx[\"unique\"])  # True\n        \"\"\"\n        return {\n            \"channel_id\": {\n                \"key\": [(\"channel_id\", 1)],\n                \"unique\": True,\n                \"name\": \"channel_id_idx\",\n            }\n        }\n\n    @property\n    def subscription_indexes(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Generate index configurations for the subscriptions collection.\n\n        Creates index definitions optimized for subscription management operations\n        including unique subscription lookups, expiration queries, and active\n        subscription filtering. Includes a compound index for efficient queries\n        on expiring active subscriptions.\n\n        Returns:\n            Dictionary mapping index names to their MongoDB index configurations\n            for subscription management.\n\n        Note:\n            The compound index on expires_at and is_active enables efficient\n            queries for finding subscriptions that need renewal, which is a\n            common operation in subscription management workflows.\n\n        Example:\n            &gt;&gt;&gt; config = MongoDBConfig()\n            &gt;&gt;&gt; sub_indexes = config.subscription_indexes\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Unique constraint on channel subscriptions\n            &gt;&gt;&gt; channel_idx = sub_indexes[\"channel_id\"]\n            &gt;&gt;&gt; print(channel_idx[\"unique\"])  # True\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Compound index for expiration queries\n            &gt;&gt;&gt; compound_idx = sub_indexes[\"expires_at_active\"]\n            &gt;&gt;&gt; print(compound_idx)  # [(\"expires_at\", 1), (\"is_active\", 1)]\n        \"\"\"\n        return {\n            \"channel_id\": {\"unique\": True},\n            \"expires_at\": {},\n            \"is_active\": {},\n            \"expires_at_active\": [(\"expires_at\", 1), (\"is_active\", 1)],\n        }\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.MongoDBConfig--create-indexes-for-optimal-query-performance","title":"Create indexes for optimal query performance","text":"<p>db = mongo_client[config.database_name] videos = db[config.videos_collection]</p> <p>for index_config in config.video_indexes.values(): ...     videos.create_index(index_config[\"key\"], **index_config)</p>"},{"location":"documentation/api/#ytindexer.indexer.MongoDBConfig.channel_indexes","title":"<code>channel_indexes</code>  <code>property</code>","text":"<p>Generate index configurations for the channels collection.</p> <p>Creates index definitions for channel-related queries, primarily focused on unique channel identification and lookups.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping index names to their MongoDB index configurations</p> <code>Dict[str, Dict[str, Any]]</code> <p>for the channels collection.</p> Example <p>config = MongoDBConfig() channel_indexes = config.channel_indexes</p>"},{"location":"documentation/api/#ytindexer.indexer.MongoDBConfig.channel_indexes--unique-index-for-channel-identification","title":"Unique index for channel identification","text":"<p>channel_idx = channel_indexes[\"channel_id\"] print(channel_idx[\"unique\"])  # True</p>"},{"location":"documentation/api/#ytindexer.indexer.MongoDBConfig.subscription_indexes","title":"<code>subscription_indexes</code>  <code>property</code>","text":"<p>Generate index configurations for the subscriptions collection.</p> <p>Creates index definitions optimized for subscription management operations including unique subscription lookups, expiration queries, and active subscription filtering. Includes a compound index for efficient queries on expiring active subscriptions.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping index names to their MongoDB index configurations</p> <code>Dict[str, Dict[str, Any]]</code> <p>for subscription management.</p> Note <p>The compound index on expires_at and is_active enables efficient queries for finding subscriptions that need renewal, which is a common operation in subscription management workflows.</p> Example <p>config = MongoDBConfig() sub_indexes = config.subscription_indexes</p>"},{"location":"documentation/api/#ytindexer.indexer.MongoDBConfig.subscription_indexes--unique-constraint-on-channel-subscriptions","title":"Unique constraint on channel subscriptions","text":"<p>channel_idx = sub_indexes[\"channel_id\"] print(channel_idx[\"unique\"])  # True</p>"},{"location":"documentation/api/#ytindexer.indexer.MongoDBConfig.subscription_indexes--compound-index-for-expiration-queries","title":"Compound index for expiration queries","text":"<p>compound_idx = sub_indexes[\"expires_at_active\"] print(compound_idx)  # [(\"expires_at\", 1), (\"is_active\", 1)]</p>"},{"location":"documentation/api/#ytindexer.indexer.MongoDBConfig.video_indexes","title":"<code>video_indexes</code>  <code>property</code>","text":"<p>Generate index configurations for the videos collection.</p> <p>Creates index definitions optimized for common video query patterns including unique video lookups, channel-based filtering, and date-based sorting and filtering operations.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping index names to their MongoDB index configurations.</p> <code>Dict[str, Dict[str, Any]]</code> <p>Each configuration includes the key specification, uniqueness constraint,</p> <code>Dict[str, Dict[str, Any]]</code> <p>and index name.</p> Note <p>The video_id index enforces uniqueness to prevent duplicate video documents, while other indexes are non-unique to support filtering and sorting operations.</p> Example <p>config = MongoDBConfig() indexes = config.video_indexes</p>"},{"location":"documentation/api/#ytindexer.indexer.MongoDBConfig.video_indexes--unique-index-for-video-lookups","title":"Unique index for video lookups","text":"<p>video_id_idx = indexes[\"video_id\"] print(video_id_idx[\"unique\"])  # True</p>"},{"location":"documentation/api/#ytindexer.indexer.MongoDBConfig.video_indexes--non-unique-index-for-channel-filtering","title":"Non-unique index for channel filtering","text":"<p>channel_idx = indexes[\"channel_id_non\"] print(channel_idx[\"unique\"])  # False</p>"},{"location":"documentation/api/#ytindexer.indexer.RetryConfig","title":"<code>RetryConfig</code>  <code>dataclass</code>","text":"<p>Configuration for retry logic with exponential backoff.</p> <p>This class defines parameters for implementing robust retry mechanisms with exponential backoff to handle transient failures gracefully. The configuration controls retry attempts, timing, and backoff behavior.</p> <p>The exponential backoff algorithm increases delays between retry attempts to reduce load on failing systems and improve the likelihood of eventual success. The max_delay parameter prevents delays from becoming excessive.</p> <p>Attributes:</p> Name Type Description <code>max_attempts</code> <code>int</code> <p>Maximum number of retry attempts before giving up. Includes the initial attempt, so max_attempts=3 means 1 initial attempt plus 2 retries.</p> <code>base_delay</code> <code>float</code> <p>Initial delay in seconds before the first retry attempt. Subsequent delays are calculated using exponential backoff.</p> <code>max_delay</code> <code>float</code> <p>Maximum delay in seconds between retry attempts. Prevents exponential backoff from creating excessively long delays.</p> <code>exponential_base</code> <code>float</code> <p>Base for exponential backoff calculation. Common values are 2.0 (doubling) or 1.5 (50% increase per attempt).</p> Example Note <p>Actual delays include random jitter to prevent thundering herd problems when multiple clients retry simultaneously. The jitter is typically 10-30% of the calculated delay.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/config.py</code> <pre><code>@dataclass\nclass RetryConfig:\n    \"\"\"Configuration for retry logic with exponential backoff.\n\n    This class defines parameters for implementing robust retry mechanisms\n    with exponential backoff to handle transient failures gracefully. The\n    configuration controls retry attempts, timing, and backoff behavior.\n\n    The exponential backoff algorithm increases delays between retry attempts\n    to reduce load on failing systems and improve the likelihood of eventual\n    success. The max_delay parameter prevents delays from becoming excessive.\n\n    Attributes:\n        max_attempts: Maximum number of retry attempts before giving up.\n            Includes the initial attempt, so max_attempts=3 means 1 initial\n            attempt plus 2 retries.\n        base_delay: Initial delay in seconds before the first retry attempt.\n            Subsequent delays are calculated using exponential backoff.\n        max_delay: Maximum delay in seconds between retry attempts. Prevents\n            exponential backoff from creating excessively long delays.\n        exponential_base: Base for exponential backoff calculation. Common\n            values are 2.0 (doubling) or 1.5 (50% increase per attempt).\n\n    Example:\n        &gt;&gt;&gt; # Conservative retry configuration\n        &gt;&gt;&gt; config = RetryConfig(\n        ...     max_attempts=3,\n        ...     base_delay=1.0,\n        ...     max_delay=30.0,\n        ...     exponential_base=2.0\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Aggressive retry for critical operations\n        &gt;&gt;&gt; critical_config = RetryConfig(\n        ...     max_attempts=10,\n        ...     base_delay=0.5,\n        ...     max_delay=120.0,\n        ...     exponential_base=1.5\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Delay calculation example:\n        &gt;&gt;&gt; # Attempt 1: base_delay * (exponential_base ^ 0) = 1.0 * 1 = 1.0s\n        &gt;&gt;&gt; # Attempt 2: base_delay * (exponential_base ^ 1) = 1.0 * 2 = 2.0s  \n        &gt;&gt;&gt; # Attempt 3: base_delay * (exponential_base ^ 2) = 1.0 * 4 = 4.0s\n\n    Note:\n        Actual delays include random jitter to prevent thundering herd problems\n        when multiple clients retry simultaneously. The jitter is typically\n        10-30% of the calculated delay.\n    \"\"\"\n\n    max_attempts: int = 3\n    base_delay: float = 1.0\n    max_delay: float = 60.0\n    exponential_base: float = 2.0\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.RetryConfig--conservative-retry-configuration","title":"Conservative retry configuration","text":"<p>config = RetryConfig( ...     max_attempts=3, ...     base_delay=1.0, ...     max_delay=30.0, ...     exponential_base=2.0 ... )</p>"},{"location":"documentation/api/#ytindexer.indexer.RetryConfig--aggressive-retry-for-critical-operations","title":"Aggressive retry for critical operations","text":"<p>critical_config = RetryConfig( ...     max_attempts=10, ...     base_delay=0.5, ...     max_delay=120.0, ...     exponential_base=1.5 ... )</p>"},{"location":"documentation/api/#ytindexer.indexer.RetryConfig--delay-calculation-example","title":"Delay calculation example:","text":""},{"location":"documentation/api/#ytindexer.indexer.RetryConfig--attempt-1-base_delay-exponential_base-0-10-1-10s","title":"Attempt 1: base_delay * (exponential_base ^ 0) = 1.0 * 1 = 1.0s","text":""},{"location":"documentation/api/#ytindexer.indexer.RetryConfig--attempt-2-base_delay-exponential_base-1-10-2-20s","title":"Attempt 2: base_delay * (exponential_base ^ 1) = 1.0 * 2 = 2.0s","text":""},{"location":"documentation/api/#ytindexer.indexer.RetryConfig--attempt-3-base_delay-exponential_base-2-10-4-40s","title":"Attempt 3: base_delay * (exponential_base ^ 2) = 1.0 * 4 = 4.0s","text":""},{"location":"documentation/api/#ytindexer.indexer.SearchIndexingService","title":"<code>SearchIndexingService</code>","text":"<p>               Bases: <code>HealthCheckable</code></p> <p>Handles video search indexing in Elasticsearch</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/indexing.py</code> <pre><code>class SearchIndexingService(HealthCheckable):\n    \"\"\"Handles video search indexing in Elasticsearch\"\"\"\n\n    def __init__(\n        self, client: Any, config: ElasticsearchConfig, retry_config: RetryConfig\n    ):\n        self.client = client\n        self.config = config\n        self.retry = RetryableOperation(retry_config)\n        logger.info(\"Initialized SearchIndexingService\")\n\n    async def ensure_index(self) -&gt; OperationResult:\n        \"\"\"Create Elasticsearch index if it doesn't exist\"\"\"\n        try:\n            if not await self.client.indices.exists(index=self.config.index_name):\n                await self.client.indices.create(\n                    index=self.config.index_name, body=self.config.mapping\n                )\n                logger.info(f\"Created Elasticsearch index: {self.config.index_name}\")\n                return OperationResult.success(\n                    f\"Created index: {self.config.index_name}\"\n                )\n            else:\n                return OperationResult.success(\n                    f\"Index already exists: {self.config.index_name}\"\n                )\n        except Exception as e:\n            logger.error(f\"Failed to ensure index: {str(e)}\")\n            return OperationResult.failure(f\"Failed to ensure index: {str(e)}\", e)\n\n    async def index_video(self, video_data: Dict[str, Any]) -&gt; OperationResult:\n        \"\"\"Index video metadata in Elasticsearch with retry logic\"\"\"\n        video_id = video_data.get(\"video_id\")\n        if not video_id:\n            return OperationResult.failure(\"Video data missing video_id\")\n\n        async def _index_operation():\n            await self.client.index(\n                index=self.config.index_name, id=video_id, body=video_data, refresh=True\n            )\n            return video_id\n\n        try:\n            result_id = await self.retry.execute(\n                _index_operation, f\"index_video_{video_id}\"\n            )\n            logger.debug(f\"Indexed video in Elasticsearch: {result_id}\")\n            return OperationResult.success(\n                f\"Indexed video: {result_id}\", metadata={\"video_id\": result_id}\n            )\n        except Exception as e:\n            logger.error(f\"Failed to index video in Elasticsearch: {str(e)}\")\n            logger.debug(traceback.format_exc())\n            return OperationResult.failure(f\"Failed to index video: {str(e)}\", e)\n\n    async def health_check(self) -&gt; HealthStatus:\n        \"\"\"Check Elasticsearch cluster health\"\"\"\n        start_time = time.time()\n        try:\n            health = await self.client.cluster.health()\n            response_time = (time.time() - start_time) * 1000\n\n            is_healthy = health.get(\"status\") in [\"green\", \"yellow\"]\n            message = f\"Cluster status: {health.get('status', 'unknown')}\"\n\n            return HealthStatus(\n                service_name=\"elasticsearch\",\n                is_healthy=is_healthy,\n                response_time_ms=response_time,\n                message=message,\n                metadata=health,\n            )\n        except Exception as e:\n            response_time = (time.time() - start_time) * 1000\n            return HealthStatus(\n                service_name=\"elasticsearch\",\n                is_healthy=False,\n                response_time_ms=response_time,\n                message=f\"Health check failed: {str(e)}\",\n            )\n\n    async def close(self):\n        \"\"\"Close the Elasticsearch client\"\"\"\n        await self.client.close()\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.SearchIndexingService.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the Elasticsearch client</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/indexing.py</code> <pre><code>async def close(self):\n    \"\"\"Close the Elasticsearch client\"\"\"\n    await self.client.close()\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.SearchIndexingService.ensure_index","title":"<code>ensure_index()</code>  <code>async</code>","text":"<p>Create Elasticsearch index if it doesn't exist</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/indexing.py</code> <pre><code>async def ensure_index(self) -&gt; OperationResult:\n    \"\"\"Create Elasticsearch index if it doesn't exist\"\"\"\n    try:\n        if not await self.client.indices.exists(index=self.config.index_name):\n            await self.client.indices.create(\n                index=self.config.index_name, body=self.config.mapping\n            )\n            logger.info(f\"Created Elasticsearch index: {self.config.index_name}\")\n            return OperationResult.success(\n                f\"Created index: {self.config.index_name}\"\n            )\n        else:\n            return OperationResult.success(\n                f\"Index already exists: {self.config.index_name}\"\n            )\n    except Exception as e:\n        logger.error(f\"Failed to ensure index: {str(e)}\")\n        return OperationResult.failure(f\"Failed to ensure index: {str(e)}\", e)\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.SearchIndexingService.health_check","title":"<code>health_check()</code>  <code>async</code>","text":"<p>Check Elasticsearch cluster health</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/indexing.py</code> <pre><code>async def health_check(self) -&gt; HealthStatus:\n    \"\"\"Check Elasticsearch cluster health\"\"\"\n    start_time = time.time()\n    try:\n        health = await self.client.cluster.health()\n        response_time = (time.time() - start_time) * 1000\n\n        is_healthy = health.get(\"status\") in [\"green\", \"yellow\"]\n        message = f\"Cluster status: {health.get('status', 'unknown')}\"\n\n        return HealthStatus(\n            service_name=\"elasticsearch\",\n            is_healthy=is_healthy,\n            response_time_ms=response_time,\n            message=message,\n            metadata=health,\n        )\n    except Exception as e:\n        response_time = (time.time() - start_time) * 1000\n        return HealthStatus(\n            service_name=\"elasticsearch\",\n            is_healthy=False,\n            response_time_ms=response_time,\n            message=f\"Health check failed: {str(e)}\",\n        )\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.SearchIndexingService.index_video","title":"<code>index_video(video_data)</code>  <code>async</code>","text":"<p>Index video metadata in Elasticsearch with retry logic</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/indexing.py</code> <pre><code>async def index_video(self, video_data: Dict[str, Any]) -&gt; OperationResult:\n    \"\"\"Index video metadata in Elasticsearch with retry logic\"\"\"\n    video_id = video_data.get(\"video_id\")\n    if not video_id:\n        return OperationResult.failure(\"Video data missing video_id\")\n\n    async def _index_operation():\n        await self.client.index(\n            index=self.config.index_name, id=video_id, body=video_data, refresh=True\n        )\n        return video_id\n\n    try:\n        result_id = await self.retry.execute(\n            _index_operation, f\"index_video_{video_id}\"\n        )\n        logger.debug(f\"Indexed video in Elasticsearch: {result_id}\")\n        return OperationResult.success(\n            f\"Indexed video: {result_id}\", metadata={\"video_id\": result_id}\n        )\n    except Exception as e:\n        logger.error(f\"Failed to index video in Elasticsearch: {str(e)}\")\n        logger.debug(traceback.format_exc())\n        return OperationResult.failure(f\"Failed to index video: {str(e)}\", e)\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.VideoIndexingProcessor","title":"<code>VideoIndexingProcessor</code>","text":"<p>               Bases: <code>HealthCheckable</code></p> <p>Orchestrates the video indexing process with enhanced error handling and health checks</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/processor.py</code> <pre><code>class VideoIndexingProcessor(HealthCheckable):\n    \"\"\"Orchestrates the video indexing process with enhanced error handling and health checks\"\"\"\n\n    def __init__(\n        self,\n        input_queue: Queue,\n        video_storage: VideoStorageService,\n        search_indexing: SearchIndexingService,\n        channel_stats: ChannelStatsService,\n        transcript_service: VideoTranscriptService,\n        max_concurrent_tasks: int = 10,\n        poll_interval: float = 1.0,\n    ):\n        self.input_queue = input_queue\n        self.video_storage = video_storage\n        self.search_indexing = search_indexing\n        self.channel_stats = channel_stats\n        self.transcript_service = transcript_service\n        self.max_concurrent_tasks = max_concurrent_tasks\n        self.poll_interval = poll_interval\n        self._running = False\n        self._shutdown_event = asyncio.Event()\n        self._active_tasks: set = set()\n        logger.info(\"Initialized VideoIndexingProcessor\")\n\n    async def ensure_indices(self) -&gt; OperationResult:\n        \"\"\"Ensure all required database indices and mappings exist\"\"\"\n        results = []\n\n        storage_result = await self.video_storage.ensure_indices()\n        results.append((\"storage\", storage_result))\n\n        indexing_result = await self.search_indexing.ensure_index()\n        results.append((\"indexing\", indexing_result))\n\n        stats_result = await self.channel_stats.ensure_indices()\n        results.append((\"stats\", stats_result))\n\n        logger.info(results)\n\n        failed_services = [name for name, result in results if result.is_failure]\n\n        if failed_services:\n            return OperationResult.failure(\n                f\"Failed to ensure indices for: {failed_services}\",\n                metadata={\n                    \"results\": {name: result.message for name, result in results}\n                },\n            )\n        else:\n            return OperationResult.success(\n                \"All indices ensured successfully\",\n                metadata={\n                    \"results\": {name: result.message for name, result in results}\n                },\n            )\n\n    async def _enrich_video_with_transcript(\n        self, video_data: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Enrich video data with transcript information\"\"\"\n        video_id = video_data.get(\"video_id\")\n        if not video_id:\n            logger.warning(\n                \"Video data missing video_id, skipping transcript enrichment\"\n            )\n            return video_data\n\n        try:\n            # Get transcript in a thread pool to avoid blocking\n            loop = asyncio.get_event_loop()\n            transcript = await loop.run_in_executor(\n                None, self.transcript_service.get_transcript, video_id\n            )\n\n            if transcript:\n                video_data[\"transcript\"] = transcript\n                logger.debug(f\"Added transcript to video {video_id}\")\n            else:\n                logger.debug(f\"No transcript available for video {video_id}\")\n                video_data[\"transcript\"] = None\n\n        except Exception as e:\n            logger.warning(f\"Failed to get transcript for video {video_id}: {e}\")\n            video_data[\"transcript\"] = None\n\n        return video_data\n\n    async def process_video(self, video_data: Dict[str, Any]) -&gt; ProcessingResult:\n        \"\"\"Process a single video through all indexing services\"\"\"\n        video_id = video_data.get(\"video_id\", \"unknown\")\n\n        # Enrich with transcript data (non-critical operation)\n        enriched_video_data = await self._enrich_video_with_transcript(video_data)\n\n        # Store in MongoDB (critical operation)\n        storage_result = await self.video_storage.store_video(video_data)\n\n        # Index in Elasticsearch (non-critical)\n        indexing_result = await self.search_indexing.index_video(video_data)\n\n        # Update channel statistics (non-critical)\n        stats_result = await self.channel_stats.update_channel_stats(video_data)\n\n        result = ProcessingResult(\n            video_id=video_id,\n            storage_result=storage_result,\n            indexing_result=indexing_result,\n            stats_result=stats_result,\n        )\n\n        if result.is_success:\n            has_transcript = enriched_video_data.get(\"transcript\") is not None\n            logger.debug(\n                f\"Successfully processed video: {video_id} (transcript: {has_transcript})\"\n            )\n        elif result.overall_status == OperationStatus.PARTIAL_SUCCESS:\n            logger.warning(\n                f\"Partially processed video {video_id}: storage succeeded but other operations failed\"\n            )\n        else:\n            logger.error(f\"Failed to process video {video_id}: storage failed\")\n\n        return result\n\n    async def _process_video_with_cleanup(\n        self, video_data: Dict[str, Any]\n    ) -&gt; ProcessingResult:\n        \"\"\"Process video and handle task cleanup\"\"\"\n        task = asyncio.current_task()\n        try:\n            return await self.process_video(video_data)\n        finally:\n            # Clean up task reference\n            if task:\n                self._active_tasks.discard(task)\n\n    async def run(self) -&gt; None:\n        \"\"\"Main processing loop - consumes videos from queue and processes them\"\"\"\n        logger.info(\"Starting VideoIndexingProcessor main loop\")\n        self._running = True\n\n        try:\n            while self._running and not self._shutdown_event.is_set():\n                try:\n                    # Check if we have capacity for more tasks\n                    if len(self._active_tasks) &gt;= self.max_concurrent_tasks:\n                        # Wait for some tasks to complete\n                        if self._active_tasks:\n                            _, self._active_tasks = await asyncio.wait(\n                                self._active_tasks,\n                                return_when=asyncio.FIRST_COMPLETED,\n                                timeout=self.poll_interval,\n                            )\n                        else:\n                            await asyncio.sleep(self.poll_interval)\n                        continue\n\n                    # Try to get video data from queue\n                    video_data = await self._get_next_video()\n\n                    if video_data is None:\n                        # No video available, wait before trying again\n                        await asyncio.sleep(self.poll_interval)\n                        continue\n\n                    # Create and track processing task\n                    task = asyncio.create_task(\n                        self._process_video_with_cleanup(video_data)\n                    )\n                    self._active_tasks.add(task)\n\n                    logger.debug(\n                        f\"Started processing task for video: {video_data.get('video_id', 'unknown')}\"\n                    )\n\n                except asyncio.CancelledError:\n                    logger.info(\"Processing loop cancelled\")\n                    break\n                except Exception as e:\n                    logger.error(f\"Error in main processing loop: {e}\")\n                    await asyncio.sleep(self.poll_interval)\n\n        finally:\n            logger.info(\"Shutting down VideoIndexingProcessor\")\n            await self._cleanup_active_tasks()\n\n    async def _get_next_video(self) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get next video from queue with timeout\"\"\"\n        try:\n            result = self.input_queue.dequeue()\n            return result\n        except Exception as e:\n            logger.error(f\"Error getting video from queue: {e}\")\n            return None\n\n    async def _cleanup_active_tasks(self) -&gt; None:\n        \"\"\"Clean up any remaining active tasks\"\"\"\n        if not self._active_tasks:\n            return\n\n        logger.info(f\"Waiting for {len(self._active_tasks)} active tasks to complete\")\n\n        # Give tasks a chance to complete gracefully\n        try:\n            await asyncio.wait_for(\n                asyncio.gather(*self._active_tasks, return_exceptions=True),\n                timeout=30.0,  # 30 second timeout for graceful shutdown\n            )\n        except asyncio.TimeoutError:\n            logger.warning(\n                \"Timeout waiting for tasks to complete, cancelling remaining tasks\"\n            )\n            for task in self._active_tasks:\n                if not task.done():\n                    task.cancel()\n\n            # Wait for cancellations to complete\n            await asyncio.gather(*self._active_tasks, return_exceptions=True)\n\n        self._active_tasks.clear()\n\n    async def stop(self) -&gt; None:\n        \"\"\"Gracefully stop the processor\"\"\"\n        logger.info(\"Stopping VideoIndexingProcessor\")\n        self._running = False\n        self._shutdown_event.set()\n\n    def is_running(self) -&gt; bool:\n        \"\"\"Check if the processor is currently running\"\"\"\n        return self._running\n\n    @property\n    def active_task_count(self) -&gt; int:\n        \"\"\"Get the number of currently active processing tasks\"\"\"\n        return len(self._active_tasks)\n\n    async def health_check(self) -&gt; HealthStatus:\n        \"\"\"Check health of all dependent services\"\"\"\n        start_time = time.time()\n\n        try:\n            # Check all services concurrently\n            health_checks = await asyncio.gather(\n                self.video_storage.health_check(),\n                self.search_indexing.health_check(),\n                self.channel_stats.health_check(),\n                return_exceptions=True,\n            )\n\n            response_time = (time.time() - start_time) * 1000\n\n            # Determine overall health\n            unhealthy_services = []\n            service_statuses = {}\n\n            for i, (service_name, check) in enumerate(\n                [\n                    (\"storage\", health_checks[0]),\n                    (\"indexing\", health_checks[1]),\n                    (\"stats\", health_checks[2]),\n                ]\n            ):\n                if isinstance(check, Exception):\n                    unhealthy_services.append(service_name)\n                    service_statuses[service_name] = f\"Error: {str(check)}\"\n                elif not check.is_healthy:\n                    unhealthy_services.append(service_name)\n                    service_statuses[service_name] = check.message\n                else:\n                    service_statuses[service_name] = \"healthy\"\n\n            is_healthy = len(unhealthy_services) == 0 and self._running\n            processor_status = (\n                f\"running ({self.active_task_count} active tasks)\"\n                if self._running\n                else \"stopped\"\n            )\n            message = f\"Processor {processor_status}. \" + (\n                \"All services healthy\"\n                if len(unhealthy_services) == 0\n                else f\"Unhealthy services: {unhealthy_services}\"\n            )\n\n            return HealthStatus(\n                service_name=\"video_processor\",\n                is_healthy=is_healthy,\n                response_time_ms=response_time,\n                message=message,\n                metadata={\n                    \"services\": service_statuses,\n                    \"active_tasks\": self.active_task_count,\n                    \"is_running\": self._running,\n                },\n            )\n\n        except Exception as e:\n            response_time = (time.time() - start_time) * 1000\n            return HealthStatus(\n                service_name=\"video_processor\",\n                is_healthy=False,\n                response_time_ms=response_time,\n                message=f\"Health check failed: {str(e)}\",\n            )\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.VideoIndexingProcessor.active_task_count","title":"<code>active_task_count</code>  <code>property</code>","text":"<p>Get the number of currently active processing tasks</p>"},{"location":"documentation/api/#ytindexer.indexer.VideoIndexingProcessor.ensure_indices","title":"<code>ensure_indices()</code>  <code>async</code>","text":"<p>Ensure all required database indices and mappings exist</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/processor.py</code> <pre><code>async def ensure_indices(self) -&gt; OperationResult:\n    \"\"\"Ensure all required database indices and mappings exist\"\"\"\n    results = []\n\n    storage_result = await self.video_storage.ensure_indices()\n    results.append((\"storage\", storage_result))\n\n    indexing_result = await self.search_indexing.ensure_index()\n    results.append((\"indexing\", indexing_result))\n\n    stats_result = await self.channel_stats.ensure_indices()\n    results.append((\"stats\", stats_result))\n\n    logger.info(results)\n\n    failed_services = [name for name, result in results if result.is_failure]\n\n    if failed_services:\n        return OperationResult.failure(\n            f\"Failed to ensure indices for: {failed_services}\",\n            metadata={\n                \"results\": {name: result.message for name, result in results}\n            },\n        )\n    else:\n        return OperationResult.success(\n            \"All indices ensured successfully\",\n            metadata={\n                \"results\": {name: result.message for name, result in results}\n            },\n        )\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.VideoIndexingProcessor.health_check","title":"<code>health_check()</code>  <code>async</code>","text":"<p>Check health of all dependent services</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/processor.py</code> <pre><code>async def health_check(self) -&gt; HealthStatus:\n    \"\"\"Check health of all dependent services\"\"\"\n    start_time = time.time()\n\n    try:\n        # Check all services concurrently\n        health_checks = await asyncio.gather(\n            self.video_storage.health_check(),\n            self.search_indexing.health_check(),\n            self.channel_stats.health_check(),\n            return_exceptions=True,\n        )\n\n        response_time = (time.time() - start_time) * 1000\n\n        # Determine overall health\n        unhealthy_services = []\n        service_statuses = {}\n\n        for i, (service_name, check) in enumerate(\n            [\n                (\"storage\", health_checks[0]),\n                (\"indexing\", health_checks[1]),\n                (\"stats\", health_checks[2]),\n            ]\n        ):\n            if isinstance(check, Exception):\n                unhealthy_services.append(service_name)\n                service_statuses[service_name] = f\"Error: {str(check)}\"\n            elif not check.is_healthy:\n                unhealthy_services.append(service_name)\n                service_statuses[service_name] = check.message\n            else:\n                service_statuses[service_name] = \"healthy\"\n\n        is_healthy = len(unhealthy_services) == 0 and self._running\n        processor_status = (\n            f\"running ({self.active_task_count} active tasks)\"\n            if self._running\n            else \"stopped\"\n        )\n        message = f\"Processor {processor_status}. \" + (\n            \"All services healthy\"\n            if len(unhealthy_services) == 0\n            else f\"Unhealthy services: {unhealthy_services}\"\n        )\n\n        return HealthStatus(\n            service_name=\"video_processor\",\n            is_healthy=is_healthy,\n            response_time_ms=response_time,\n            message=message,\n            metadata={\n                \"services\": service_statuses,\n                \"active_tasks\": self.active_task_count,\n                \"is_running\": self._running,\n            },\n        )\n\n    except Exception as e:\n        response_time = (time.time() - start_time) * 1000\n        return HealthStatus(\n            service_name=\"video_processor\",\n            is_healthy=False,\n            response_time_ms=response_time,\n            message=f\"Health check failed: {str(e)}\",\n        )\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.VideoIndexingProcessor.is_running","title":"<code>is_running()</code>","text":"<p>Check if the processor is currently running</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/processor.py</code> <pre><code>def is_running(self) -&gt; bool:\n    \"\"\"Check if the processor is currently running\"\"\"\n    return self._running\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.VideoIndexingProcessor.process_video","title":"<code>process_video(video_data)</code>  <code>async</code>","text":"<p>Process a single video through all indexing services</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/processor.py</code> <pre><code>async def process_video(self, video_data: Dict[str, Any]) -&gt; ProcessingResult:\n    \"\"\"Process a single video through all indexing services\"\"\"\n    video_id = video_data.get(\"video_id\", \"unknown\")\n\n    # Enrich with transcript data (non-critical operation)\n    enriched_video_data = await self._enrich_video_with_transcript(video_data)\n\n    # Store in MongoDB (critical operation)\n    storage_result = await self.video_storage.store_video(video_data)\n\n    # Index in Elasticsearch (non-critical)\n    indexing_result = await self.search_indexing.index_video(video_data)\n\n    # Update channel statistics (non-critical)\n    stats_result = await self.channel_stats.update_channel_stats(video_data)\n\n    result = ProcessingResult(\n        video_id=video_id,\n        storage_result=storage_result,\n        indexing_result=indexing_result,\n        stats_result=stats_result,\n    )\n\n    if result.is_success:\n        has_transcript = enriched_video_data.get(\"transcript\") is not None\n        logger.debug(\n            f\"Successfully processed video: {video_id} (transcript: {has_transcript})\"\n        )\n    elif result.overall_status == OperationStatus.PARTIAL_SUCCESS:\n        logger.warning(\n            f\"Partially processed video {video_id}: storage succeeded but other operations failed\"\n        )\n    else:\n        logger.error(f\"Failed to process video {video_id}: storage failed\")\n\n    return result\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.VideoIndexingProcessor.run","title":"<code>run()</code>  <code>async</code>","text":"<p>Main processing loop - consumes videos from queue and processes them</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/processor.py</code> <pre><code>async def run(self) -&gt; None:\n    \"\"\"Main processing loop - consumes videos from queue and processes them\"\"\"\n    logger.info(\"Starting VideoIndexingProcessor main loop\")\n    self._running = True\n\n    try:\n        while self._running and not self._shutdown_event.is_set():\n            try:\n                # Check if we have capacity for more tasks\n                if len(self._active_tasks) &gt;= self.max_concurrent_tasks:\n                    # Wait for some tasks to complete\n                    if self._active_tasks:\n                        _, self._active_tasks = await asyncio.wait(\n                            self._active_tasks,\n                            return_when=asyncio.FIRST_COMPLETED,\n                            timeout=self.poll_interval,\n                        )\n                    else:\n                        await asyncio.sleep(self.poll_interval)\n                    continue\n\n                # Try to get video data from queue\n                video_data = await self._get_next_video()\n\n                if video_data is None:\n                    # No video available, wait before trying again\n                    await asyncio.sleep(self.poll_interval)\n                    continue\n\n                # Create and track processing task\n                task = asyncio.create_task(\n                    self._process_video_with_cleanup(video_data)\n                )\n                self._active_tasks.add(task)\n\n                logger.debug(\n                    f\"Started processing task for video: {video_data.get('video_id', 'unknown')}\"\n                )\n\n            except asyncio.CancelledError:\n                logger.info(\"Processing loop cancelled\")\n                break\n            except Exception as e:\n                logger.error(f\"Error in main processing loop: {e}\")\n                await asyncio.sleep(self.poll_interval)\n\n    finally:\n        logger.info(\"Shutting down VideoIndexingProcessor\")\n        await self._cleanup_active_tasks()\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.VideoIndexingProcessor.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Gracefully stop the processor</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/processor.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"Gracefully stop the processor\"\"\"\n    logger.info(\"Stopping VideoIndexingProcessor\")\n    self._running = False\n    self._shutdown_event.set()\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.VideoStorageService","title":"<code>VideoStorageService</code>","text":"<p>               Bases: <code>HealthCheckable</code></p> <p>Handles video metadata storage in MongoDB</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/storage.py</code> <pre><code>class VideoStorageService(HealthCheckable):\n    \"\"\"Handles video metadata storage in MongoDB\"\"\"\n\n    def __init__(self, client: Any, config: MongoDBConfig, retry_config: RetryConfig):\n        self.client = client\n        self.config = config\n        self.retry = RetryableOperation(\n            retry_config, non_retry_exceptions=[DuplicateKeyError]\n        )\n        self.db = self.client[config.database_name]\n        self.videos_collection = self.db[config.videos_collection]\n        logger.info(\"Initialized VideoStorageService\")\n\n    async def ensure_indices(self) -&gt; OperationResult:\n        \"\"\"Ensure the required database indices exist\"\"\"\n        created_indexes = []\n        failed_indexes = []\n\n        for index_name, index_config in self.config.video_indexes.items():\n            try:\n                await self.videos_collection.create_index(index_name, **index_config)\n                created_indexes.append(index_name)\n                logger.debug(f\"Created index: {index_name}\")\n            except OperationFailure as e:\n                if \"already exists\" in str(e):\n                    logger.debug(f\"Index '{index_name}' already exists\")\n                    created_indexes.append(index_name)\n                else:\n                    failed_indexes.append(index_name)\n                    logger.error(f\"Failed to create index {index_name}: {str(e)}\")\n\n        if failed_indexes:\n            return OperationResult.failure(\n                f\"Failed to create indexes: {failed_indexes}\",\n                metadata={\"created\": created_indexes, \"failed\": failed_indexes},\n            )\n        else:\n            return OperationResult.success(\n                f\"Ensured indexes: {created_indexes}\",\n                metadata={\"indexes\": created_indexes},\n            )\n\n    async def store_video(self, video_data: Dict[str, Any]) -&gt; OperationResult:\n        \"\"\"Store video metadata in MongoDB with retry logic\"\"\"\n        video_id = video_data.get(\"video_id\")\n        if not video_id:\n            return OperationResult.failure(\"Video data missing video_id\")\n\n        async def _store_operation():\n            result = await self.videos_collection.update_one(\n                {\"video_id\": video_id},\n                {\"$set\": {**video_data, \"updated_at\": datetime.now(timezone.utc)}},\n                upsert=True,\n            )\n            return result\n\n        try:\n            result = await self.retry.execute(\n                _store_operation, f\"store_video_{video_id}\"\n            )\n            logger.debug(f\"Stored video in MongoDB: {video_id}\")\n\n            action = \"updated\" if result.matched_count &gt; 0 else \"inserted\"\n\n            return OperationResult.success(\n                f\"Video {action}: {video_id}\",\n                metadata={\"video_id\": video_id, \"action\": action},\n            )\n        except Exception as e:\n            logger.error(f\"Failed to store video in MongoDB: {str(e)}\")\n            logger.debug(traceback.format_exc())\n            return OperationResult.failure(f\"Failed to store video: {str(e)}\", e)\n\n    async def health_check(self) -&gt; HealthStatus:\n        \"\"\"Check MongoDB connection health\"\"\"\n        start_time = time.time()\n        try:\n            # Simple ping to check connection\n            await self.client.admin.command(\"ping\")\n            response_time = (time.time() - start_time) * 1000\n\n            return HealthStatus(\n                service_name=\"mongodb\",\n                is_healthy=True,\n                response_time_ms=response_time,\n                message=\"Connection healthy\",\n            )\n        except Exception as e:\n            response_time = (time.time() - start_time) * 1000\n            return HealthStatus(\n                service_name=\"mongodb\",\n                is_healthy=False,\n                response_time_ms=response_time,\n                message=f\"Health check failed: {str(e)}\",\n            )\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.VideoStorageService.ensure_indices","title":"<code>ensure_indices()</code>  <code>async</code>","text":"<p>Ensure the required database indices exist</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/storage.py</code> <pre><code>async def ensure_indices(self) -&gt; OperationResult:\n    \"\"\"Ensure the required database indices exist\"\"\"\n    created_indexes = []\n    failed_indexes = []\n\n    for index_name, index_config in self.config.video_indexes.items():\n        try:\n            await self.videos_collection.create_index(index_name, **index_config)\n            created_indexes.append(index_name)\n            logger.debug(f\"Created index: {index_name}\")\n        except OperationFailure as e:\n            if \"already exists\" in str(e):\n                logger.debug(f\"Index '{index_name}' already exists\")\n                created_indexes.append(index_name)\n            else:\n                failed_indexes.append(index_name)\n                logger.error(f\"Failed to create index {index_name}: {str(e)}\")\n\n    if failed_indexes:\n        return OperationResult.failure(\n            f\"Failed to create indexes: {failed_indexes}\",\n            metadata={\"created\": created_indexes, \"failed\": failed_indexes},\n        )\n    else:\n        return OperationResult.success(\n            f\"Ensured indexes: {created_indexes}\",\n            metadata={\"indexes\": created_indexes},\n        )\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.VideoStorageService.health_check","title":"<code>health_check()</code>  <code>async</code>","text":"<p>Check MongoDB connection health</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/storage.py</code> <pre><code>async def health_check(self) -&gt; HealthStatus:\n    \"\"\"Check MongoDB connection health\"\"\"\n    start_time = time.time()\n    try:\n        # Simple ping to check connection\n        await self.client.admin.command(\"ping\")\n        response_time = (time.time() - start_time) * 1000\n\n        return HealthStatus(\n            service_name=\"mongodb\",\n            is_healthy=True,\n            response_time_ms=response_time,\n            message=\"Connection healthy\",\n        )\n    except Exception as e:\n        response_time = (time.time() - start_time) * 1000\n        return HealthStatus(\n            service_name=\"mongodb\",\n            is_healthy=False,\n            response_time_ms=response_time,\n            message=f\"Health check failed: {str(e)}\",\n        )\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.VideoStorageService.store_video","title":"<code>store_video(video_data)</code>  <code>async</code>","text":"<p>Store video metadata in MongoDB with retry logic</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/storage.py</code> <pre><code>async def store_video(self, video_data: Dict[str, Any]) -&gt; OperationResult:\n    \"\"\"Store video metadata in MongoDB with retry logic\"\"\"\n    video_id = video_data.get(\"video_id\")\n    if not video_id:\n        return OperationResult.failure(\"Video data missing video_id\")\n\n    async def _store_operation():\n        result = await self.videos_collection.update_one(\n            {\"video_id\": video_id},\n            {\"$set\": {**video_data, \"updated_at\": datetime.now(timezone.utc)}},\n            upsert=True,\n        )\n        return result\n\n    try:\n        result = await self.retry.execute(\n            _store_operation, f\"store_video_{video_id}\"\n        )\n        logger.debug(f\"Stored video in MongoDB: {video_id}\")\n\n        action = \"updated\" if result.matched_count &gt; 0 else \"inserted\"\n\n        return OperationResult.success(\n            f\"Video {action}: {video_id}\",\n            metadata={\"video_id\": video_id, \"action\": action},\n        )\n    except Exception as e:\n        logger.error(f\"Failed to store video in MongoDB: {str(e)}\")\n        logger.debug(traceback.format_exc())\n        return OperationResult.failure(f\"Failed to store video: {str(e)}\", e)\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.VideoTranscriptService","title":"<code>VideoTranscriptService</code>","text":"<p>Downloads YouTube video transcripts with multiple language support</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/transcript.py</code> <pre><code>class VideoTranscriptService:\n    \"\"\"Downloads YouTube video transcripts with multiple language support\"\"\"\n\n    def __init__(self, languages: Optional[List[str]] = None):\n        \"\"\"\n        Initialize the transcript service.\n\n        Args:\n            languages: Preferred languages in order of preference.\n                      Defaults to ['en', 'en-US'] if not provided.\n        \"\"\"\n        if languages is None:\n            languages = [\"en\", \"en-US\"]\n        self.languages = languages\n        self.formatter = TextFormatter()\n\n    def get_transcript(self, video_id: str) -&gt; Optional[str]:\n        \"\"\"\n        Download and format transcript for a YouTube video.\n\n        Args:\n            video_id: YouTube video ID\n\n        Returns:\n            Formatted transcript text or None if unavailable\n        \"\"\"\n        if not video_id or not video_id.strip():\n            logger.error(\"Empty or invalid video ID provided\")\n            return None\n\n        try:\n            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n\n            # Try preferred languages first\n            for language in self.languages:\n                try:\n                    transcript = transcript_list.find_transcript([language])\n                    transcript_data = transcript.fetch()\n                    formatted_text = self.formatter.format_transcript(transcript_data)\n                    logger.info(\n                        f\"Successfully retrieved transcript for {video_id} in {language}\"\n                    )\n                    return formatted_text\n                except NoTranscriptFound:\n                    logger.debug(\n                        f\"No transcript found for {video_id} in language {language}\"\n                    )\n                    continue\n                except Exception as e:\n                    logger.debug(\n                        f\"Error getting transcript in {language} for {video_id}: {e}\"\n                    )\n                    continue\n\n            # Fallback to generated English transcript\n            try:\n                transcript = transcript_list.find_generated_transcript([\"en\"])\n                transcript_data = transcript.fetch()\n                formatted_text = self.formatter.format_transcript(transcript_data)\n                logger.info(\n                    f\"Successfully retrieved generated English transcript for {video_id}\"\n                )\n                return formatted_text\n            except NoTranscriptFound:\n                logger.debug(f\"No generated English transcript found for {video_id}\")\n            except Exception as e:\n                logger.debug(f\"Error getting generated transcript for {video_id}: {e}\")\n\n            # Last resort: use any available transcript\n            try:\n                available_transcripts = list(transcript_list)\n                if available_transcripts:\n                    first_transcript = available_transcripts[0]\n                    transcript_data = first_transcript.fetch()\n                    formatted_text = self.formatter.format_transcript(transcript_data)\n                    logger.info(\n                        f\"Retrieved fallback transcript for {video_id} in {first_transcript.language}\"\n                    )\n                    return formatted_text\n                else:\n                    logger.warning(f\"No transcripts available for video {video_id}\")\n                    return None\n            except Exception as e:\n                logger.error(f\"Error fetching fallback transcript for {video_id}: {e}\")\n\n        except TranscriptsDisabled:\n            logger.warning(f\"Transcripts are disabled for video {video_id}\")\n            return None\n        except VideoUnavailable:\n            logger.error(f\"Video {video_id} is unavailable\")\n            return None\n        except NoTranscriptFound:\n            logger.warning(f\"No transcripts available for video {video_id}\")\n            return None\n        except CouldNotRetrieveTranscript as e:\n            logger.error(f\"Could not retrieve transcript for {video_id}: {e}\")\n            return None\n        except Exception as e:\n            logger.error(\n                f\"Unexpected error getting transcript for video {video_id}: {e}\"\n            )\n            return None\n\n    def get_transcript_with_timestamps(self, video_id: str) -&gt; Optional[List[Dict]]:\n        \"\"\"\n        Get transcript with timing information for each segment.\n\n        Args:\n            video_id: YouTube video ID\n\n        Returns:\n            List of transcript segments with 'text', 'start', 'duration' keys,\n            or None if transcript unavailable\n        \"\"\"\n        if not video_id or not video_id.strip():\n            logger.error(\"Empty or invalid video ID provided\")\n            return None\n\n        try:\n            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n\n            # Try preferred languages first\n            for language in self.languages:\n                try:\n                    transcript = transcript_list.find_transcript([language])\n                    transcript_data = transcript.fetch()\n                    logger.info(\n                        f\"Successfully retrieved timestamped transcript for {video_id} in {language}\"\n                    )\n                    return transcript_data\n                except NoTranscriptFound:\n                    logger.debug(\n                        f\"No transcript found for {video_id} in language {language}\"\n                    )\n                    continue\n                except Exception as e:\n                    logger.debug(\n                        f\"Error getting timestamped transcript in {language} for {video_id}: {e}\"\n                    )\n                    continue\n\n            # Fallback to generated English transcript\n            try:\n                transcript = transcript_list.find_generated_transcript([\"en\"])\n                transcript_data = transcript.fetch()\n                logger.info(\n                    f\"Successfully retrieved generated English timestamped transcript for {video_id}\"\n                )\n                return transcript_data\n            except NoTranscriptFound:\n                logger.debug(f\"No generated English transcript found for {video_id}\")\n            except Exception as e:\n                logger.debug(\n                    f\"Error getting generated timestamped transcript for {video_id}: {e}\"\n                )\n\n            # Last resort: use any available transcript\n            try:\n                available_transcripts = list(transcript_list)\n                if available_transcripts:\n                    first_transcript = available_transcripts[0]\n                    transcript_data = first_transcript.fetch()\n                    logger.info(\n                        f\"Retrieved fallback timestamped transcript for {video_id} in {first_transcript.language}\"\n                    )\n                    return transcript_data\n                else:\n                    logger.warning(f\"No transcripts available for video {video_id}\")\n                    return None\n            except Exception as e:\n                logger.error(\n                    f\"Error fetching fallback timestamped transcript for {video_id}: {e}\"\n                )\n\n        except TranscriptsDisabled:\n            logger.warning(f\"Transcripts are disabled for video {video_id}\")\n            return None\n        except VideoUnavailable:\n            logger.error(f\"Video {video_id} is unavailable\")\n            return None\n        except NoTranscriptFound:\n            logger.warning(f\"No transcripts available for video {video_id}\")\n            return None\n        except CouldNotRetrieveTranscript as e:\n            logger.error(\n                f\"Could not retrieve timestamped transcript for {video_id}: {e}\"\n            )\n            return None\n        except Exception as e:\n            logger.error(\n                f\"Unexpected error getting timestamped transcript for video {video_id}: {e}\"\n            )\n            return None\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.VideoTranscriptService.__init__","title":"<code>__init__(languages=None)</code>","text":"<p>Initialize the transcript service.</p> <p>Parameters:</p> Name Type Description Default <code>languages</code> <code>Optional[List[str]]</code> <p>Preferred languages in order of preference.       Defaults to ['en', 'en-US'] if not provided.</p> <code>None</code> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/transcript.py</code> <pre><code>def __init__(self, languages: Optional[List[str]] = None):\n    \"\"\"\n    Initialize the transcript service.\n\n    Args:\n        languages: Preferred languages in order of preference.\n                  Defaults to ['en', 'en-US'] if not provided.\n    \"\"\"\n    if languages is None:\n        languages = [\"en\", \"en-US\"]\n    self.languages = languages\n    self.formatter = TextFormatter()\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.VideoTranscriptService.get_transcript","title":"<code>get_transcript(video_id)</code>","text":"<p>Download and format transcript for a YouTube video.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>YouTube video ID</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Formatted transcript text or None if unavailable</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/transcript.py</code> <pre><code>def get_transcript(self, video_id: str) -&gt; Optional[str]:\n    \"\"\"\n    Download and format transcript for a YouTube video.\n\n    Args:\n        video_id: YouTube video ID\n\n    Returns:\n        Formatted transcript text or None if unavailable\n    \"\"\"\n    if not video_id or not video_id.strip():\n        logger.error(\"Empty or invalid video ID provided\")\n        return None\n\n    try:\n        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n\n        # Try preferred languages first\n        for language in self.languages:\n            try:\n                transcript = transcript_list.find_transcript([language])\n                transcript_data = transcript.fetch()\n                formatted_text = self.formatter.format_transcript(transcript_data)\n                logger.info(\n                    f\"Successfully retrieved transcript for {video_id} in {language}\"\n                )\n                return formatted_text\n            except NoTranscriptFound:\n                logger.debug(\n                    f\"No transcript found for {video_id} in language {language}\"\n                )\n                continue\n            except Exception as e:\n                logger.debug(\n                    f\"Error getting transcript in {language} for {video_id}: {e}\"\n                )\n                continue\n\n        # Fallback to generated English transcript\n        try:\n            transcript = transcript_list.find_generated_transcript([\"en\"])\n            transcript_data = transcript.fetch()\n            formatted_text = self.formatter.format_transcript(transcript_data)\n            logger.info(\n                f\"Successfully retrieved generated English transcript for {video_id}\"\n            )\n            return formatted_text\n        except NoTranscriptFound:\n            logger.debug(f\"No generated English transcript found for {video_id}\")\n        except Exception as e:\n            logger.debug(f\"Error getting generated transcript for {video_id}: {e}\")\n\n        # Last resort: use any available transcript\n        try:\n            available_transcripts = list(transcript_list)\n            if available_transcripts:\n                first_transcript = available_transcripts[0]\n                transcript_data = first_transcript.fetch()\n                formatted_text = self.formatter.format_transcript(transcript_data)\n                logger.info(\n                    f\"Retrieved fallback transcript for {video_id} in {first_transcript.language}\"\n                )\n                return formatted_text\n            else:\n                logger.warning(f\"No transcripts available for video {video_id}\")\n                return None\n        except Exception as e:\n            logger.error(f\"Error fetching fallback transcript for {video_id}: {e}\")\n\n    except TranscriptsDisabled:\n        logger.warning(f\"Transcripts are disabled for video {video_id}\")\n        return None\n    except VideoUnavailable:\n        logger.error(f\"Video {video_id} is unavailable\")\n        return None\n    except NoTranscriptFound:\n        logger.warning(f\"No transcripts available for video {video_id}\")\n        return None\n    except CouldNotRetrieveTranscript as e:\n        logger.error(f\"Could not retrieve transcript for {video_id}: {e}\")\n        return None\n    except Exception as e:\n        logger.error(\n            f\"Unexpected error getting transcript for video {video_id}: {e}\"\n        )\n        return None\n</code></pre>"},{"location":"documentation/api/#ytindexer.indexer.VideoTranscriptService.get_transcript_with_timestamps","title":"<code>get_transcript_with_timestamps(video_id)</code>","text":"<p>Get transcript with timing information for each segment.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>YouTube video ID</p> required <p>Returns:</p> Type Description <code>Optional[List[Dict]]</code> <p>List of transcript segments with 'text', 'start', 'duration' keys,</p> <code>Optional[List[Dict]]</code> <p>or None if transcript unavailable</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/transcript.py</code> <pre><code>def get_transcript_with_timestamps(self, video_id: str) -&gt; Optional[List[Dict]]:\n    \"\"\"\n    Get transcript with timing information for each segment.\n\n    Args:\n        video_id: YouTube video ID\n\n    Returns:\n        List of transcript segments with 'text', 'start', 'duration' keys,\n        or None if transcript unavailable\n    \"\"\"\n    if not video_id or not video_id.strip():\n        logger.error(\"Empty or invalid video ID provided\")\n        return None\n\n    try:\n        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n\n        # Try preferred languages first\n        for language in self.languages:\n            try:\n                transcript = transcript_list.find_transcript([language])\n                transcript_data = transcript.fetch()\n                logger.info(\n                    f\"Successfully retrieved timestamped transcript for {video_id} in {language}\"\n                )\n                return transcript_data\n            except NoTranscriptFound:\n                logger.debug(\n                    f\"No transcript found for {video_id} in language {language}\"\n                )\n                continue\n            except Exception as e:\n                logger.debug(\n                    f\"Error getting timestamped transcript in {language} for {video_id}: {e}\"\n                )\n                continue\n\n        # Fallback to generated English transcript\n        try:\n            transcript = transcript_list.find_generated_transcript([\"en\"])\n            transcript_data = transcript.fetch()\n            logger.info(\n                f\"Successfully retrieved generated English timestamped transcript for {video_id}\"\n            )\n            return transcript_data\n        except NoTranscriptFound:\n            logger.debug(f\"No generated English transcript found for {video_id}\")\n        except Exception as e:\n            logger.debug(\n                f\"Error getting generated timestamped transcript for {video_id}: {e}\"\n            )\n\n        # Last resort: use any available transcript\n        try:\n            available_transcripts = list(transcript_list)\n            if available_transcripts:\n                first_transcript = available_transcripts[0]\n                transcript_data = first_transcript.fetch()\n                logger.info(\n                    f\"Retrieved fallback timestamped transcript for {video_id} in {first_transcript.language}\"\n                )\n                return transcript_data\n            else:\n                logger.warning(f\"No transcripts available for video {video_id}\")\n                return None\n        except Exception as e:\n            logger.error(\n                f\"Error fetching fallback timestamped transcript for {video_id}: {e}\"\n            )\n\n    except TranscriptsDisabled:\n        logger.warning(f\"Transcripts are disabled for video {video_id}\")\n        return None\n    except VideoUnavailable:\n        logger.error(f\"Video {video_id} is unavailable\")\n        return None\n    except NoTranscriptFound:\n        logger.warning(f\"No transcripts available for video {video_id}\")\n        return None\n    except CouldNotRetrieveTranscript as e:\n        logger.error(\n            f\"Could not retrieve timestamped transcript for {video_id}: {e}\"\n        )\n        return None\n    except Exception as e:\n        logger.error(\n            f\"Unexpected error getting timestamped transcript for video {video_id}: {e}\"\n        )\n        return None\n</code></pre>"},{"location":"documentation/database/","title":"Database","text":""},{"location":"documentation/database/#ytindexer.database.AsyncDatabaseConnection","title":"<code>AsyncDatabaseConnection</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> <p>Abstract base class for asynchronous database connections.</p> <p>This class defines the interface for establishing and closing asynchronous connections to a database or similar resource. Intended to be used with dependency injection.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list for subclasses.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments for subclasses.</p> <code>{}</code> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/database/base.py</code> <pre><code>class AsyncDatabaseConnection(Generic[T], ABC):\n    \"\"\"\n    Abstract base class for asynchronous database connections.\n\n    This class defines the interface for establishing and closing\n    asynchronous connections to a database or similar resource.\n    Intended to be used with dependency injection.\n\n    Args:\n        *args: Variable length argument list for subclasses.\n        **kwargs: Arbitrary keyword arguments for subclasses.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize the connection instance.\n\n        Subclasses may override this to accept parameters such as\n        database URLs or credentials.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def connect(self) -&gt; T:\n        \"\"\"\n        Establish and return the asynchronous database connection.\n\n        This method should be overridden by subclasses to implement\n        the logic for creating the actual connection.\n\n        Returns:\n            T: An instance representing the database connection.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def close(self) -&gt; None:\n        \"\"\"\n        Close the database connection.\n\n        This method should be overridden by subclasses to implement\n        proper cleanup and resource deallocation.\n\n        Returns:\n            None\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"documentation/database/#ytindexer.database.AsyncDatabaseConnection.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the connection instance.</p> <p>Subclasses may override this to accept parameters such as database URLs or credentials.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/database/base.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initialize the connection instance.\n\n    Subclasses may override this to accept parameters such as\n    database URLs or credentials.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/database/#ytindexer.database.AsyncDatabaseConnection.close","title":"<code>close()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Close the database connection.</p> <p>This method should be overridden by subclasses to implement proper cleanup and resource deallocation.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/database/base.py</code> <pre><code>@abstractmethod\nasync def close(self) -&gt; None:\n    \"\"\"\n    Close the database connection.\n\n    This method should be overridden by subclasses to implement\n    proper cleanup and resource deallocation.\n\n    Returns:\n        None\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"documentation/database/#ytindexer.database.AsyncDatabaseConnection.connect","title":"<code>connect()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Establish and return the asynchronous database connection.</p> <p>This method should be overridden by subclasses to implement the logic for creating the actual connection.</p> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>An instance representing the database connection.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/database/base.py</code> <pre><code>@abstractmethod\nasync def connect(self) -&gt; T:\n    \"\"\"\n    Establish and return the asynchronous database connection.\n\n    This method should be overridden by subclasses to implement\n    the logic for creating the actual connection.\n\n    Returns:\n        T: An instance representing the database connection.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"documentation/database/#ytindexer.database.ElasticConnection","title":"<code>ElasticConnection</code>","text":"<p>               Bases: <code>AsyncDatabaseConnection[AsyncElasticsearch]</code></p> <p>Concrete implementation of AsyncDatabaseConnection for ElasticSearch.</p> <p>Parameters:</p> Name Type Description Default <code>dsn</code> <code>str</code> <p>The connection string or DSN for ElasticSearch.</p> required Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/database/elastic.py</code> <pre><code>class ElasticConnection(AsyncDatabaseConnection[AsyncElasticsearch]):\n    \"\"\"\n    Concrete implementation of AsyncDatabaseConnection for ElasticSearch.\n\n    Args:\n        dsn (str): The connection string or DSN for ElasticSearch.\n    \"\"\"\n\n    def __init__(self, dsn: str):\n        self.dsn = dsn\n        self._client: AsyncElasticsearch | None = None\n        self._lock = asyncio.Lock()\n\n    async def connect(self) -&gt; AsyncElasticsearch:\n        \"\"\"\n        Establish and return an AsyncElasticsearch client instance.\n\n        Returns:\n            AsyncElasticsearch: The ElasticSearch async client.\n\n        Raises:\n            ConnectionError: If connection to ElasticSearch fails.\n        \"\"\"\n        async with self._lock:\n            if self._client is None:\n                try:\n                    self._client = AsyncElasticsearch(self.dsn)\n                    await self._client.info()\n                    logger.info(\n                        \"Successfully connected to Elastic at: {host}\", host=self.dsn\n                    )\n                except ConnectionError as conn_fail:\n                    logger.error(\n                        \"Couldn't connect to Elastic: {error}\", error=conn_fail\n                    )\n                    raise\n            return self._client\n\n    async def close(self) -&gt; None:\n        \"\"\"\n        Close the AsyncElasticsearch client connection.\n        \"\"\"\n        async with self._lock:\n            if self._client is not None:\n                await self._client.close()\n                self._client = None\n                logger.error(\"Elastic client close\")\n</code></pre>"},{"location":"documentation/database/#ytindexer.database.ElasticConnection.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the AsyncElasticsearch client connection.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/database/elastic.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"\n    Close the AsyncElasticsearch client connection.\n    \"\"\"\n    async with self._lock:\n        if self._client is not None:\n            await self._client.close()\n            self._client = None\n            logger.error(\"Elastic client close\")\n</code></pre>"},{"location":"documentation/database/#ytindexer.database.ElasticConnection.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Establish and return an AsyncElasticsearch client instance.</p> <p>Returns:</p> Name Type Description <code>AsyncElasticsearch</code> <code>AsyncElasticsearch</code> <p>The ElasticSearch async client.</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If connection to ElasticSearch fails.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/database/elastic.py</code> <pre><code>async def connect(self) -&gt; AsyncElasticsearch:\n    \"\"\"\n    Establish and return an AsyncElasticsearch client instance.\n\n    Returns:\n        AsyncElasticsearch: The ElasticSearch async client.\n\n    Raises:\n        ConnectionError: If connection to ElasticSearch fails.\n    \"\"\"\n    async with self._lock:\n        if self._client is None:\n            try:\n                self._client = AsyncElasticsearch(self.dsn)\n                await self._client.info()\n                logger.info(\n                    \"Successfully connected to Elastic at: {host}\", host=self.dsn\n                )\n            except ConnectionError as conn_fail:\n                logger.error(\n                    \"Couldn't connect to Elastic: {error}\", error=conn_fail\n                )\n                raise\n        return self._client\n</code></pre>"},{"location":"documentation/database/#ytindexer.database.MongoConnection","title":"<code>MongoConnection</code>","text":"<p>               Bases: <code>AsyncDatabaseConnection[AsyncIOMotorClient]</code></p> <p>Concrete implementation of AsyncDatabaseConnection for MongoDB using Motor.</p> <p>Parameters:</p> Name Type Description Default <code>dsn</code> <code>str</code> <p>MongoDB connection string.</p> required Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/database/mongo.py</code> <pre><code>class MongoConnection(AsyncDatabaseConnection[AsyncIOMotorClient]):\n    \"\"\"\n    Concrete implementation of AsyncDatabaseConnection for MongoDB using Motor.\n\n    Args:\n        dsn (str): MongoDB connection string.\n    \"\"\"\n\n    def __init__(self, dsn: str):\n        self.dsn = dsn\n        self._client: AsyncIOMotorClient | None = None\n        self._lock = asyncio.Lock()\n\n    async def connect(self) -&gt; AsyncIOMotorClient:\n        \"\"\"\n        Establish and return an AsyncIOMotorClient instance.\n\n        Returns:\n            AsyncIOMotorClient: The async MongoDB client.\n\n        Raises:\n            ConnectionFailure: If connection to MongoDB fails.\n        \"\"\"\n        async with self._lock:\n            if self._client is None:\n                try:\n                    self._client = AsyncIOMotorClient(self.dsn)\n                    # Optionally test connection by pinging\n                    await self._client.admin.command(\"ping\")\n                    logger.info(\n                        \"Successfully connected to MongoDB at: {host}\", host=self.dsn\n                    )\n                except ConnectionFailure as conn_fail:\n                    logger.error(\n                        \"Couldn't connect to the MongoDB database: {error}\",\n                        error=conn_fail,\n                    )\n                    raise\n            return self._client\n\n    async def close(self) -&gt; None:\n        \"\"\"\n        Close the MongoDB client connection.\n        \"\"\"\n        async with self._lock:\n            if self._client is not None:\n                self._client.close()\n                self._client = None\n</code></pre>"},{"location":"documentation/database/#ytindexer.database.MongoConnection.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the MongoDB client connection.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/database/mongo.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"\n    Close the MongoDB client connection.\n    \"\"\"\n    async with self._lock:\n        if self._client is not None:\n            self._client.close()\n            self._client = None\n</code></pre>"},{"location":"documentation/database/#ytindexer.database.MongoConnection.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Establish and return an AsyncIOMotorClient instance.</p> <p>Returns:</p> Name Type Description <code>AsyncIOMotorClient</code> <code>AsyncIOMotorClient</code> <p>The async MongoDB client.</p> <p>Raises:</p> Type Description <code>ConnectionFailure</code> <p>If connection to MongoDB fails.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/database/mongo.py</code> <pre><code>async def connect(self) -&gt; AsyncIOMotorClient:\n    \"\"\"\n    Establish and return an AsyncIOMotorClient instance.\n\n    Returns:\n        AsyncIOMotorClient: The async MongoDB client.\n\n    Raises:\n        ConnectionFailure: If connection to MongoDB fails.\n    \"\"\"\n    async with self._lock:\n        if self._client is None:\n            try:\n                self._client = AsyncIOMotorClient(self.dsn)\n                # Optionally test connection by pinging\n                await self._client.admin.command(\"ping\")\n                logger.info(\n                    \"Successfully connected to MongoDB at: {host}\", host=self.dsn\n                )\n            except ConnectionFailure as conn_fail:\n                logger.error(\n                    \"Couldn't connect to the MongoDB database: {error}\",\n                    error=conn_fail,\n                )\n                raise\n        return self._client\n</code></pre>"},{"location":"documentation/database/#ytindexer.database.ValkeyConnection","title":"<code>ValkeyConnection</code>","text":"<p>               Bases: <code>AsyncDatabaseConnection[Valkey]</code></p> <p>Concrete implementation of AsyncDatabaseConnection for Valkey client.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>Valkey host address.</p> required <code>port</code> <code>int</code> <p>Valkey port.</p> required <code>password</code> <code>str</code> <p>Password for Valkey authentication.</p> required Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/database/valkey.py</code> <pre><code>class ValkeyConnection(AsyncDatabaseConnection[valkey.client.Valkey]):\n    \"\"\"\n    Concrete implementation of AsyncDatabaseConnection for Valkey client.\n\n    Args:\n        host (str): Valkey host address.\n        port (int): Valkey port.\n        password (str): Password for Valkey authentication.\n    \"\"\"\n\n    def __init__(self, host: str, port: int, password: str):\n        self.host = host\n        self.port = port\n        self.password = password\n        self._client: valkey.client.Valkey | None = None\n        self._lock = asyncio.Lock()\n\n    async def connect(self) -&gt; valkey.client.Valkey:\n        \"\"\"\n        Establish and return a Valkey client connection.\n\n        Returns:\n            valkey.client.Valkey: The Valkey client instance.\n\n        Raises:\n            valkey.exceptions.ConnectionError: If connection to Valkey fails.\n        \"\"\"\n        async with self._lock:\n            if self._client is None:\n                try:\n                    self._client = valkey.Valkey(\n                        host=self.host,\n                        port=self.port,\n                        username=None,\n                        password=self.password,\n                        db=0,\n                    )\n                    self._client.ping()\n                    logger.info(\n                        \"Successfully connected to Valkey at: {host}\", host=self.host\n                    )\n                except valkey.exceptions.ConnectionError as conn_fail:\n                    logger.error(\n                        \"Couldn't connect to the Valkey: {error}\", error=conn_fail\n                    )\n                    raise\n            return self._client\n\n    async def close(self) -&gt; None:\n        \"\"\"\n        Close the Valkey client connection.\n        \"\"\"\n        async with self._lock:\n            if self._client is not None:\n                self._client.close()\n                self._client = None\n</code></pre>"},{"location":"documentation/database/#ytindexer.database.ValkeyConnection.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the Valkey client connection.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/database/valkey.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"\n    Close the Valkey client connection.\n    \"\"\"\n    async with self._lock:\n        if self._client is not None:\n            self._client.close()\n            self._client = None\n</code></pre>"},{"location":"documentation/database/#ytindexer.database.ValkeyConnection.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Establish and return a Valkey client connection.</p> <p>Returns:</p> Type Description <code>Valkey</code> <p>valkey.client.Valkey: The Valkey client instance.</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If connection to Valkey fails.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/database/valkey.py</code> <pre><code>async def connect(self) -&gt; valkey.client.Valkey:\n    \"\"\"\n    Establish and return a Valkey client connection.\n\n    Returns:\n        valkey.client.Valkey: The Valkey client instance.\n\n    Raises:\n        valkey.exceptions.ConnectionError: If connection to Valkey fails.\n    \"\"\"\n    async with self._lock:\n        if self._client is None:\n            try:\n                self._client = valkey.Valkey(\n                    host=self.host,\n                    port=self.port,\n                    username=None,\n                    password=self.password,\n                    db=0,\n                )\n                self._client.ping()\n                logger.info(\n                    \"Successfully connected to Valkey at: {host}\", host=self.host\n                )\n            except valkey.exceptions.ConnectionError as conn_fail:\n                logger.error(\n                    \"Couldn't connect to the Valkey: {error}\", error=conn_fail\n                )\n                raise\n        return self._client\n</code></pre>"},{"location":"documentation/indexer/","title":"Indexer","text":""},{"location":"documentation/indexer/#ytindexer.indexer.ChannelStatsService","title":"<code>ChannelStatsService</code>","text":"<p>               Bases: <code>HealthCheckable</code></p> <p>Handles channel statistics updates</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/stats.py</code> <pre><code>class ChannelStatsService(HealthCheckable):\n    \"\"\"Handles channel statistics updates\"\"\"\n\n    def __init__(self, client: Any, config: MongoDBConfig, retry_config: RetryConfig):\n        self.client = client\n        self.config = config\n        self.retry = RetryableOperation(retry_config)\n        self.db = self.client[config.database_name]\n        self.channels_collection = self.db[config.channels_collection]\n        logger.info(\"Initialized ChannelStatsService\")\n\n    async def ensure_indices(self) -&gt; OperationResult:\n        \"\"\"Ensure the required database indices exist\"\"\"\n        created_indexes = []\n        failed_indexes = []\n\n        for index_name, index_config in self.config.channel_indexes.items():\n            try:\n                await self.channels_collection.create_index(index_name, **index_config)\n                created_indexes.append(index_name)\n                logger.debug(f\"Created index: {index_name}\")\n            except OperationFailure as e:\n                if \"already exists\" in str(e):\n                    logger.debug(f\"Index '{index_name}' already exists\")\n                    created_indexes.append(index_name)\n                else:\n                    failed_indexes.append(index_name)\n                    logger.error(f\"Failed to create index {index_name}: {str(e)}\")\n\n        if failed_indexes:\n            return OperationResult.failure(\n                f\"Failed to create indexes: {failed_indexes}\",\n                metadata={\"created\": created_indexes, \"failed\": failed_indexes},\n            )\n        else:\n            return OperationResult.success(\n                f\"Ensured indexes: {created_indexes}\",\n                metadata={\"indexes\": created_indexes},\n            )\n\n    async def update_channel_stats(self, video_data: Dict[str, Any]) -&gt; OperationResult:\n        \"\"\"Update channel statistics based on video data with retry logic\"\"\"\n        channel_id = video_data.get(\"channel_id\")\n        if not channel_id:\n            return OperationResult.failure(\"Video data missing channel_id\")\n\n        async def _update_operation():\n            result = await self.channels_collection.update_one(\n                {\"channel_id\": channel_id},\n                {\n                    \"$inc\": {\"video_count\": 1},\n                    \"$set\": {\"last_activity\": datetime.now(timezone.utc)},\n                    \"$setOnInsert\": {\"first_seen\": datetime.now(timezone.utc)},\n                },\n                upsert=True,\n            )\n            return result\n\n        try:\n            result = await self.retry.execute(\n                _update_operation, f\"update_channel_stats_{channel_id}\"\n            )\n            logger.debug(f\"Updated channel stats: {channel_id}\")\n\n            action = \"updated\" if result.matched_count &gt; 0 else \"created\"\n            return OperationResult.success(\n                f\"Channel stats {action}: {channel_id}\",\n                metadata={\"channel_id\": channel_id, \"action\": action},\n            )\n        except Exception as e:\n            logger.error(f\"Failed to update channel stats: {str(e)}\")\n            logger.debug(traceback.format_exc())\n            return OperationResult.failure(\n                f\"Failed to update channel stats: {str(e)}\", e\n            )\n\n    async def health_check(self) -&gt; HealthStatus:\n        \"\"\"Check MongoDB connection health for channels collection\"\"\"\n        start_time = time.time()\n        try:\n            # Test access to channels collection\n            await self.channels_collection.count_documents({}, limit=1)\n            response_time = (time.time() - start_time) * 1000\n\n            return HealthStatus(\n                service_name=\"mongodb_channels\",\n                is_healthy=True,\n                response_time_ms=response_time,\n                message=\"Channels collection accessible\",\n            )\n        except Exception as e:\n            response_time = (time.time() - start_time) * 1000\n            return HealthStatus(\n                service_name=\"mongodb_channels\",\n                is_healthy=False,\n                response_time_ms=response_time,\n                message=f\"Health check failed: {str(e)}\",\n            )\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.ChannelStatsService.ensure_indices","title":"<code>ensure_indices()</code>  <code>async</code>","text":"<p>Ensure the required database indices exist</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/stats.py</code> <pre><code>async def ensure_indices(self) -&gt; OperationResult:\n    \"\"\"Ensure the required database indices exist\"\"\"\n    created_indexes = []\n    failed_indexes = []\n\n    for index_name, index_config in self.config.channel_indexes.items():\n        try:\n            await self.channels_collection.create_index(index_name, **index_config)\n            created_indexes.append(index_name)\n            logger.debug(f\"Created index: {index_name}\")\n        except OperationFailure as e:\n            if \"already exists\" in str(e):\n                logger.debug(f\"Index '{index_name}' already exists\")\n                created_indexes.append(index_name)\n            else:\n                failed_indexes.append(index_name)\n                logger.error(f\"Failed to create index {index_name}: {str(e)}\")\n\n    if failed_indexes:\n        return OperationResult.failure(\n            f\"Failed to create indexes: {failed_indexes}\",\n            metadata={\"created\": created_indexes, \"failed\": failed_indexes},\n        )\n    else:\n        return OperationResult.success(\n            f\"Ensured indexes: {created_indexes}\",\n            metadata={\"indexes\": created_indexes},\n        )\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.ChannelStatsService.health_check","title":"<code>health_check()</code>  <code>async</code>","text":"<p>Check MongoDB connection health for channels collection</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/stats.py</code> <pre><code>async def health_check(self) -&gt; HealthStatus:\n    \"\"\"Check MongoDB connection health for channels collection\"\"\"\n    start_time = time.time()\n    try:\n        # Test access to channels collection\n        await self.channels_collection.count_documents({}, limit=1)\n        response_time = (time.time() - start_time) * 1000\n\n        return HealthStatus(\n            service_name=\"mongodb_channels\",\n            is_healthy=True,\n            response_time_ms=response_time,\n            message=\"Channels collection accessible\",\n        )\n    except Exception as e:\n        response_time = (time.time() - start_time) * 1000\n        return HealthStatus(\n            service_name=\"mongodb_channels\",\n            is_healthy=False,\n            response_time_ms=response_time,\n            message=f\"Health check failed: {str(e)}\",\n        )\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.ChannelStatsService.update_channel_stats","title":"<code>update_channel_stats(video_data)</code>  <code>async</code>","text":"<p>Update channel statistics based on video data with retry logic</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/stats.py</code> <pre><code>async def update_channel_stats(self, video_data: Dict[str, Any]) -&gt; OperationResult:\n    \"\"\"Update channel statistics based on video data with retry logic\"\"\"\n    channel_id = video_data.get(\"channel_id\")\n    if not channel_id:\n        return OperationResult.failure(\"Video data missing channel_id\")\n\n    async def _update_operation():\n        result = await self.channels_collection.update_one(\n            {\"channel_id\": channel_id},\n            {\n                \"$inc\": {\"video_count\": 1},\n                \"$set\": {\"last_activity\": datetime.now(timezone.utc)},\n                \"$setOnInsert\": {\"first_seen\": datetime.now(timezone.utc)},\n            },\n            upsert=True,\n        )\n        return result\n\n    try:\n        result = await self.retry.execute(\n            _update_operation, f\"update_channel_stats_{channel_id}\"\n        )\n        logger.debug(f\"Updated channel stats: {channel_id}\")\n\n        action = \"updated\" if result.matched_count &gt; 0 else \"created\"\n        return OperationResult.success(\n            f\"Channel stats {action}: {channel_id}\",\n            metadata={\"channel_id\": channel_id, \"action\": action},\n        )\n    except Exception as e:\n        logger.error(f\"Failed to update channel stats: {str(e)}\")\n        logger.debug(traceback.format_exc())\n        return OperationResult.failure(\n            f\"Failed to update channel stats: {str(e)}\", e\n        )\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.ElasticsearchConfig","title":"<code>ElasticsearchConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Elasticsearch indexing and search operations.</p> <p>This class manages Elasticsearch-specific settings including index configuration, field mappings, and analysis settings. It provides a computed mapping property that generates the complete index configuration based on the configured parameters.</p> <p>The mapping includes optimized field types for video metadata, proper analyzers for text search, and index settings for performance tuning.</p> <p>Attributes:</p> Name Type Description <code>index_name</code> <code>str</code> <p>Name of the Elasticsearch index for storing video documents.</p> <code>shards</code> <code>int</code> <p>Number of primary shards for the index. More shards allow better distribution across nodes but increase overhead.</p> <code>replicas</code> <code>int</code> <p>Number of replica shards for each primary shard. Replicas provide redundancy and can improve search throughput.</p> Example <p>config = ElasticsearchConfig( ...     index_name=\"videos_production\", ...     shards=3, ...     replicas=2 ... )</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/config.py</code> <pre><code>@dataclass\nclass ElasticsearchConfig:\n    \"\"\"Configuration for Elasticsearch indexing and search operations.\n\n    This class manages Elasticsearch-specific settings including index configuration,\n    field mappings, and analysis settings. It provides a computed mapping property\n    that generates the complete index configuration based on the configured parameters.\n\n    The mapping includes optimized field types for video metadata, proper analyzers\n    for text search, and index settings for performance tuning.\n\n    Attributes:\n        index_name: Name of the Elasticsearch index for storing video documents.\n        shards: Number of primary shards for the index. More shards allow better\n            distribution across nodes but increase overhead.\n        replicas: Number of replica shards for each primary shard. Replicas provide\n            redundancy and can improve search throughput.\n\n    Example:\n        &gt;&gt;&gt; config = ElasticsearchConfig(\n        ...     index_name=\"videos_production\",\n        ...     shards=3,\n        ...     replicas=2\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Create index with the computed mapping\n        &gt;&gt;&gt; es_client.indices.create(\n        ...     index=config.index_name,\n        ...     body=config.mapping\n        ... )\n    \"\"\"\n\n    index_name: str = \"videos\"\n    shards: int = 1\n    replicas: int = 0\n\n    @property\n    def mapping(self) -&gt; Dict[str, Any]:\n        \"\"\"Generate the complete Elasticsearch index mapping and settings.\n\n        Creates a comprehensive mapping configuration that defines how video\n        documents are indexed and stored. The mapping includes:\n\n        - Keyword fields for exact matching (video_id, channel_id, tags)\n        - Text fields with standard analyzer for full-text search\n        - Multi-field configurations for both search and aggregations\n        - Appropriate data types for metrics and timestamps\n        - Index settings based on configured shard and replica counts\n\n        Returns:\n            Dictionary containing the complete Elasticsearch mapping configuration\n            with both field mappings and index settings.\n\n        Note:\n            The mapping is generated dynamically based on current attribute values,\n            so changes to shards or replicas will be reflected in subsequent calls.\n\n        Example:\n            &gt;&gt;&gt; config = ElasticsearchConfig(shards=2, replicas=1)\n            &gt;&gt;&gt; mapping = config.mapping\n            &gt;&gt;&gt; print(mapping[\"settings\"][\"number_of_shards\"])  # 2\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Text fields support both search and keyword aggregations\n            &gt;&gt;&gt; title_mapping = mapping[\"mappings\"][\"properties\"][\"title\"]\n            &gt;&gt;&gt; print(title_mapping[\"type\"])  # \"text\"\n            &gt;&gt;&gt; print(title_mapping[\"fields\"][\"keyword\"][\"type\"])  # \"keyword\"\n        \"\"\"\n        return {\n            \"mappings\": {\n                \"properties\": {\n                    \"video_id\": {\"type\": \"keyword\"},\n                    \"channel_id\": {\"type\": \"keyword\"},\n                    \"title\": {\n                        \"type\": \"text\",\n                        \"analyzer\": \"standard\",\n                        \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n                    },\n                    \"description\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n                    \"published\": {\"type\": \"date\"},\n                    \"updated\": {\"type\": \"date\"},\n                    \"author\": {\n                        \"type\": \"text\",\n                        \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n                    },\n                    \"tags\": {\"type\": \"keyword\"},\n                    \"categories\": {\"type\": \"keyword\"},\n                    \"duration\": {\"type\": \"integer\"},\n                    \"view_count\": {\"type\": \"long\"},\n                    \"like_count\": {\"type\": \"long\"},\n                    \"comment_count\": {\"type\": \"long\"},\n                    \"processed_at\": {\"type\": \"date\"},\n                }\n            },\n            \"settings\": {\n                \"number_of_shards\": self.shards,\n                \"number_of_replicas\": self.replicas,\n            },\n        }\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.ElasticsearchConfig--create-index-with-the-computed-mapping","title":"Create index with the computed mapping","text":"<p>es_client.indices.create( ...     index=config.index_name, ...     body=config.mapping ... )</p>"},{"location":"documentation/indexer/#ytindexer.indexer.ElasticsearchConfig.mapping","title":"<code>mapping</code>  <code>property</code>","text":"<p>Generate the complete Elasticsearch index mapping and settings.</p> <p>Creates a comprehensive mapping configuration that defines how video documents are indexed and stored. The mapping includes:</p> <ul> <li>Keyword fields for exact matching (video_id, channel_id, tags)</li> <li>Text fields with standard analyzer for full-text search</li> <li>Multi-field configurations for both search and aggregations</li> <li>Appropriate data types for metrics and timestamps</li> <li>Index settings based on configured shard and replica counts</li> </ul> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing the complete Elasticsearch mapping configuration</p> <code>Dict[str, Any]</code> <p>with both field mappings and index settings.</p> Note <p>The mapping is generated dynamically based on current attribute values, so changes to shards or replicas will be reflected in subsequent calls.</p> Example <p>config = ElasticsearchConfig(shards=2, replicas=1) mapping = config.mapping print(mapping[\"settings\"][\"number_of_shards\"])  # 2</p>"},{"location":"documentation/indexer/#ytindexer.indexer.ElasticsearchConfig.mapping--text-fields-support-both-search-and-keyword-aggregations","title":"Text fields support both search and keyword aggregations","text":"<p>title_mapping = mapping[\"mappings\"][\"properties\"][\"title\"] print(title_mapping[\"type\"])  # \"text\" print(title_mapping[\"fields\"][\"keyword\"][\"type\"])  # \"keyword\"</p>"},{"location":"documentation/indexer/#ytindexer.indexer.MongoDBConfig","title":"<code>MongoDBConfig</code>  <code>dataclass</code>","text":"<p>Configuration for MongoDB collections and database indexes.</p> <p>This class manages MongoDB-specific configuration including database and collection names, as well as index definitions for optimal query performance. It provides computed properties that generate index configurations for different collection types.</p> <p>The index configurations are optimized for common query patterns including lookups by ID, filtering by channel, date-based queries, and subscription management operations.</p> <p>Attributes:</p> Name Type Description <code>database_name</code> <code>str</code> <p>Name of the MongoDB database containing the collections.</p> <code>videos_collection</code> <code>str</code> <p>Name of the collection storing video metadata documents.</p> <code>channels_collection</code> <code>str</code> <p>Name of the collection storing channel information.</p> Example <p>config = MongoDBConfig( ...     database_name=\"video_platform\", ...     videos_collection=\"video_metadata\", ...     channels_collection=\"channel_data\" ... )</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/config.py</code> <pre><code>@dataclass\nclass MongoDBConfig:\n    \"\"\"Configuration for MongoDB collections and database indexes.\n\n    This class manages MongoDB-specific configuration including database and\n    collection names, as well as index definitions for optimal query performance.\n    It provides computed properties that generate index configurations for\n    different collection types.\n\n    The index configurations are optimized for common query patterns including\n    lookups by ID, filtering by channel, date-based queries, and subscription\n    management operations.\n\n    Attributes:\n        database_name: Name of the MongoDB database containing the collections.\n        videos_collection: Name of the collection storing video metadata documents.\n        channels_collection: Name of the collection storing channel information.\n\n    Example:\n        &gt;&gt;&gt; config = MongoDBConfig(\n        ...     database_name=\"video_platform\",\n        ...     videos_collection=\"video_metadata\",\n        ...     channels_collection=\"channel_data\"\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Create indexes for optimal query performance\n        &gt;&gt;&gt; db = mongo_client[config.database_name]\n        &gt;&gt;&gt; videos = db[config.videos_collection]\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; for index_config in config.video_indexes.values():\n        ...     videos.create_index(index_config[\"key\"], **index_config)\n    \"\"\"\n\n    database_name: str = \"mongo\"\n    videos_collection: str = \"videos\"\n    channels_collection: str = \"channels\"\n\n    @property\n    def video_indexes(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Generate index configurations for the videos collection.\n\n        Creates index definitions optimized for common video query patterns\n        including unique video lookups, channel-based filtering, and date-based\n        sorting and filtering operations.\n\n        Returns:\n            Dictionary mapping index names to their MongoDB index configurations.\n            Each configuration includes the key specification, uniqueness constraint,\n            and index name.\n\n        Note:\n            The video_id index enforces uniqueness to prevent duplicate video\n            documents, while other indexes are non-unique to support filtering\n            and sorting operations.\n\n        Example:\n            &gt;&gt;&gt; config = MongoDBConfig()\n            &gt;&gt;&gt; indexes = config.video_indexes\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Unique index for video lookups\n            &gt;&gt;&gt; video_id_idx = indexes[\"video_id\"]\n            &gt;&gt;&gt; print(video_id_idx[\"unique\"])  # True\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Non-unique index for channel filtering\n            &gt;&gt;&gt; channel_idx = indexes[\"channel_id_non\"]\n            &gt;&gt;&gt; print(channel_idx[\"unique\"])  # False\n        \"\"\"\n        return {\n            \"video_id\": {\n                \"key\": [(\"video_id\", 1)],\n                \"unique\": True,\n                \"name\": \"video_id_idx\",\n            },\n            \"channel_id_non\": {\n                \"key\": [(\"channel_id\", 1)],\n                \"unique\": False,\n                \"name\": \"channel_id_non_idx\",\n            },\n            \"published_non\": {\n                \"key\": [(\"published\", 1)],\n                \"unique\": False,\n                \"name\": \"published_non_idx\",\n            },\n        }\n\n    @property\n    def channel_indexes(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Generate index configurations for the channels collection.\n\n        Creates index definitions for channel-related queries, primarily focused\n        on unique channel identification and lookups.\n\n        Returns:\n            Dictionary mapping index names to their MongoDB index configurations\n            for the channels collection.\n\n        Example:\n            &gt;&gt;&gt; config = MongoDBConfig()\n            &gt;&gt;&gt; channel_indexes = config.channel_indexes\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Unique index for channel identification\n            &gt;&gt;&gt; channel_idx = channel_indexes[\"channel_id\"]\n            &gt;&gt;&gt; print(channel_idx[\"unique\"])  # True\n        \"\"\"\n        return {\n            \"channel_id\": {\n                \"key\": [(\"channel_id\", 1)],\n                \"unique\": True,\n                \"name\": \"channel_id_idx\",\n            }\n        }\n\n    @property\n    def subscription_indexes(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Generate index configurations for the subscriptions collection.\n\n        Creates index definitions optimized for subscription management operations\n        including unique subscription lookups, expiration queries, and active\n        subscription filtering. Includes a compound index for efficient queries\n        on expiring active subscriptions.\n\n        Returns:\n            Dictionary mapping index names to their MongoDB index configurations\n            for subscription management.\n\n        Note:\n            The compound index on expires_at and is_active enables efficient\n            queries for finding subscriptions that need renewal, which is a\n            common operation in subscription management workflows.\n\n        Example:\n            &gt;&gt;&gt; config = MongoDBConfig()\n            &gt;&gt;&gt; sub_indexes = config.subscription_indexes\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Unique constraint on channel subscriptions\n            &gt;&gt;&gt; channel_idx = sub_indexes[\"channel_id\"]\n            &gt;&gt;&gt; print(channel_idx[\"unique\"])  # True\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Compound index for expiration queries\n            &gt;&gt;&gt; compound_idx = sub_indexes[\"expires_at_active\"]\n            &gt;&gt;&gt; print(compound_idx)  # [(\"expires_at\", 1), (\"is_active\", 1)]\n        \"\"\"\n        return {\n            \"channel_id\": {\"unique\": True},\n            \"expires_at\": {},\n            \"is_active\": {},\n            \"expires_at_active\": [(\"expires_at\", 1), (\"is_active\", 1)],\n        }\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.MongoDBConfig--create-indexes-for-optimal-query-performance","title":"Create indexes for optimal query performance","text":"<p>db = mongo_client[config.database_name] videos = db[config.videos_collection]</p> <p>for index_config in config.video_indexes.values(): ...     videos.create_index(index_config[\"key\"], **index_config)</p>"},{"location":"documentation/indexer/#ytindexer.indexer.MongoDBConfig.channel_indexes","title":"<code>channel_indexes</code>  <code>property</code>","text":"<p>Generate index configurations for the channels collection.</p> <p>Creates index definitions for channel-related queries, primarily focused on unique channel identification and lookups.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping index names to their MongoDB index configurations</p> <code>Dict[str, Dict[str, Any]]</code> <p>for the channels collection.</p> Example <p>config = MongoDBConfig() channel_indexes = config.channel_indexes</p>"},{"location":"documentation/indexer/#ytindexer.indexer.MongoDBConfig.channel_indexes--unique-index-for-channel-identification","title":"Unique index for channel identification","text":"<p>channel_idx = channel_indexes[\"channel_id\"] print(channel_idx[\"unique\"])  # True</p>"},{"location":"documentation/indexer/#ytindexer.indexer.MongoDBConfig.subscription_indexes","title":"<code>subscription_indexes</code>  <code>property</code>","text":"<p>Generate index configurations for the subscriptions collection.</p> <p>Creates index definitions optimized for subscription management operations including unique subscription lookups, expiration queries, and active subscription filtering. Includes a compound index for efficient queries on expiring active subscriptions.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping index names to their MongoDB index configurations</p> <code>Dict[str, Dict[str, Any]]</code> <p>for subscription management.</p> Note <p>The compound index on expires_at and is_active enables efficient queries for finding subscriptions that need renewal, which is a common operation in subscription management workflows.</p> Example <p>config = MongoDBConfig() sub_indexes = config.subscription_indexes</p>"},{"location":"documentation/indexer/#ytindexer.indexer.MongoDBConfig.subscription_indexes--unique-constraint-on-channel-subscriptions","title":"Unique constraint on channel subscriptions","text":"<p>channel_idx = sub_indexes[\"channel_id\"] print(channel_idx[\"unique\"])  # True</p>"},{"location":"documentation/indexer/#ytindexer.indexer.MongoDBConfig.subscription_indexes--compound-index-for-expiration-queries","title":"Compound index for expiration queries","text":"<p>compound_idx = sub_indexes[\"expires_at_active\"] print(compound_idx)  # [(\"expires_at\", 1), (\"is_active\", 1)]</p>"},{"location":"documentation/indexer/#ytindexer.indexer.MongoDBConfig.video_indexes","title":"<code>video_indexes</code>  <code>property</code>","text":"<p>Generate index configurations for the videos collection.</p> <p>Creates index definitions optimized for common video query patterns including unique video lookups, channel-based filtering, and date-based sorting and filtering operations.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping index names to their MongoDB index configurations.</p> <code>Dict[str, Dict[str, Any]]</code> <p>Each configuration includes the key specification, uniqueness constraint,</p> <code>Dict[str, Dict[str, Any]]</code> <p>and index name.</p> Note <p>The video_id index enforces uniqueness to prevent duplicate video documents, while other indexes are non-unique to support filtering and sorting operations.</p> Example <p>config = MongoDBConfig() indexes = config.video_indexes</p>"},{"location":"documentation/indexer/#ytindexer.indexer.MongoDBConfig.video_indexes--unique-index-for-video-lookups","title":"Unique index for video lookups","text":"<p>video_id_idx = indexes[\"video_id\"] print(video_id_idx[\"unique\"])  # True</p>"},{"location":"documentation/indexer/#ytindexer.indexer.MongoDBConfig.video_indexes--non-unique-index-for-channel-filtering","title":"Non-unique index for channel filtering","text":"<p>channel_idx = indexes[\"channel_id_non\"] print(channel_idx[\"unique\"])  # False</p>"},{"location":"documentation/indexer/#ytindexer.indexer.RetryConfig","title":"<code>RetryConfig</code>  <code>dataclass</code>","text":"<p>Configuration for retry logic with exponential backoff.</p> <p>This class defines parameters for implementing robust retry mechanisms with exponential backoff to handle transient failures gracefully. The configuration controls retry attempts, timing, and backoff behavior.</p> <p>The exponential backoff algorithm increases delays between retry attempts to reduce load on failing systems and improve the likelihood of eventual success. The max_delay parameter prevents delays from becoming excessive.</p> <p>Attributes:</p> Name Type Description <code>max_attempts</code> <code>int</code> <p>Maximum number of retry attempts before giving up. Includes the initial attempt, so max_attempts=3 means 1 initial attempt plus 2 retries.</p> <code>base_delay</code> <code>float</code> <p>Initial delay in seconds before the first retry attempt. Subsequent delays are calculated using exponential backoff.</p> <code>max_delay</code> <code>float</code> <p>Maximum delay in seconds between retry attempts. Prevents exponential backoff from creating excessively long delays.</p> <code>exponential_base</code> <code>float</code> <p>Base for exponential backoff calculation. Common values are 2.0 (doubling) or 1.5 (50% increase per attempt).</p> Example Note <p>Actual delays include random jitter to prevent thundering herd problems when multiple clients retry simultaneously. The jitter is typically 10-30% of the calculated delay.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/config.py</code> <pre><code>@dataclass\nclass RetryConfig:\n    \"\"\"Configuration for retry logic with exponential backoff.\n\n    This class defines parameters for implementing robust retry mechanisms\n    with exponential backoff to handle transient failures gracefully. The\n    configuration controls retry attempts, timing, and backoff behavior.\n\n    The exponential backoff algorithm increases delays between retry attempts\n    to reduce load on failing systems and improve the likelihood of eventual\n    success. The max_delay parameter prevents delays from becoming excessive.\n\n    Attributes:\n        max_attempts: Maximum number of retry attempts before giving up.\n            Includes the initial attempt, so max_attempts=3 means 1 initial\n            attempt plus 2 retries.\n        base_delay: Initial delay in seconds before the first retry attempt.\n            Subsequent delays are calculated using exponential backoff.\n        max_delay: Maximum delay in seconds between retry attempts. Prevents\n            exponential backoff from creating excessively long delays.\n        exponential_base: Base for exponential backoff calculation. Common\n            values are 2.0 (doubling) or 1.5 (50% increase per attempt).\n\n    Example:\n        &gt;&gt;&gt; # Conservative retry configuration\n        &gt;&gt;&gt; config = RetryConfig(\n        ...     max_attempts=3,\n        ...     base_delay=1.0,\n        ...     max_delay=30.0,\n        ...     exponential_base=2.0\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Aggressive retry for critical operations\n        &gt;&gt;&gt; critical_config = RetryConfig(\n        ...     max_attempts=10,\n        ...     base_delay=0.5,\n        ...     max_delay=120.0,\n        ...     exponential_base=1.5\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Delay calculation example:\n        &gt;&gt;&gt; # Attempt 1: base_delay * (exponential_base ^ 0) = 1.0 * 1 = 1.0s\n        &gt;&gt;&gt; # Attempt 2: base_delay * (exponential_base ^ 1) = 1.0 * 2 = 2.0s  \n        &gt;&gt;&gt; # Attempt 3: base_delay * (exponential_base ^ 2) = 1.0 * 4 = 4.0s\n\n    Note:\n        Actual delays include random jitter to prevent thundering herd problems\n        when multiple clients retry simultaneously. The jitter is typically\n        10-30% of the calculated delay.\n    \"\"\"\n\n    max_attempts: int = 3\n    base_delay: float = 1.0\n    max_delay: float = 60.0\n    exponential_base: float = 2.0\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.RetryConfig--conservative-retry-configuration","title":"Conservative retry configuration","text":"<p>config = RetryConfig( ...     max_attempts=3, ...     base_delay=1.0, ...     max_delay=30.0, ...     exponential_base=2.0 ... )</p>"},{"location":"documentation/indexer/#ytindexer.indexer.RetryConfig--aggressive-retry-for-critical-operations","title":"Aggressive retry for critical operations","text":"<p>critical_config = RetryConfig( ...     max_attempts=10, ...     base_delay=0.5, ...     max_delay=120.0, ...     exponential_base=1.5 ... )</p>"},{"location":"documentation/indexer/#ytindexer.indexer.RetryConfig--delay-calculation-example","title":"Delay calculation example:","text":""},{"location":"documentation/indexer/#ytindexer.indexer.RetryConfig--attempt-1-base_delay-exponential_base-0-10-1-10s","title":"Attempt 1: base_delay * (exponential_base ^ 0) = 1.0 * 1 = 1.0s","text":""},{"location":"documentation/indexer/#ytindexer.indexer.RetryConfig--attempt-2-base_delay-exponential_base-1-10-2-20s","title":"Attempt 2: base_delay * (exponential_base ^ 1) = 1.0 * 2 = 2.0s","text":""},{"location":"documentation/indexer/#ytindexer.indexer.RetryConfig--attempt-3-base_delay-exponential_base-2-10-4-40s","title":"Attempt 3: base_delay * (exponential_base ^ 2) = 1.0 * 4 = 4.0s","text":""},{"location":"documentation/indexer/#ytindexer.indexer.SearchIndexingService","title":"<code>SearchIndexingService</code>","text":"<p>               Bases: <code>HealthCheckable</code></p> <p>Handles video search indexing in Elasticsearch</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/indexing.py</code> <pre><code>class SearchIndexingService(HealthCheckable):\n    \"\"\"Handles video search indexing in Elasticsearch\"\"\"\n\n    def __init__(\n        self, client: Any, config: ElasticsearchConfig, retry_config: RetryConfig\n    ):\n        self.client = client\n        self.config = config\n        self.retry = RetryableOperation(retry_config)\n        logger.info(\"Initialized SearchIndexingService\")\n\n    async def ensure_index(self) -&gt; OperationResult:\n        \"\"\"Create Elasticsearch index if it doesn't exist\"\"\"\n        try:\n            if not await self.client.indices.exists(index=self.config.index_name):\n                await self.client.indices.create(\n                    index=self.config.index_name, body=self.config.mapping\n                )\n                logger.info(f\"Created Elasticsearch index: {self.config.index_name}\")\n                return OperationResult.success(\n                    f\"Created index: {self.config.index_name}\"\n                )\n            else:\n                return OperationResult.success(\n                    f\"Index already exists: {self.config.index_name}\"\n                )\n        except Exception as e:\n            logger.error(f\"Failed to ensure index: {str(e)}\")\n            return OperationResult.failure(f\"Failed to ensure index: {str(e)}\", e)\n\n    async def index_video(self, video_data: Dict[str, Any]) -&gt; OperationResult:\n        \"\"\"Index video metadata in Elasticsearch with retry logic\"\"\"\n        video_id = video_data.get(\"video_id\")\n        if not video_id:\n            return OperationResult.failure(\"Video data missing video_id\")\n\n        async def _index_operation():\n            await self.client.index(\n                index=self.config.index_name, id=video_id, body=video_data, refresh=True\n            )\n            return video_id\n\n        try:\n            result_id = await self.retry.execute(\n                _index_operation, f\"index_video_{video_id}\"\n            )\n            logger.debug(f\"Indexed video in Elasticsearch: {result_id}\")\n            return OperationResult.success(\n                f\"Indexed video: {result_id}\", metadata={\"video_id\": result_id}\n            )\n        except Exception as e:\n            logger.error(f\"Failed to index video in Elasticsearch: {str(e)}\")\n            logger.debug(traceback.format_exc())\n            return OperationResult.failure(f\"Failed to index video: {str(e)}\", e)\n\n    async def health_check(self) -&gt; HealthStatus:\n        \"\"\"Check Elasticsearch cluster health\"\"\"\n        start_time = time.time()\n        try:\n            health = await self.client.cluster.health()\n            response_time = (time.time() - start_time) * 1000\n\n            is_healthy = health.get(\"status\") in [\"green\", \"yellow\"]\n            message = f\"Cluster status: {health.get('status', 'unknown')}\"\n\n            return HealthStatus(\n                service_name=\"elasticsearch\",\n                is_healthy=is_healthy,\n                response_time_ms=response_time,\n                message=message,\n                metadata=health,\n            )\n        except Exception as e:\n            response_time = (time.time() - start_time) * 1000\n            return HealthStatus(\n                service_name=\"elasticsearch\",\n                is_healthy=False,\n                response_time_ms=response_time,\n                message=f\"Health check failed: {str(e)}\",\n            )\n\n    async def close(self):\n        \"\"\"Close the Elasticsearch client\"\"\"\n        await self.client.close()\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.SearchIndexingService.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the Elasticsearch client</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/indexing.py</code> <pre><code>async def close(self):\n    \"\"\"Close the Elasticsearch client\"\"\"\n    await self.client.close()\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.SearchIndexingService.ensure_index","title":"<code>ensure_index()</code>  <code>async</code>","text":"<p>Create Elasticsearch index if it doesn't exist</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/indexing.py</code> <pre><code>async def ensure_index(self) -&gt; OperationResult:\n    \"\"\"Create Elasticsearch index if it doesn't exist\"\"\"\n    try:\n        if not await self.client.indices.exists(index=self.config.index_name):\n            await self.client.indices.create(\n                index=self.config.index_name, body=self.config.mapping\n            )\n            logger.info(f\"Created Elasticsearch index: {self.config.index_name}\")\n            return OperationResult.success(\n                f\"Created index: {self.config.index_name}\"\n            )\n        else:\n            return OperationResult.success(\n                f\"Index already exists: {self.config.index_name}\"\n            )\n    except Exception as e:\n        logger.error(f\"Failed to ensure index: {str(e)}\")\n        return OperationResult.failure(f\"Failed to ensure index: {str(e)}\", e)\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.SearchIndexingService.health_check","title":"<code>health_check()</code>  <code>async</code>","text":"<p>Check Elasticsearch cluster health</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/indexing.py</code> <pre><code>async def health_check(self) -&gt; HealthStatus:\n    \"\"\"Check Elasticsearch cluster health\"\"\"\n    start_time = time.time()\n    try:\n        health = await self.client.cluster.health()\n        response_time = (time.time() - start_time) * 1000\n\n        is_healthy = health.get(\"status\") in [\"green\", \"yellow\"]\n        message = f\"Cluster status: {health.get('status', 'unknown')}\"\n\n        return HealthStatus(\n            service_name=\"elasticsearch\",\n            is_healthy=is_healthy,\n            response_time_ms=response_time,\n            message=message,\n            metadata=health,\n        )\n    except Exception as e:\n        response_time = (time.time() - start_time) * 1000\n        return HealthStatus(\n            service_name=\"elasticsearch\",\n            is_healthy=False,\n            response_time_ms=response_time,\n            message=f\"Health check failed: {str(e)}\",\n        )\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.SearchIndexingService.index_video","title":"<code>index_video(video_data)</code>  <code>async</code>","text":"<p>Index video metadata in Elasticsearch with retry logic</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/indexing.py</code> <pre><code>async def index_video(self, video_data: Dict[str, Any]) -&gt; OperationResult:\n    \"\"\"Index video metadata in Elasticsearch with retry logic\"\"\"\n    video_id = video_data.get(\"video_id\")\n    if not video_id:\n        return OperationResult.failure(\"Video data missing video_id\")\n\n    async def _index_operation():\n        await self.client.index(\n            index=self.config.index_name, id=video_id, body=video_data, refresh=True\n        )\n        return video_id\n\n    try:\n        result_id = await self.retry.execute(\n            _index_operation, f\"index_video_{video_id}\"\n        )\n        logger.debug(f\"Indexed video in Elasticsearch: {result_id}\")\n        return OperationResult.success(\n            f\"Indexed video: {result_id}\", metadata={\"video_id\": result_id}\n        )\n    except Exception as e:\n        logger.error(f\"Failed to index video in Elasticsearch: {str(e)}\")\n        logger.debug(traceback.format_exc())\n        return OperationResult.failure(f\"Failed to index video: {str(e)}\", e)\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoIndexingProcessor","title":"<code>VideoIndexingProcessor</code>","text":"<p>               Bases: <code>HealthCheckable</code></p> <p>Orchestrates the video indexing process with enhanced error handling and health checks</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/processor.py</code> <pre><code>class VideoIndexingProcessor(HealthCheckable):\n    \"\"\"Orchestrates the video indexing process with enhanced error handling and health checks\"\"\"\n\n    def __init__(\n        self,\n        input_queue: Queue,\n        video_storage: VideoStorageService,\n        search_indexing: SearchIndexingService,\n        channel_stats: ChannelStatsService,\n        transcript_service: VideoTranscriptService,\n        max_concurrent_tasks: int = 10,\n        poll_interval: float = 1.0,\n    ):\n        self.input_queue = input_queue\n        self.video_storage = video_storage\n        self.search_indexing = search_indexing\n        self.channel_stats = channel_stats\n        self.transcript_service = transcript_service\n        self.max_concurrent_tasks = max_concurrent_tasks\n        self.poll_interval = poll_interval\n        self._running = False\n        self._shutdown_event = asyncio.Event()\n        self._active_tasks: set = set()\n        logger.info(\"Initialized VideoIndexingProcessor\")\n\n    async def ensure_indices(self) -&gt; OperationResult:\n        \"\"\"Ensure all required database indices and mappings exist\"\"\"\n        results = []\n\n        storage_result = await self.video_storage.ensure_indices()\n        results.append((\"storage\", storage_result))\n\n        indexing_result = await self.search_indexing.ensure_index()\n        results.append((\"indexing\", indexing_result))\n\n        stats_result = await self.channel_stats.ensure_indices()\n        results.append((\"stats\", stats_result))\n\n        logger.info(results)\n\n        failed_services = [name for name, result in results if result.is_failure]\n\n        if failed_services:\n            return OperationResult.failure(\n                f\"Failed to ensure indices for: {failed_services}\",\n                metadata={\n                    \"results\": {name: result.message for name, result in results}\n                },\n            )\n        else:\n            return OperationResult.success(\n                \"All indices ensured successfully\",\n                metadata={\n                    \"results\": {name: result.message for name, result in results}\n                },\n            )\n\n    async def _enrich_video_with_transcript(\n        self, video_data: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Enrich video data with transcript information\"\"\"\n        video_id = video_data.get(\"video_id\")\n        if not video_id:\n            logger.warning(\n                \"Video data missing video_id, skipping transcript enrichment\"\n            )\n            return video_data\n\n        try:\n            # Get transcript in a thread pool to avoid blocking\n            loop = asyncio.get_event_loop()\n            transcript = await loop.run_in_executor(\n                None, self.transcript_service.get_transcript, video_id\n            )\n\n            if transcript:\n                video_data[\"transcript\"] = transcript\n                logger.debug(f\"Added transcript to video {video_id}\")\n            else:\n                logger.debug(f\"No transcript available for video {video_id}\")\n                video_data[\"transcript\"] = None\n\n        except Exception as e:\n            logger.warning(f\"Failed to get transcript for video {video_id}: {e}\")\n            video_data[\"transcript\"] = None\n\n        return video_data\n\n    async def process_video(self, video_data: Dict[str, Any]) -&gt; ProcessingResult:\n        \"\"\"Process a single video through all indexing services\"\"\"\n        video_id = video_data.get(\"video_id\", \"unknown\")\n\n        # Enrich with transcript data (non-critical operation)\n        enriched_video_data = await self._enrich_video_with_transcript(video_data)\n\n        # Store in MongoDB (critical operation)\n        storage_result = await self.video_storage.store_video(video_data)\n\n        # Index in Elasticsearch (non-critical)\n        indexing_result = await self.search_indexing.index_video(video_data)\n\n        # Update channel statistics (non-critical)\n        stats_result = await self.channel_stats.update_channel_stats(video_data)\n\n        result = ProcessingResult(\n            video_id=video_id,\n            storage_result=storage_result,\n            indexing_result=indexing_result,\n            stats_result=stats_result,\n        )\n\n        if result.is_success:\n            has_transcript = enriched_video_data.get(\"transcript\") is not None\n            logger.debug(\n                f\"Successfully processed video: {video_id} (transcript: {has_transcript})\"\n            )\n        elif result.overall_status == OperationStatus.PARTIAL_SUCCESS:\n            logger.warning(\n                f\"Partially processed video {video_id}: storage succeeded but other operations failed\"\n            )\n        else:\n            logger.error(f\"Failed to process video {video_id}: storage failed\")\n\n        return result\n\n    async def _process_video_with_cleanup(\n        self, video_data: Dict[str, Any]\n    ) -&gt; ProcessingResult:\n        \"\"\"Process video and handle task cleanup\"\"\"\n        task = asyncio.current_task()\n        try:\n            return await self.process_video(video_data)\n        finally:\n            # Clean up task reference\n            if task:\n                self._active_tasks.discard(task)\n\n    async def run(self) -&gt; None:\n        \"\"\"Main processing loop - consumes videos from queue and processes them\"\"\"\n        logger.info(\"Starting VideoIndexingProcessor main loop\")\n        self._running = True\n\n        try:\n            while self._running and not self._shutdown_event.is_set():\n                try:\n                    # Check if we have capacity for more tasks\n                    if len(self._active_tasks) &gt;= self.max_concurrent_tasks:\n                        # Wait for some tasks to complete\n                        if self._active_tasks:\n                            _, self._active_tasks = await asyncio.wait(\n                                self._active_tasks,\n                                return_when=asyncio.FIRST_COMPLETED,\n                                timeout=self.poll_interval,\n                            )\n                        else:\n                            await asyncio.sleep(self.poll_interval)\n                        continue\n\n                    # Try to get video data from queue\n                    video_data = await self._get_next_video()\n\n                    if video_data is None:\n                        # No video available, wait before trying again\n                        await asyncio.sleep(self.poll_interval)\n                        continue\n\n                    # Create and track processing task\n                    task = asyncio.create_task(\n                        self._process_video_with_cleanup(video_data)\n                    )\n                    self._active_tasks.add(task)\n\n                    logger.debug(\n                        f\"Started processing task for video: {video_data.get('video_id', 'unknown')}\"\n                    )\n\n                except asyncio.CancelledError:\n                    logger.info(\"Processing loop cancelled\")\n                    break\n                except Exception as e:\n                    logger.error(f\"Error in main processing loop: {e}\")\n                    await asyncio.sleep(self.poll_interval)\n\n        finally:\n            logger.info(\"Shutting down VideoIndexingProcessor\")\n            await self._cleanup_active_tasks()\n\n    async def _get_next_video(self) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get next video from queue with timeout\"\"\"\n        try:\n            result = self.input_queue.dequeue()\n            return result\n        except Exception as e:\n            logger.error(f\"Error getting video from queue: {e}\")\n            return None\n\n    async def _cleanup_active_tasks(self) -&gt; None:\n        \"\"\"Clean up any remaining active tasks\"\"\"\n        if not self._active_tasks:\n            return\n\n        logger.info(f\"Waiting for {len(self._active_tasks)} active tasks to complete\")\n\n        # Give tasks a chance to complete gracefully\n        try:\n            await asyncio.wait_for(\n                asyncio.gather(*self._active_tasks, return_exceptions=True),\n                timeout=30.0,  # 30 second timeout for graceful shutdown\n            )\n        except asyncio.TimeoutError:\n            logger.warning(\n                \"Timeout waiting for tasks to complete, cancelling remaining tasks\"\n            )\n            for task in self._active_tasks:\n                if not task.done():\n                    task.cancel()\n\n            # Wait for cancellations to complete\n            await asyncio.gather(*self._active_tasks, return_exceptions=True)\n\n        self._active_tasks.clear()\n\n    async def stop(self) -&gt; None:\n        \"\"\"Gracefully stop the processor\"\"\"\n        logger.info(\"Stopping VideoIndexingProcessor\")\n        self._running = False\n        self._shutdown_event.set()\n\n    def is_running(self) -&gt; bool:\n        \"\"\"Check if the processor is currently running\"\"\"\n        return self._running\n\n    @property\n    def active_task_count(self) -&gt; int:\n        \"\"\"Get the number of currently active processing tasks\"\"\"\n        return len(self._active_tasks)\n\n    async def health_check(self) -&gt; HealthStatus:\n        \"\"\"Check health of all dependent services\"\"\"\n        start_time = time.time()\n\n        try:\n            # Check all services concurrently\n            health_checks = await asyncio.gather(\n                self.video_storage.health_check(),\n                self.search_indexing.health_check(),\n                self.channel_stats.health_check(),\n                return_exceptions=True,\n            )\n\n            response_time = (time.time() - start_time) * 1000\n\n            # Determine overall health\n            unhealthy_services = []\n            service_statuses = {}\n\n            for i, (service_name, check) in enumerate(\n                [\n                    (\"storage\", health_checks[0]),\n                    (\"indexing\", health_checks[1]),\n                    (\"stats\", health_checks[2]),\n                ]\n            ):\n                if isinstance(check, Exception):\n                    unhealthy_services.append(service_name)\n                    service_statuses[service_name] = f\"Error: {str(check)}\"\n                elif not check.is_healthy:\n                    unhealthy_services.append(service_name)\n                    service_statuses[service_name] = check.message\n                else:\n                    service_statuses[service_name] = \"healthy\"\n\n            is_healthy = len(unhealthy_services) == 0 and self._running\n            processor_status = (\n                f\"running ({self.active_task_count} active tasks)\"\n                if self._running\n                else \"stopped\"\n            )\n            message = f\"Processor {processor_status}. \" + (\n                \"All services healthy\"\n                if len(unhealthy_services) == 0\n                else f\"Unhealthy services: {unhealthy_services}\"\n            )\n\n            return HealthStatus(\n                service_name=\"video_processor\",\n                is_healthy=is_healthy,\n                response_time_ms=response_time,\n                message=message,\n                metadata={\n                    \"services\": service_statuses,\n                    \"active_tasks\": self.active_task_count,\n                    \"is_running\": self._running,\n                },\n            )\n\n        except Exception as e:\n            response_time = (time.time() - start_time) * 1000\n            return HealthStatus(\n                service_name=\"video_processor\",\n                is_healthy=False,\n                response_time_ms=response_time,\n                message=f\"Health check failed: {str(e)}\",\n            )\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoIndexingProcessor.active_task_count","title":"<code>active_task_count</code>  <code>property</code>","text":"<p>Get the number of currently active processing tasks</p>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoIndexingProcessor.ensure_indices","title":"<code>ensure_indices()</code>  <code>async</code>","text":"<p>Ensure all required database indices and mappings exist</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/processor.py</code> <pre><code>async def ensure_indices(self) -&gt; OperationResult:\n    \"\"\"Ensure all required database indices and mappings exist\"\"\"\n    results = []\n\n    storage_result = await self.video_storage.ensure_indices()\n    results.append((\"storage\", storage_result))\n\n    indexing_result = await self.search_indexing.ensure_index()\n    results.append((\"indexing\", indexing_result))\n\n    stats_result = await self.channel_stats.ensure_indices()\n    results.append((\"stats\", stats_result))\n\n    logger.info(results)\n\n    failed_services = [name for name, result in results if result.is_failure]\n\n    if failed_services:\n        return OperationResult.failure(\n            f\"Failed to ensure indices for: {failed_services}\",\n            metadata={\n                \"results\": {name: result.message for name, result in results}\n            },\n        )\n    else:\n        return OperationResult.success(\n            \"All indices ensured successfully\",\n            metadata={\n                \"results\": {name: result.message for name, result in results}\n            },\n        )\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoIndexingProcessor.health_check","title":"<code>health_check()</code>  <code>async</code>","text":"<p>Check health of all dependent services</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/processor.py</code> <pre><code>async def health_check(self) -&gt; HealthStatus:\n    \"\"\"Check health of all dependent services\"\"\"\n    start_time = time.time()\n\n    try:\n        # Check all services concurrently\n        health_checks = await asyncio.gather(\n            self.video_storage.health_check(),\n            self.search_indexing.health_check(),\n            self.channel_stats.health_check(),\n            return_exceptions=True,\n        )\n\n        response_time = (time.time() - start_time) * 1000\n\n        # Determine overall health\n        unhealthy_services = []\n        service_statuses = {}\n\n        for i, (service_name, check) in enumerate(\n            [\n                (\"storage\", health_checks[0]),\n                (\"indexing\", health_checks[1]),\n                (\"stats\", health_checks[2]),\n            ]\n        ):\n            if isinstance(check, Exception):\n                unhealthy_services.append(service_name)\n                service_statuses[service_name] = f\"Error: {str(check)}\"\n            elif not check.is_healthy:\n                unhealthy_services.append(service_name)\n                service_statuses[service_name] = check.message\n            else:\n                service_statuses[service_name] = \"healthy\"\n\n        is_healthy = len(unhealthy_services) == 0 and self._running\n        processor_status = (\n            f\"running ({self.active_task_count} active tasks)\"\n            if self._running\n            else \"stopped\"\n        )\n        message = f\"Processor {processor_status}. \" + (\n            \"All services healthy\"\n            if len(unhealthy_services) == 0\n            else f\"Unhealthy services: {unhealthy_services}\"\n        )\n\n        return HealthStatus(\n            service_name=\"video_processor\",\n            is_healthy=is_healthy,\n            response_time_ms=response_time,\n            message=message,\n            metadata={\n                \"services\": service_statuses,\n                \"active_tasks\": self.active_task_count,\n                \"is_running\": self._running,\n            },\n        )\n\n    except Exception as e:\n        response_time = (time.time() - start_time) * 1000\n        return HealthStatus(\n            service_name=\"video_processor\",\n            is_healthy=False,\n            response_time_ms=response_time,\n            message=f\"Health check failed: {str(e)}\",\n        )\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoIndexingProcessor.is_running","title":"<code>is_running()</code>","text":"<p>Check if the processor is currently running</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/processor.py</code> <pre><code>def is_running(self) -&gt; bool:\n    \"\"\"Check if the processor is currently running\"\"\"\n    return self._running\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoIndexingProcessor.process_video","title":"<code>process_video(video_data)</code>  <code>async</code>","text":"<p>Process a single video through all indexing services</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/processor.py</code> <pre><code>async def process_video(self, video_data: Dict[str, Any]) -&gt; ProcessingResult:\n    \"\"\"Process a single video through all indexing services\"\"\"\n    video_id = video_data.get(\"video_id\", \"unknown\")\n\n    # Enrich with transcript data (non-critical operation)\n    enriched_video_data = await self._enrich_video_with_transcript(video_data)\n\n    # Store in MongoDB (critical operation)\n    storage_result = await self.video_storage.store_video(video_data)\n\n    # Index in Elasticsearch (non-critical)\n    indexing_result = await self.search_indexing.index_video(video_data)\n\n    # Update channel statistics (non-critical)\n    stats_result = await self.channel_stats.update_channel_stats(video_data)\n\n    result = ProcessingResult(\n        video_id=video_id,\n        storage_result=storage_result,\n        indexing_result=indexing_result,\n        stats_result=stats_result,\n    )\n\n    if result.is_success:\n        has_transcript = enriched_video_data.get(\"transcript\") is not None\n        logger.debug(\n            f\"Successfully processed video: {video_id} (transcript: {has_transcript})\"\n        )\n    elif result.overall_status == OperationStatus.PARTIAL_SUCCESS:\n        logger.warning(\n            f\"Partially processed video {video_id}: storage succeeded but other operations failed\"\n        )\n    else:\n        logger.error(f\"Failed to process video {video_id}: storage failed\")\n\n    return result\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoIndexingProcessor.run","title":"<code>run()</code>  <code>async</code>","text":"<p>Main processing loop - consumes videos from queue and processes them</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/processor.py</code> <pre><code>async def run(self) -&gt; None:\n    \"\"\"Main processing loop - consumes videos from queue and processes them\"\"\"\n    logger.info(\"Starting VideoIndexingProcessor main loop\")\n    self._running = True\n\n    try:\n        while self._running and not self._shutdown_event.is_set():\n            try:\n                # Check if we have capacity for more tasks\n                if len(self._active_tasks) &gt;= self.max_concurrent_tasks:\n                    # Wait for some tasks to complete\n                    if self._active_tasks:\n                        _, self._active_tasks = await asyncio.wait(\n                            self._active_tasks,\n                            return_when=asyncio.FIRST_COMPLETED,\n                            timeout=self.poll_interval,\n                        )\n                    else:\n                        await asyncio.sleep(self.poll_interval)\n                    continue\n\n                # Try to get video data from queue\n                video_data = await self._get_next_video()\n\n                if video_data is None:\n                    # No video available, wait before trying again\n                    await asyncio.sleep(self.poll_interval)\n                    continue\n\n                # Create and track processing task\n                task = asyncio.create_task(\n                    self._process_video_with_cleanup(video_data)\n                )\n                self._active_tasks.add(task)\n\n                logger.debug(\n                    f\"Started processing task for video: {video_data.get('video_id', 'unknown')}\"\n                )\n\n            except asyncio.CancelledError:\n                logger.info(\"Processing loop cancelled\")\n                break\n            except Exception as e:\n                logger.error(f\"Error in main processing loop: {e}\")\n                await asyncio.sleep(self.poll_interval)\n\n    finally:\n        logger.info(\"Shutting down VideoIndexingProcessor\")\n        await self._cleanup_active_tasks()\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoIndexingProcessor.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Gracefully stop the processor</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/processor.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"Gracefully stop the processor\"\"\"\n    logger.info(\"Stopping VideoIndexingProcessor\")\n    self._running = False\n    self._shutdown_event.set()\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoStorageService","title":"<code>VideoStorageService</code>","text":"<p>               Bases: <code>HealthCheckable</code></p> <p>Handles video metadata storage in MongoDB</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/storage.py</code> <pre><code>class VideoStorageService(HealthCheckable):\n    \"\"\"Handles video metadata storage in MongoDB\"\"\"\n\n    def __init__(self, client: Any, config: MongoDBConfig, retry_config: RetryConfig):\n        self.client = client\n        self.config = config\n        self.retry = RetryableOperation(\n            retry_config, non_retry_exceptions=[DuplicateKeyError]\n        )\n        self.db = self.client[config.database_name]\n        self.videos_collection = self.db[config.videos_collection]\n        logger.info(\"Initialized VideoStorageService\")\n\n    async def ensure_indices(self) -&gt; OperationResult:\n        \"\"\"Ensure the required database indices exist\"\"\"\n        created_indexes = []\n        failed_indexes = []\n\n        for index_name, index_config in self.config.video_indexes.items():\n            try:\n                await self.videos_collection.create_index(index_name, **index_config)\n                created_indexes.append(index_name)\n                logger.debug(f\"Created index: {index_name}\")\n            except OperationFailure as e:\n                if \"already exists\" in str(e):\n                    logger.debug(f\"Index '{index_name}' already exists\")\n                    created_indexes.append(index_name)\n                else:\n                    failed_indexes.append(index_name)\n                    logger.error(f\"Failed to create index {index_name}: {str(e)}\")\n\n        if failed_indexes:\n            return OperationResult.failure(\n                f\"Failed to create indexes: {failed_indexes}\",\n                metadata={\"created\": created_indexes, \"failed\": failed_indexes},\n            )\n        else:\n            return OperationResult.success(\n                f\"Ensured indexes: {created_indexes}\",\n                metadata={\"indexes\": created_indexes},\n            )\n\n    async def store_video(self, video_data: Dict[str, Any]) -&gt; OperationResult:\n        \"\"\"Store video metadata in MongoDB with retry logic\"\"\"\n        video_id = video_data.get(\"video_id\")\n        if not video_id:\n            return OperationResult.failure(\"Video data missing video_id\")\n\n        async def _store_operation():\n            result = await self.videos_collection.update_one(\n                {\"video_id\": video_id},\n                {\"$set\": {**video_data, \"updated_at\": datetime.now(timezone.utc)}},\n                upsert=True,\n            )\n            return result\n\n        try:\n            result = await self.retry.execute(\n                _store_operation, f\"store_video_{video_id}\"\n            )\n            logger.debug(f\"Stored video in MongoDB: {video_id}\")\n\n            action = \"updated\" if result.matched_count &gt; 0 else \"inserted\"\n\n            return OperationResult.success(\n                f\"Video {action}: {video_id}\",\n                metadata={\"video_id\": video_id, \"action\": action},\n            )\n        except Exception as e:\n            logger.error(f\"Failed to store video in MongoDB: {str(e)}\")\n            logger.debug(traceback.format_exc())\n            return OperationResult.failure(f\"Failed to store video: {str(e)}\", e)\n\n    async def health_check(self) -&gt; HealthStatus:\n        \"\"\"Check MongoDB connection health\"\"\"\n        start_time = time.time()\n        try:\n            # Simple ping to check connection\n            await self.client.admin.command(\"ping\")\n            response_time = (time.time() - start_time) * 1000\n\n            return HealthStatus(\n                service_name=\"mongodb\",\n                is_healthy=True,\n                response_time_ms=response_time,\n                message=\"Connection healthy\",\n            )\n        except Exception as e:\n            response_time = (time.time() - start_time) * 1000\n            return HealthStatus(\n                service_name=\"mongodb\",\n                is_healthy=False,\n                response_time_ms=response_time,\n                message=f\"Health check failed: {str(e)}\",\n            )\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoStorageService.ensure_indices","title":"<code>ensure_indices()</code>  <code>async</code>","text":"<p>Ensure the required database indices exist</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/storage.py</code> <pre><code>async def ensure_indices(self) -&gt; OperationResult:\n    \"\"\"Ensure the required database indices exist\"\"\"\n    created_indexes = []\n    failed_indexes = []\n\n    for index_name, index_config in self.config.video_indexes.items():\n        try:\n            await self.videos_collection.create_index(index_name, **index_config)\n            created_indexes.append(index_name)\n            logger.debug(f\"Created index: {index_name}\")\n        except OperationFailure as e:\n            if \"already exists\" in str(e):\n                logger.debug(f\"Index '{index_name}' already exists\")\n                created_indexes.append(index_name)\n            else:\n                failed_indexes.append(index_name)\n                logger.error(f\"Failed to create index {index_name}: {str(e)}\")\n\n    if failed_indexes:\n        return OperationResult.failure(\n            f\"Failed to create indexes: {failed_indexes}\",\n            metadata={\"created\": created_indexes, \"failed\": failed_indexes},\n        )\n    else:\n        return OperationResult.success(\n            f\"Ensured indexes: {created_indexes}\",\n            metadata={\"indexes\": created_indexes},\n        )\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoStorageService.health_check","title":"<code>health_check()</code>  <code>async</code>","text":"<p>Check MongoDB connection health</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/storage.py</code> <pre><code>async def health_check(self) -&gt; HealthStatus:\n    \"\"\"Check MongoDB connection health\"\"\"\n    start_time = time.time()\n    try:\n        # Simple ping to check connection\n        await self.client.admin.command(\"ping\")\n        response_time = (time.time() - start_time) * 1000\n\n        return HealthStatus(\n            service_name=\"mongodb\",\n            is_healthy=True,\n            response_time_ms=response_time,\n            message=\"Connection healthy\",\n        )\n    except Exception as e:\n        response_time = (time.time() - start_time) * 1000\n        return HealthStatus(\n            service_name=\"mongodb\",\n            is_healthy=False,\n            response_time_ms=response_time,\n            message=f\"Health check failed: {str(e)}\",\n        )\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoStorageService.store_video","title":"<code>store_video(video_data)</code>  <code>async</code>","text":"<p>Store video metadata in MongoDB with retry logic</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/storage.py</code> <pre><code>async def store_video(self, video_data: Dict[str, Any]) -&gt; OperationResult:\n    \"\"\"Store video metadata in MongoDB with retry logic\"\"\"\n    video_id = video_data.get(\"video_id\")\n    if not video_id:\n        return OperationResult.failure(\"Video data missing video_id\")\n\n    async def _store_operation():\n        result = await self.videos_collection.update_one(\n            {\"video_id\": video_id},\n            {\"$set\": {**video_data, \"updated_at\": datetime.now(timezone.utc)}},\n            upsert=True,\n        )\n        return result\n\n    try:\n        result = await self.retry.execute(\n            _store_operation, f\"store_video_{video_id}\"\n        )\n        logger.debug(f\"Stored video in MongoDB: {video_id}\")\n\n        action = \"updated\" if result.matched_count &gt; 0 else \"inserted\"\n\n        return OperationResult.success(\n            f\"Video {action}: {video_id}\",\n            metadata={\"video_id\": video_id, \"action\": action},\n        )\n    except Exception as e:\n        logger.error(f\"Failed to store video in MongoDB: {str(e)}\")\n        logger.debug(traceback.format_exc())\n        return OperationResult.failure(f\"Failed to store video: {str(e)}\", e)\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoTranscriptService","title":"<code>VideoTranscriptService</code>","text":"<p>Downloads YouTube video transcripts with multiple language support</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/transcript.py</code> <pre><code>class VideoTranscriptService:\n    \"\"\"Downloads YouTube video transcripts with multiple language support\"\"\"\n\n    def __init__(self, languages: Optional[List[str]] = None):\n        \"\"\"\n        Initialize the transcript service.\n\n        Args:\n            languages: Preferred languages in order of preference.\n                      Defaults to ['en', 'en-US'] if not provided.\n        \"\"\"\n        if languages is None:\n            languages = [\"en\", \"en-US\"]\n        self.languages = languages\n        self.formatter = TextFormatter()\n\n    def get_transcript(self, video_id: str) -&gt; Optional[str]:\n        \"\"\"\n        Download and format transcript for a YouTube video.\n\n        Args:\n            video_id: YouTube video ID\n\n        Returns:\n            Formatted transcript text or None if unavailable\n        \"\"\"\n        if not video_id or not video_id.strip():\n            logger.error(\"Empty or invalid video ID provided\")\n            return None\n\n        try:\n            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n\n            # Try preferred languages first\n            for language in self.languages:\n                try:\n                    transcript = transcript_list.find_transcript([language])\n                    transcript_data = transcript.fetch()\n                    formatted_text = self.formatter.format_transcript(transcript_data)\n                    logger.info(\n                        f\"Successfully retrieved transcript for {video_id} in {language}\"\n                    )\n                    return formatted_text\n                except NoTranscriptFound:\n                    logger.debug(\n                        f\"No transcript found for {video_id} in language {language}\"\n                    )\n                    continue\n                except Exception as e:\n                    logger.debug(\n                        f\"Error getting transcript in {language} for {video_id}: {e}\"\n                    )\n                    continue\n\n            # Fallback to generated English transcript\n            try:\n                transcript = transcript_list.find_generated_transcript([\"en\"])\n                transcript_data = transcript.fetch()\n                formatted_text = self.formatter.format_transcript(transcript_data)\n                logger.info(\n                    f\"Successfully retrieved generated English transcript for {video_id}\"\n                )\n                return formatted_text\n            except NoTranscriptFound:\n                logger.debug(f\"No generated English transcript found for {video_id}\")\n            except Exception as e:\n                logger.debug(f\"Error getting generated transcript for {video_id}: {e}\")\n\n            # Last resort: use any available transcript\n            try:\n                available_transcripts = list(transcript_list)\n                if available_transcripts:\n                    first_transcript = available_transcripts[0]\n                    transcript_data = first_transcript.fetch()\n                    formatted_text = self.formatter.format_transcript(transcript_data)\n                    logger.info(\n                        f\"Retrieved fallback transcript for {video_id} in {first_transcript.language}\"\n                    )\n                    return formatted_text\n                else:\n                    logger.warning(f\"No transcripts available for video {video_id}\")\n                    return None\n            except Exception as e:\n                logger.error(f\"Error fetching fallback transcript for {video_id}: {e}\")\n\n        except TranscriptsDisabled:\n            logger.warning(f\"Transcripts are disabled for video {video_id}\")\n            return None\n        except VideoUnavailable:\n            logger.error(f\"Video {video_id} is unavailable\")\n            return None\n        except NoTranscriptFound:\n            logger.warning(f\"No transcripts available for video {video_id}\")\n            return None\n        except CouldNotRetrieveTranscript as e:\n            logger.error(f\"Could not retrieve transcript for {video_id}: {e}\")\n            return None\n        except Exception as e:\n            logger.error(\n                f\"Unexpected error getting transcript for video {video_id}: {e}\"\n            )\n            return None\n\n    def get_transcript_with_timestamps(self, video_id: str) -&gt; Optional[List[Dict]]:\n        \"\"\"\n        Get transcript with timing information for each segment.\n\n        Args:\n            video_id: YouTube video ID\n\n        Returns:\n            List of transcript segments with 'text', 'start', 'duration' keys,\n            or None if transcript unavailable\n        \"\"\"\n        if not video_id or not video_id.strip():\n            logger.error(\"Empty or invalid video ID provided\")\n            return None\n\n        try:\n            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n\n            # Try preferred languages first\n            for language in self.languages:\n                try:\n                    transcript = transcript_list.find_transcript([language])\n                    transcript_data = transcript.fetch()\n                    logger.info(\n                        f\"Successfully retrieved timestamped transcript for {video_id} in {language}\"\n                    )\n                    return transcript_data\n                except NoTranscriptFound:\n                    logger.debug(\n                        f\"No transcript found for {video_id} in language {language}\"\n                    )\n                    continue\n                except Exception as e:\n                    logger.debug(\n                        f\"Error getting timestamped transcript in {language} for {video_id}: {e}\"\n                    )\n                    continue\n\n            # Fallback to generated English transcript\n            try:\n                transcript = transcript_list.find_generated_transcript([\"en\"])\n                transcript_data = transcript.fetch()\n                logger.info(\n                    f\"Successfully retrieved generated English timestamped transcript for {video_id}\"\n                )\n                return transcript_data\n            except NoTranscriptFound:\n                logger.debug(f\"No generated English transcript found for {video_id}\")\n            except Exception as e:\n                logger.debug(\n                    f\"Error getting generated timestamped transcript for {video_id}: {e}\"\n                )\n\n            # Last resort: use any available transcript\n            try:\n                available_transcripts = list(transcript_list)\n                if available_transcripts:\n                    first_transcript = available_transcripts[0]\n                    transcript_data = first_transcript.fetch()\n                    logger.info(\n                        f\"Retrieved fallback timestamped transcript for {video_id} in {first_transcript.language}\"\n                    )\n                    return transcript_data\n                else:\n                    logger.warning(f\"No transcripts available for video {video_id}\")\n                    return None\n            except Exception as e:\n                logger.error(\n                    f\"Error fetching fallback timestamped transcript for {video_id}: {e}\"\n                )\n\n        except TranscriptsDisabled:\n            logger.warning(f\"Transcripts are disabled for video {video_id}\")\n            return None\n        except VideoUnavailable:\n            logger.error(f\"Video {video_id} is unavailable\")\n            return None\n        except NoTranscriptFound:\n            logger.warning(f\"No transcripts available for video {video_id}\")\n            return None\n        except CouldNotRetrieveTranscript as e:\n            logger.error(\n                f\"Could not retrieve timestamped transcript for {video_id}: {e}\"\n            )\n            return None\n        except Exception as e:\n            logger.error(\n                f\"Unexpected error getting timestamped transcript for video {video_id}: {e}\"\n            )\n            return None\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoTranscriptService.__init__","title":"<code>__init__(languages=None)</code>","text":"<p>Initialize the transcript service.</p> <p>Parameters:</p> Name Type Description Default <code>languages</code> <code>Optional[List[str]]</code> <p>Preferred languages in order of preference.       Defaults to ['en', 'en-US'] if not provided.</p> <code>None</code> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/transcript.py</code> <pre><code>def __init__(self, languages: Optional[List[str]] = None):\n    \"\"\"\n    Initialize the transcript service.\n\n    Args:\n        languages: Preferred languages in order of preference.\n                  Defaults to ['en', 'en-US'] if not provided.\n    \"\"\"\n    if languages is None:\n        languages = [\"en\", \"en-US\"]\n    self.languages = languages\n    self.formatter = TextFormatter()\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoTranscriptService.get_transcript","title":"<code>get_transcript(video_id)</code>","text":"<p>Download and format transcript for a YouTube video.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>YouTube video ID</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Formatted transcript text or None if unavailable</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/transcript.py</code> <pre><code>def get_transcript(self, video_id: str) -&gt; Optional[str]:\n    \"\"\"\n    Download and format transcript for a YouTube video.\n\n    Args:\n        video_id: YouTube video ID\n\n    Returns:\n        Formatted transcript text or None if unavailable\n    \"\"\"\n    if not video_id or not video_id.strip():\n        logger.error(\"Empty or invalid video ID provided\")\n        return None\n\n    try:\n        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n\n        # Try preferred languages first\n        for language in self.languages:\n            try:\n                transcript = transcript_list.find_transcript([language])\n                transcript_data = transcript.fetch()\n                formatted_text = self.formatter.format_transcript(transcript_data)\n                logger.info(\n                    f\"Successfully retrieved transcript for {video_id} in {language}\"\n                )\n                return formatted_text\n            except NoTranscriptFound:\n                logger.debug(\n                    f\"No transcript found for {video_id} in language {language}\"\n                )\n                continue\n            except Exception as e:\n                logger.debug(\n                    f\"Error getting transcript in {language} for {video_id}: {e}\"\n                )\n                continue\n\n        # Fallback to generated English transcript\n        try:\n            transcript = transcript_list.find_generated_transcript([\"en\"])\n            transcript_data = transcript.fetch()\n            formatted_text = self.formatter.format_transcript(transcript_data)\n            logger.info(\n                f\"Successfully retrieved generated English transcript for {video_id}\"\n            )\n            return formatted_text\n        except NoTranscriptFound:\n            logger.debug(f\"No generated English transcript found for {video_id}\")\n        except Exception as e:\n            logger.debug(f\"Error getting generated transcript for {video_id}: {e}\")\n\n        # Last resort: use any available transcript\n        try:\n            available_transcripts = list(transcript_list)\n            if available_transcripts:\n                first_transcript = available_transcripts[0]\n                transcript_data = first_transcript.fetch()\n                formatted_text = self.formatter.format_transcript(transcript_data)\n                logger.info(\n                    f\"Retrieved fallback transcript for {video_id} in {first_transcript.language}\"\n                )\n                return formatted_text\n            else:\n                logger.warning(f\"No transcripts available for video {video_id}\")\n                return None\n        except Exception as e:\n            logger.error(f\"Error fetching fallback transcript for {video_id}: {e}\")\n\n    except TranscriptsDisabled:\n        logger.warning(f\"Transcripts are disabled for video {video_id}\")\n        return None\n    except VideoUnavailable:\n        logger.error(f\"Video {video_id} is unavailable\")\n        return None\n    except NoTranscriptFound:\n        logger.warning(f\"No transcripts available for video {video_id}\")\n        return None\n    except CouldNotRetrieveTranscript as e:\n        logger.error(f\"Could not retrieve transcript for {video_id}: {e}\")\n        return None\n    except Exception as e:\n        logger.error(\n            f\"Unexpected error getting transcript for video {video_id}: {e}\"\n        )\n        return None\n</code></pre>"},{"location":"documentation/indexer/#ytindexer.indexer.VideoTranscriptService.get_transcript_with_timestamps","title":"<code>get_transcript_with_timestamps(video_id)</code>","text":"<p>Get transcript with timing information for each segment.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>YouTube video ID</p> required <p>Returns:</p> Type Description <code>Optional[List[Dict]]</code> <p>List of transcript segments with 'text', 'start', 'duration' keys,</p> <code>Optional[List[Dict]]</code> <p>or None if transcript unavailable</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/indexer/transcript.py</code> <pre><code>def get_transcript_with_timestamps(self, video_id: str) -&gt; Optional[List[Dict]]:\n    \"\"\"\n    Get transcript with timing information for each segment.\n\n    Args:\n        video_id: YouTube video ID\n\n    Returns:\n        List of transcript segments with 'text', 'start', 'duration' keys,\n        or None if transcript unavailable\n    \"\"\"\n    if not video_id or not video_id.strip():\n        logger.error(\"Empty or invalid video ID provided\")\n        return None\n\n    try:\n        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n\n        # Try preferred languages first\n        for language in self.languages:\n            try:\n                transcript = transcript_list.find_transcript([language])\n                transcript_data = transcript.fetch()\n                logger.info(\n                    f\"Successfully retrieved timestamped transcript for {video_id} in {language}\"\n                )\n                return transcript_data\n            except NoTranscriptFound:\n                logger.debug(\n                    f\"No transcript found for {video_id} in language {language}\"\n                )\n                continue\n            except Exception as e:\n                logger.debug(\n                    f\"Error getting timestamped transcript in {language} for {video_id}: {e}\"\n                )\n                continue\n\n        # Fallback to generated English transcript\n        try:\n            transcript = transcript_list.find_generated_transcript([\"en\"])\n            transcript_data = transcript.fetch()\n            logger.info(\n                f\"Successfully retrieved generated English timestamped transcript for {video_id}\"\n            )\n            return transcript_data\n        except NoTranscriptFound:\n            logger.debug(f\"No generated English transcript found for {video_id}\")\n        except Exception as e:\n            logger.debug(\n                f\"Error getting generated timestamped transcript for {video_id}: {e}\"\n            )\n\n        # Last resort: use any available transcript\n        try:\n            available_transcripts = list(transcript_list)\n            if available_transcripts:\n                first_transcript = available_transcripts[0]\n                transcript_data = first_transcript.fetch()\n                logger.info(\n                    f\"Retrieved fallback timestamped transcript for {video_id} in {first_transcript.language}\"\n                )\n                return transcript_data\n            else:\n                logger.warning(f\"No transcripts available for video {video_id}\")\n                return None\n        except Exception as e:\n            logger.error(\n                f\"Error fetching fallback timestamped transcript for {video_id}: {e}\"\n            )\n\n    except TranscriptsDisabled:\n        logger.warning(f\"Transcripts are disabled for video {video_id}\")\n        return None\n    except VideoUnavailable:\n        logger.error(f\"Video {video_id} is unavailable\")\n        return None\n    except NoTranscriptFound:\n        logger.warning(f\"No transcripts available for video {video_id}\")\n        return None\n    except CouldNotRetrieveTranscript as e:\n        logger.error(\n            f\"Could not retrieve timestamped transcript for {video_id}: {e}\"\n        )\n        return None\n    except Exception as e:\n        logger.error(\n            f\"Unexpected error getting timestamped transcript for video {video_id}: {e}\"\n        )\n        return None\n</code></pre>"},{"location":"documentation/queues/","title":"Queues","text":""},{"location":"documentation/queues/#ytindexer.queues.MessageQueue","title":"<code>MessageQueue</code>","text":"<p>               Bases: <code>Queue</code></p> <p>Queue implementation using Valkey/Redis for message tasks.</p> <p>This queue stores serialized JSON-compatible tasks in a Redis list. Tasks can be enqueued, dequeued singly or in batches.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>Any</code> <p>Redis or Valkey client instance used for queue operations.</p> <code>queue_name</code> <code>str</code> <p>Name/key of the Redis list representing the queue.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/queues/message.py</code> <pre><code>class MessageQueue(Queue):\n    \"\"\"Queue implementation using Valkey/Redis for message tasks.\n\n    This queue stores serialized JSON-compatible tasks in a Redis list.\n    Tasks can be enqueued, dequeued singly or in batches.\n\n    Attributes:\n        client (Any): Redis or Valkey client instance used for queue operations.\n        queue_name (str): Name/key of the Redis list representing the queue.\n    \"\"\"\n\n    def __init__(self, client: Any, queue_name: str = \"queue\"):\n        \"\"\"\n        Initialize a MessageQueue instance.\n\n        Args:\n            client (Any): Redis/Valkey client instance.\n            queue_name (str, optional): Name of the queue (Redis list key). Defaults to \"queue\".\n        \"\"\"\n        self.client = client\n        self.queue_name = queue_name\n        logger.info(f\"Initialized MessageQueue with name: {queue_name}\")\n\n    def enqueue(self, task_data: Any) -&gt; None:\n        \"\"\"\n        Add a task to the queue.\n\n        Serializes the task to JSON if it is a dict or list before pushing.\n\n        Args:\n            task_data (Any): The task data to enqueue. Can be any JSON-serializable object or string.\n        \"\"\"\n        if isinstance(task_data, (dict, list)):\n            task_data = json.dumps(task_data)\n        self.client.lpush(self.queue_name, task_data)\n        logger.debug(f\"Enqueued task to {self.queue_name}\")\n\n    def dequeue(self, timeout: float = 0.1) -&gt; Any:\n        \"\"\"\n        Remove and return a single task from the queue.\n\n        Performs a blocking pop with a timeout.\n\n        Args:\n            timeout (float, optional): Timeout in seconds to wait for a task before returning None. Defaults to 0.1.\n\n        Returns:\n            Any: The dequeued task, parsed from JSON if possible, or raw string if not JSON. Returns None if no task is available.\n        \"\"\"\n        result = self.client.brpop(self.queue_name, timeout=timeout)\n        if not result:\n            return None\n\n        data = result[1]\n        try:\n            return json.loads(data)\n        except json.JSONDecodeError:\n            return data\n\n    def batch_dequeue(self, batch_size: int = 10) -&gt; List[Any]:\n        \"\"\"\n        Remove and return multiple tasks from the queue in a batch.\n\n        Uses a Redis pipeline to pop multiple items atomically.\n\n        Args:\n            batch_size (int, optional): Number of tasks to dequeue. Defaults to 10.\n\n        Returns:\n            List[Any]: List of dequeued tasks, each parsed from JSON if possible, otherwise raw string.\n        \"\"\"\n        pipeline = self.client.pipeline()\n        pipeline.multi()\n        for _ in range(batch_size):\n            pipeline.rpop(self.queue_name)\n        results = pipeline.execute()\n\n        processed_results = []\n        for result in results:\n            if result is None:\n                continue\n            try:\n                processed_results.append(json.loads(result))\n            except json.JSONDecodeError:\n                processed_results.append(result)\n\n        return processed_results\n\n    def queue_size(self) -&gt; int:\n        \"\"\"\n        Get the current size of the queue.\n\n        Returns:\n            int: Number of tasks currently in the queue.\n        \"\"\"\n        return self.client.llen(self.queue_name)\n</code></pre>"},{"location":"documentation/queues/#ytindexer.queues.MessageQueue.__init__","title":"<code>__init__(client, queue_name='queue')</code>","text":"<p>Initialize a MessageQueue instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>Redis/Valkey client instance.</p> required <code>queue_name</code> <code>str</code> <p>Name of the queue (Redis list key). Defaults to \"queue\".</p> <code>'queue'</code> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/queues/message.py</code> <pre><code>def __init__(self, client: Any, queue_name: str = \"queue\"):\n    \"\"\"\n    Initialize a MessageQueue instance.\n\n    Args:\n        client (Any): Redis/Valkey client instance.\n        queue_name (str, optional): Name of the queue (Redis list key). Defaults to \"queue\".\n    \"\"\"\n    self.client = client\n    self.queue_name = queue_name\n    logger.info(f\"Initialized MessageQueue with name: {queue_name}\")\n</code></pre>"},{"location":"documentation/queues/#ytindexer.queues.MessageQueue.batch_dequeue","title":"<code>batch_dequeue(batch_size=10)</code>","text":"<p>Remove and return multiple tasks from the queue in a batch.</p> <p>Uses a Redis pipeline to pop multiple items atomically.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of tasks to dequeue. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List[Any]: List of dequeued tasks, each parsed from JSON if possible, otherwise raw string.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/queues/message.py</code> <pre><code>def batch_dequeue(self, batch_size: int = 10) -&gt; List[Any]:\n    \"\"\"\n    Remove and return multiple tasks from the queue in a batch.\n\n    Uses a Redis pipeline to pop multiple items atomically.\n\n    Args:\n        batch_size (int, optional): Number of tasks to dequeue. Defaults to 10.\n\n    Returns:\n        List[Any]: List of dequeued tasks, each parsed from JSON if possible, otherwise raw string.\n    \"\"\"\n    pipeline = self.client.pipeline()\n    pipeline.multi()\n    for _ in range(batch_size):\n        pipeline.rpop(self.queue_name)\n    results = pipeline.execute()\n\n    processed_results = []\n    for result in results:\n        if result is None:\n            continue\n        try:\n            processed_results.append(json.loads(result))\n        except json.JSONDecodeError:\n            processed_results.append(result)\n\n    return processed_results\n</code></pre>"},{"location":"documentation/queues/#ytindexer.queues.MessageQueue.dequeue","title":"<code>dequeue(timeout=0.1)</code>","text":"<p>Remove and return a single task from the queue.</p> <p>Performs a blocking pop with a timeout.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Timeout in seconds to wait for a task before returning None. Defaults to 0.1.</p> <code>0.1</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The dequeued task, parsed from JSON if possible, or raw string if not JSON. Returns None if no task is available.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/queues/message.py</code> <pre><code>def dequeue(self, timeout: float = 0.1) -&gt; Any:\n    \"\"\"\n    Remove and return a single task from the queue.\n\n    Performs a blocking pop with a timeout.\n\n    Args:\n        timeout (float, optional): Timeout in seconds to wait for a task before returning None. Defaults to 0.1.\n\n    Returns:\n        Any: The dequeued task, parsed from JSON if possible, or raw string if not JSON. Returns None if no task is available.\n    \"\"\"\n    result = self.client.brpop(self.queue_name, timeout=timeout)\n    if not result:\n        return None\n\n    data = result[1]\n    try:\n        return json.loads(data)\n    except json.JSONDecodeError:\n        return data\n</code></pre>"},{"location":"documentation/queues/#ytindexer.queues.MessageQueue.enqueue","title":"<code>enqueue(task_data)</code>","text":"<p>Add a task to the queue.</p> <p>Serializes the task to JSON if it is a dict or list before pushing.</p> <p>Parameters:</p> Name Type Description Default <code>task_data</code> <code>Any</code> <p>The task data to enqueue. Can be any JSON-serializable object or string.</p> required Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/queues/message.py</code> <pre><code>def enqueue(self, task_data: Any) -&gt; None:\n    \"\"\"\n    Add a task to the queue.\n\n    Serializes the task to JSON if it is a dict or list before pushing.\n\n    Args:\n        task_data (Any): The task data to enqueue. Can be any JSON-serializable object or string.\n    \"\"\"\n    if isinstance(task_data, (dict, list)):\n        task_data = json.dumps(task_data)\n    self.client.lpush(self.queue_name, task_data)\n    logger.debug(f\"Enqueued task to {self.queue_name}\")\n</code></pre>"},{"location":"documentation/queues/#ytindexer.queues.MessageQueue.queue_size","title":"<code>queue_size()</code>","text":"<p>Get the current size of the queue.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of tasks currently in the queue.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/queues/message.py</code> <pre><code>def queue_size(self) -&gt; int:\n    \"\"\"\n    Get the current size of the queue.\n\n    Returns:\n        int: Number of tasks currently in the queue.\n    \"\"\"\n    return self.client.llen(self.queue_name)\n</code></pre>"},{"location":"documentation/queues/#ytindexer.queues.Queue","title":"<code>Queue</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the interface for queue implementations.</p> <p>This class enforces methods for enqueuing, dequeuing (single and batch), and querying the size of the queue. Subclasses must implement these methods according to the underlying queue mechanism.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/queues/base.py</code> <pre><code>class Queue(ABC):\n    \"\"\"Abstract base class defining the interface for queue implementations.\n\n    This class enforces methods for enqueuing, dequeuing (single and batch), and querying\n    the size of the queue. Subclasses must implement these methods according to\n    the underlying queue mechanism.\n    \"\"\"\n\n    @abstractmethod\n    def enqueue(self, task_data: Any) -&gt; None:\n        \"\"\"\n        Add an item to the queue.\n\n        Args:\n            task_data (Any): The item to be added to the queue.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def dequeue(self) -&gt; Any:\n        \"\"\"\n        Remove and return a single item from the queue.\n\n        Returns:\n            Any: The item removed from the queue. Should return None if the queue is empty.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def batch_dequeue(self, batch_size: int) -&gt; List[Any]:\n        \"\"\"\n        Remove and return multiple items from the queue.\n\n        Args:\n            batch_size (int): Number of items to remove from the queue.\n\n        Returns:\n            List[Any]: A list of items removed from the queue. The list can be shorter than\n            batch_size if the queue has fewer items.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def queue_size(self) -&gt; int:\n        \"\"\"\n        Return the current number of items in the queue.\n\n        Returns:\n            int: The count of items currently in the queue.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"documentation/queues/#ytindexer.queues.Queue.batch_dequeue","title":"<code>batch_dequeue(batch_size)</code>  <code>abstractmethod</code>","text":"<p>Remove and return multiple items from the queue.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of items to remove from the queue.</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>List[Any]: A list of items removed from the queue. The list can be shorter than</p> <code>List[Any]</code> <p>batch_size if the queue has fewer items.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/queues/base.py</code> <pre><code>@abstractmethod\ndef batch_dequeue(self, batch_size: int) -&gt; List[Any]:\n    \"\"\"\n    Remove and return multiple items from the queue.\n\n    Args:\n        batch_size (int): Number of items to remove from the queue.\n\n    Returns:\n        List[Any]: A list of items removed from the queue. The list can be shorter than\n        batch_size if the queue has fewer items.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/queues/#ytindexer.queues.Queue.dequeue","title":"<code>dequeue()</code>  <code>abstractmethod</code>","text":"<p>Remove and return a single item from the queue.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The item removed from the queue. Should return None if the queue is empty.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/queues/base.py</code> <pre><code>@abstractmethod\ndef dequeue(self) -&gt; Any:\n    \"\"\"\n    Remove and return a single item from the queue.\n\n    Returns:\n        Any: The item removed from the queue. Should return None if the queue is empty.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/queues/#ytindexer.queues.Queue.enqueue","title":"<code>enqueue(task_data)</code>  <code>abstractmethod</code>","text":"<p>Add an item to the queue.</p> <p>Parameters:</p> Name Type Description Default <code>task_data</code> <code>Any</code> <p>The item to be added to the queue.</p> required Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/queues/base.py</code> <pre><code>@abstractmethod\ndef enqueue(self, task_data: Any) -&gt; None:\n    \"\"\"\n    Add an item to the queue.\n\n    Args:\n        task_data (Any): The item to be added to the queue.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/queues/#ytindexer.queues.Queue.queue_size","title":"<code>queue_size()</code>  <code>abstractmethod</code>","text":"<p>Return the current number of items in the queue.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The count of items currently in the queue.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/queues/base.py</code> <pre><code>@abstractmethod\ndef queue_size(self) -&gt; int:\n    \"\"\"\n    Return the current number of items in the queue.\n\n    Returns:\n        int: The count of items currently in the queue.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/worker/","title":"Worker","text":""},{"location":"documentation/worker/#ytindexer.worker.YouTubeNotificationParser","title":"<code>YouTubeNotificationParser</code>","text":"<p>Parser for YouTube PubSubHubbub notification XML data.</p> <p>Attributes:</p> Name Type Description <code>namespaces</code> <code>dict</code> <p>XML namespaces used for parsing.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/worker/parser.py</code> <pre><code>class YouTubeNotificationParser:\n    \"\"\"\n    Parser for YouTube PubSubHubbub notification XML data.\n\n    Attributes:\n        namespaces (dict): XML namespaces used for parsing.\n    \"\"\"\n\n    namespaces = {\n        \"xmlns\": \"http://www.w3.org/2005/Atom\",\n        \"yt\": \"http://www.youtube.com/xml/schemas/2015\",\n    }\n\n    @staticmethod\n    def _find_text(entry: ET.Element, tag: str) -&gt; Optional[str]:\n        \"\"\"\n        Find text content for a given tag inside an XML element.\n\n        Args:\n            entry (ET.Element): XML element to search within.\n            tag (str): Tag name to find.\n\n        Returns:\n            Optional[str]: Text content of the found element or None if not found.\n        \"\"\"\n        elem = entry.find(tag, YouTubeNotificationParser.namespaces)\n        return elem.text if elem is not None else None\n\n    @staticmethod\n    def _find_link(entry: ET.Element) -&gt; Optional[str]:\n        \"\"\"\n        Find the 'href' attribute from the link element inside the XML entry.\n\n        Args:\n            entry (ET.Element): XML element to search within.\n\n        Returns:\n            Optional[str]: URL string if found, otherwise None.\n        \"\"\"\n        link_elem = entry.find(\"xmlns:link\", YouTubeNotificationParser.namespaces)\n        return link_elem.get(\"href\") if link_elem is not None else None\n\n    @staticmethod\n    def parse(xml_data: str) -&gt; Optional[YouTubeNotification]:\n        \"\"\"\n        Parse YouTube PubSubHubbub notification XML string into a YouTubeNotification model.\n\n        Args:\n            xml_data (str): XML string of the notification.\n\n        Returns:\n            Optional[YouTubeNotification]: Parsed notification object or None if parsing fails.\n        \"\"\"\n        try:\n            root = ET.fromstring(xml_data)\n            entry = root.find(\"xmlns:entry\", YouTubeNotificationParser.namespaces)\n            if entry is None:\n                logger.warning(\"No entry found in notification XML\")\n                return None\n\n            video_id = YouTubeNotificationParser._find_text(entry, \"yt:videoId\")\n            if not video_id:\n                logger.warning(\"Missing video ID in notification\")\n                return None\n\n            notification = YouTubeNotification(\n                video_id=video_id,\n                channel_id=YouTubeNotificationParser._find_text(entry, \"yt:channelId\"),\n                title=YouTubeNotificationParser._find_text(entry, \"xmlns:title\"),\n                published=YouTubeNotificationParser._find_text(\n                    entry, \"xmlns:published\"\n                ),\n                updated=YouTubeNotificationParser._find_text(entry, \"xmlns:updated\"),\n                link=YouTubeNotificationParser._find_link(entry),\n                author=YouTubeNotificationParser._find_text(\n                    entry, \"./xmlns:author/xmlns:name\"\n                ),\n                processed_at=datetime.now(timezone.utc),\n                source=\"pubsubhubbub\",\n            )\n            return notification\n\n        except Exception as e:\n            logger.error(f\"Failed to parse notification XML: {e}\")\n            logger.debug(f\"Notification data: {xml_data}\")\n            return None\n</code></pre>"},{"location":"documentation/worker/#ytindexer.worker.YouTubeNotificationParser.parse","title":"<code>parse(xml_data)</code>  <code>staticmethod</code>","text":"<p>Parse YouTube PubSubHubbub notification XML string into a YouTubeNotification model.</p> <p>Parameters:</p> Name Type Description Default <code>xml_data</code> <code>str</code> <p>XML string of the notification.</p> required <p>Returns:</p> Type Description <code>Optional[YouTubeNotification]</code> <p>Optional[YouTubeNotification]: Parsed notification object or None if parsing fails.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/worker/parser.py</code> <pre><code>@staticmethod\ndef parse(xml_data: str) -&gt; Optional[YouTubeNotification]:\n    \"\"\"\n    Parse YouTube PubSubHubbub notification XML string into a YouTubeNotification model.\n\n    Args:\n        xml_data (str): XML string of the notification.\n\n    Returns:\n        Optional[YouTubeNotification]: Parsed notification object or None if parsing fails.\n    \"\"\"\n    try:\n        root = ET.fromstring(xml_data)\n        entry = root.find(\"xmlns:entry\", YouTubeNotificationParser.namespaces)\n        if entry is None:\n            logger.warning(\"No entry found in notification XML\")\n            return None\n\n        video_id = YouTubeNotificationParser._find_text(entry, \"yt:videoId\")\n        if not video_id:\n            logger.warning(\"Missing video ID in notification\")\n            return None\n\n        notification = YouTubeNotification(\n            video_id=video_id,\n            channel_id=YouTubeNotificationParser._find_text(entry, \"yt:channelId\"),\n            title=YouTubeNotificationParser._find_text(entry, \"xmlns:title\"),\n            published=YouTubeNotificationParser._find_text(\n                entry, \"xmlns:published\"\n            ),\n            updated=YouTubeNotificationParser._find_text(entry, \"xmlns:updated\"),\n            link=YouTubeNotificationParser._find_link(entry),\n            author=YouTubeNotificationParser._find_text(\n                entry, \"./xmlns:author/xmlns:name\"\n            ),\n            processed_at=datetime.now(timezone.utc),\n            source=\"pubsubhubbub\",\n        )\n        return notification\n\n    except Exception as e:\n        logger.error(f\"Failed to parse notification XML: {e}\")\n        logger.debug(f\"Notification data: {xml_data}\")\n        return None\n</code></pre>"},{"location":"documentation/worker/#ytindexer.worker.YouTubeNotificationProcessor","title":"<code>YouTubeNotificationProcessor</code>","text":"<p>Processes YouTube PubSubHubbub notifications from the queue using the parser.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/worker/processor.py</code> <pre><code>class YouTubeNotificationProcessor:\n    \"\"\"\n    Processes YouTube PubSubHubbub notifications from the queue using the parser.\n    \"\"\"\n\n    def __init__(self, notification_queue: Queue, output_queue: Queue, parser: Any):\n        self.notification_queue = notification_queue\n        self.output_queue = output_queue\n        self.parser = parser\n        self._shutdown_event = asyncio.Event()\n\n    async def process_notification(self, xml_data: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"\n        Process a single YouTube notification XML.\n\n        Args:\n            xml_data (str): Raw XML data from the notification.\n\n        Returns:\n            Optional[Dict[str, Any]]: Extracted video metadata if successful, otherwise None.\n        \"\"\"\n        try:\n            metadata = self.parser.parse(xml_data)\n            if metadata is None:\n                logger.warning(\"Notification processing returned None metadata\")\n            return metadata\n        except Exception as e:\n            logger.error(f\"Failed to process notification: {e}\")\n            logger.debug(traceback.format_exc())\n            return None\n\n    async def process_batch(self, batch_size: int = 10) -&gt; int:\n        \"\"\"\n        Process a batch of notifications from the queue.\n\n        Args:\n            batch_size (int): Number of notifications to process in a batch.\n\n        Returns:\n            int: Number of successfully processed notifications.\n        \"\"\"\n        notifications = []\n        processed = 0\n\n        for _ in range(batch_size):\n            notification = self.notification_queue.dequeue(timeout=0.1)\n            if notification is None:\n                break\n            notifications.append(notification)\n\n        if not notifications:\n            return 0\n\n        tasks = [\n            self.process_notification(notification) for notification in notifications\n        ]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        for result in results:\n            if isinstance(result, Exception):\n                logger.error(f\"Error during notification processing: {result}\")\n                continue\n            if result is not None:\n                if isinstance(result, dict):\n                    self.output_queue.enqueue(result)\n                elif isinstance(result, YouTubeNotification):\n                    self.output_queue.enqueue(result.model_dump_json())\n                else:\n                    raise ValueError(f\"Can't equeue payload with type: {type(result)}\")\n                processed += 1\n\n        logger.info(f\"Processed {processed}/{len(notifications)} notifications\")\n        return processed\n\n    async def run(self, poll_interval: float = 0.5):\n        \"\"\"\n        Run the worker process to continuously process notifications.\n\n        Args:\n            poll_interval (float): Time to wait between polling the queue.\n        \"\"\"\n        logger.info(\"Starting YouTube notification processor worker\")\n\n        try:\n            while not self._shutdown_event.is_set():\n                queue_size = self.notification_queue.queue_size()\n\n                if queue_size &gt; 0:\n                    logger.debug(f\"Queue has {queue_size} notifications pending\")\n                    await self.process_batch()\n                else:\n                    await asyncio.sleep(poll_interval)\n\n        except asyncio.CancelledError:\n            logger.info(\"Worker cancelled gracefully\")\n            raise\n        except Exception as e:\n            logger.error(f\"Worker encountered an error: {str(e)}\")\n            logger.error(traceback.format_exc())\n\n    def shutdown(self):\n        \"\"\"\n        Signal the processor to shut down gracefully.\n        \"\"\"\n        self._shutdown_event.set()\n</code></pre>"},{"location":"documentation/worker/#ytindexer.worker.YouTubeNotificationProcessor.process_batch","title":"<code>process_batch(batch_size=10)</code>  <code>async</code>","text":"<p>Process a batch of notifications from the queue.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of notifications to process in a batch.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of successfully processed notifications.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/worker/processor.py</code> <pre><code>async def process_batch(self, batch_size: int = 10) -&gt; int:\n    \"\"\"\n    Process a batch of notifications from the queue.\n\n    Args:\n        batch_size (int): Number of notifications to process in a batch.\n\n    Returns:\n        int: Number of successfully processed notifications.\n    \"\"\"\n    notifications = []\n    processed = 0\n\n    for _ in range(batch_size):\n        notification = self.notification_queue.dequeue(timeout=0.1)\n        if notification is None:\n            break\n        notifications.append(notification)\n\n    if not notifications:\n        return 0\n\n    tasks = [\n        self.process_notification(notification) for notification in notifications\n    ]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    for result in results:\n        if isinstance(result, Exception):\n            logger.error(f\"Error during notification processing: {result}\")\n            continue\n        if result is not None:\n            if isinstance(result, dict):\n                self.output_queue.enqueue(result)\n            elif isinstance(result, YouTubeNotification):\n                self.output_queue.enqueue(result.model_dump_json())\n            else:\n                raise ValueError(f\"Can't equeue payload with type: {type(result)}\")\n            processed += 1\n\n    logger.info(f\"Processed {processed}/{len(notifications)} notifications\")\n    return processed\n</code></pre>"},{"location":"documentation/worker/#ytindexer.worker.YouTubeNotificationProcessor.process_notification","title":"<code>process_notification(xml_data)</code>  <code>async</code>","text":"<p>Process a single YouTube notification XML.</p> <p>Parameters:</p> Name Type Description Default <code>xml_data</code> <code>str</code> <p>Raw XML data from the notification.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Optional[Dict[str, Any]]: Extracted video metadata if successful, otherwise None.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/worker/processor.py</code> <pre><code>async def process_notification(self, xml_data: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Process a single YouTube notification XML.\n\n    Args:\n        xml_data (str): Raw XML data from the notification.\n\n    Returns:\n        Optional[Dict[str, Any]]: Extracted video metadata if successful, otherwise None.\n    \"\"\"\n    try:\n        metadata = self.parser.parse(xml_data)\n        if metadata is None:\n            logger.warning(\"Notification processing returned None metadata\")\n        return metadata\n    except Exception as e:\n        logger.error(f\"Failed to process notification: {e}\")\n        logger.debug(traceback.format_exc())\n        return None\n</code></pre>"},{"location":"documentation/worker/#ytindexer.worker.YouTubeNotificationProcessor.run","title":"<code>run(poll_interval=0.5)</code>  <code>async</code>","text":"<p>Run the worker process to continuously process notifications.</p> <p>Parameters:</p> Name Type Description Default <code>poll_interval</code> <code>float</code> <p>Time to wait between polling the queue.</p> <code>0.5</code> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/worker/processor.py</code> <pre><code>async def run(self, poll_interval: float = 0.5):\n    \"\"\"\n    Run the worker process to continuously process notifications.\n\n    Args:\n        poll_interval (float): Time to wait between polling the queue.\n    \"\"\"\n    logger.info(\"Starting YouTube notification processor worker\")\n\n    try:\n        while not self._shutdown_event.is_set():\n            queue_size = self.notification_queue.queue_size()\n\n            if queue_size &gt; 0:\n                logger.debug(f\"Queue has {queue_size} notifications pending\")\n                await self.process_batch()\n            else:\n                await asyncio.sleep(poll_interval)\n\n    except asyncio.CancelledError:\n        logger.info(\"Worker cancelled gracefully\")\n        raise\n    except Exception as e:\n        logger.error(f\"Worker encountered an error: {str(e)}\")\n        logger.error(traceback.format_exc())\n</code></pre>"},{"location":"documentation/worker/#ytindexer.worker.YouTubeNotificationProcessor.shutdown","title":"<code>shutdown()</code>","text":"<p>Signal the processor to shut down gracefully.</p> Source code in <code>.venv/lib/python3.12/site-packages/ytindexer/worker/processor.py</code> <pre><code>def shutdown(self):\n    \"\"\"\n    Signal the processor to shut down gracefully.\n    \"\"\"\n    self._shutdown_event.set()\n</code></pre>"},{"location":"getting-started/configuration/","title":"Configuration","text":""},{"location":"getting-started/configuration/#configuration","title":"Configuration","text":""},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file with the following configuration, you can use the .env.template as base:</p> <pre><code>GOOGLE_API_KEY=&lt;YOUR API KEY&gt;\nGOOGLE_API_URL=\"https://www.googleapis.com/youtube/v3/search\"\n\nNGROK_URL=&lt;YOU NGROK URL&gt;\n\nQUEUE_HOST=\"localhost\"\nQUEUE_PORT=6379\nQUEUE_PASSWORD=&lt;YOUR PASSWORD&gt;\n\nMONGO_INITDB_ROOT_USERNAME=&lt;YOUR USERNAME&gt;\nMONGO_INITDB_ROOT_PASSWORD=&lt;YOUR PASSWORD&gt;\n\nMONGO_NAME=\"mongo\"\nMONGO_HOST=\"localhost\"\nMONGO_PORT=27017\nMONGO_USERNAME=&lt;YOUR USERNAME&gt;\nMONGO_PASSWORD=&lt;YOUR PASSWORD&gt;\nMONGO_AUTH=&lt;AUTH DB NAME&gt;\n\nELASTIC_INDEX_NAME=\"videos\"\nELASTIC_HOST=\"localhost\"\nELASTIC_PORT=9200\nELASTIC_USERNAME=&lt;YOUR USERNAME&gt;\nELASTIC_PASSWORD=&lt;YOUR PASSWORD&gt;\nELASTIC_SCHEME=&lt;HTTP or HTTPS&gt;\n\nTRANSCRIPT_LANGUAGES=[\"en\", \"fr\", ...]\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will walk you through setting up the YouTube Video Indexer system on your local development environment or production server.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#software-dependencies","title":"Software Dependencies","text":""},{"location":"getting-started/installation/#required-software","title":"Required Software","text":"<ul> <li>python: 3.12+</li> <li>docker engine: Docker version 28.1.1, build 4eba377</li> <li>docker compose: Docker Compose v2.35.1</li> <li>git: For source code management</li> <li>uv: astral-uv package manager</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-docker-compose-recommended","title":"Method 1: Docker Compose (Recommended)","text":"<p>This is the fastest way to get the system running with all dependencies.</p>"},{"location":"getting-started/installation/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/maikereis/youtube-video-indexer.git\ncd youtube-video-indexer\n</code></pre>"},{"location":"getting-started/installation/#step-2-configure-environment","title":"Step 2: Configure Environment","text":"<pre><code># Copy environment template\ncp .env.example .env\n\n# Edit configuration (see Configuration section below)\nnano .env\n</code></pre>"},{"location":"getting-started/installation/#step-3-start-services","title":"Step 3: Start Services","text":"<pre><code># Start all services in background\ndocker compose up -d\n\n# View logs\ndocker compose logs -f\n</code></pre>"},{"location":"getting-started/installation/#step-4-verify-installation","title":"Step 4: Verify Installation","text":"<pre><code># Check service health\ncurl http://localhost:8080/api/v1/health/\n\n# Expected response\n{\"name\":\"YouTube Indexer API\",\"version\":\"1.0.0\",\"status\":\"online\",\"docs\":\"/docs\"}\n</code></pre>"},{"location":"getting-started/installation/#method-2-development-setup","title":"Method 2: Development Setup","text":"<p>For contributors and developers.</p>"},{"location":"getting-started/installation/#step-1-fork-and-clone","title":"Step 1: Fork and Clone","text":"<pre><code># Fork the repository on GitHub, then:\ngit clone https://github.com/YOUR_USERNAME/youtube-video-indexer.git\ncd youtube-video-indexer\n\n# Add upstream remote\ngit remote add upstream https://github.com/maikereis/youtube-video-indexer.git\n</code></pre>"},{"location":"getting-started/installation/#step-2-install-development-dependencies","title":"Step 2: Install Development Dependencies","text":"<pre><code># Create development environment\nuv sync\n\n# Install with development dependencies\nuv sync --dev\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"getting-started/installation/#step-3-run-tests","title":"Step 3: Run Tests","text":"<pre><code># Run test suite\npytest\n</code></pre>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":""},{"location":"getting-started/quick-start/#basic-usage","title":"Basic Usage","text":"<p>First you will need to ingest a video metada. Here i will create a post method to simulate a notification received from ByteByteGo youtube channel</p> <pre><code>import requests\n\nchannel_id = \"UCZgt6AzoyjslHTC9dz0UoTw\"  # Use the channel ID you desire to ingest content\nvideo_id = \"2g1G8Jr88xU\" # A video ID of a video from the channel you select\ntitle = \"System Design Was HARD - Until You Knew the Trade-Offs, Part 2\" # The video title\n\npayload = f\"\"\"&lt;?xml version='1.0' encoding='UTF-8'?&gt;\n&lt;feed xmlns:yt=\"http://www.youtube.com/xml/schemas/2015\" xmlns=\"http://www.w3.org/2005/Atom\"&gt;\n    &lt;entry&gt;\n        &lt;yt:videoId&gt;{video_id}&lt;/yt:videoId&gt;\n        &lt;yt:channelId&gt;{channel_id}&lt;/yt:channelId&gt;\n        &lt;title&gt;{title}&lt;/title&gt;\n        &lt;link rel=\"alternate\" href=\"https://www.youtube.com/watch?v={video_id}\"/&gt;\n        &lt;author&gt;\n            &lt;name&gt;ByteByteGo&lt;/name&gt;\n        &lt;/author&gt;\n        &lt;published&gt;2025-05-11T17:00:34+00:00&lt;/published&gt;\n        &lt;updated&gt;2025-05-24T15:50:55.820073346+00:00&lt;/updated&gt;\n    &lt;/entry&gt;\n&lt;/feed&gt;\"\"\"\n\nheaders = {'Content-Type': 'application/xml'}\n\nurl = \"http://localhost:8080/webhooks\"  # Replace with your webhook URL, if you use NGrok it will be like: \"https://&lt;random stuff&gt;.ngrok-free.app/webhooks\"\n\nresponse = requests.post(url, data=payload, headers=headers)\n\nprint(f\"Status Code: {response.status_code}\")\nprint(f\"Response Text: {response.text}\")\n</code></pre> <p>Now we can query the indexed videos:</p> <pre><code>import requests\n\n# Search for videos\nresponse = requests.get('http://localhost:8080/api/v1/videos', {\n    'query': 'System Design',\n    'limit': 10\n})\n\nvideos = response.json()\nprint(f\"Found {len(videos)} videos\")\nprint(videos[\"results\"][0])\n</code></pre>"}]}